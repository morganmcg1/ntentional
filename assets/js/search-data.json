{
  
    
        "post0": {
            "title": "Text Data Cleanup with XLM-R Embeddings and UMAP Visualisation",
            "content": "In order for Machine Translation to be useful in the real world, we should should strive to train it on high quality translation data. This is doubly true for lower-resource languages such as Irish, where clean training data is relatively limited. In this article we will try and identify and remove clusters of dirty/noisey samples in our parallalel dataset. The stages we will go through are: . Generate embeddings from a pre-trained multi-lingual model, XLM-RoBERTa | Visualise these embeddings using a dimensionality technique via UMAP | Identify clusters in a sample of the data that seem to be of low translation quality via Bokeh | Remove similar samples from the main dataset via nmslib | . Dimension Reduction - UMAP . Dimensionality reduction techniques attempt to find the latent features in your data . | Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique published in 2018 by McInnes and Healy that can be used for visualisation of high dimensional data, similarly to [[t-SNE]]. . | Its main advantage is that it is fast (faster than t-SNE when dealing with large datasets) and also better maintains the global structure of the data. . | Pair Code has an excellent explanation complete with visualisations of UMAP if you&#39;d like to learn more and docs also have excellent interactive visualisation examples from users . | It also has beautifully well-written documentation . | McInnes gives a useful overview of UMAP here at the SciPy 2018 conference: ## Similarity Search - nmslib . | nmslib is an &quot;efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces&quot; and is easily installed with pip install nmslib . | In testing Ben Frederickson found that between nmslib, FAISS (Facebook) and annoy (Spotify) nmslib was the fastest nearest neighbours library for CPU. Note FAISS on GPU is blazes past nmslib and annoy, but can also difficult to set up. . | Milvus is an interesting library that was open sourced in late 2019 which offers similarity search using FAISS, nmslib or annoy as well as GPU capability. If I had had more time I would have explored this further . | ElasticSearch post and article is yet another tool we could use to carry out the similarity search between our text embeddings. . | Finally, DAIR.ai recently posted a video called &quot;101 Ways to Solve Search&quot; by Pratik Bhavsar which is a great introduction to how search works in general. Also models.pratik.ai is a really great flow chart visualisation which he tries to keep up to date of the models and techniques available for semantic search and NLP in general . | BONUS: @hmendonca&#39;s EDA notebook on kaggle is an excellent example showing how to use annoy to group and display similar images (faces in this case). | . Text Embeddings - XLM-R . We derive our text embedding by passing the text sample though XLM-RoBERTa Large and extracting the values in final hidden layer. XLM-R is a multilingual model and should work reasonably well for both Irish and English . Research has been done on BERT to show that by concatenating the final 4 layers of the model one gets even richer contextual embeddings. In the interest of simplicity we will stick to the final layer only for the moment, although using additional layers would be an interesting area of exploration! . today = date.today() . Text Embeddings for Fun and Profit . Text embeddings are incredibly valuable and can also be used for things like spell checking; have a look at @er214&#39;s post and notebook to see how they used GloVe word embeddings to create a spell-checker for their dataset. The fastai forums post is also worth a read if you have time, they also created a &quot;pretentiousness&quot; embedding to score news outlets ü§£. . While we are using embeddings for chunks of text, as opposed to individual words, the concept is the same. . Generate our Embeddings . Load Raw ParaCrawl Data . First lets load our raw data. This is data that has been crawled from the internet and contains a few different types of artifacts including: . Non-latin scripts | Other random characters (e.g. &#39;¬©&#39;,&#39;¬≥&#39;,&#39;¬∫&#39;) | Text samples that are unlikely to have been translated to Irish by a human, including Porn sites | Lighting sites (who knows why?) | . | . Lets see how much we can find by turning our samples into embeddings and using dimenstionality reduction to visualise them. . For fun, we&#39;ll label texts that contain &quot;sex&quot;, &quot;lighting&quot; and cyrillic characters like &quot;–∏&quot;, &quot;–∑&quot; or &quot;–ª&quot; see if they get clustered together when we visualise our word embeddings later. We will also lowercae our entire dataset; many of the legal texts here are fully written in uppercase, however right now we care more about the content of the text as opposed to the style . Dataset has : 784606 rows . ga en noise_type . 0 d&#39;fhonn go raibh maith agat litreacha grianghraif teagmh√£¬°il naisc | order thanks letters photos contact links | na | . 1 ingus &amp; oleg ngo &quot;ord√£¬∫&quot; go raibh maith ingus agus oleg do na fionnachtana agus aistreoidh s√£¬© chuig an reburial na tais√£ de dh√£¬° saighdi√£¬∫ir√£ an airm dearg. | ingus &amp; oleg ngo &quot;order&quot; thank ingus and oleg for the discovery and transfer to the reburial of the remains of two soldiers of the red army. | na | . 2 renarsp poibl√£ eagra√£ ocht &quot;ord√£¬∫&quot; mbu√£ ochas len√£¬°r gcomhghleaca√£ - √£ s√£¬°ideoir√£ renarsp, a bhrath agus chun c√£¬∫naimh i dh√£ saighdi√£¬∫ir gearm√£¬°nach. saighdi√£¬∫ir√£ a tharchur chuig an reburial an reilig na gearm√£¬°ine. | renarsp public organization &quot;order&quot; thanks to our colleague - users renarsp, for the detection of and assistance in the exhumation of a german soldier. soldiers referred to the reburial of the german cemetery. | na | . Get Pre-Trained XLM-RoBERTa Model . Next we will initialise our XLM-RoBERTa model (commonly known as &quot;XLM-R&quot;). I chose this model as it is a multilingual model trained on 100 languages, including both English and Irish, if we choose to look at the Irish embeddings too. . Here we will embed the English samples as this is where a lot of the noise in the dataset seems to stem from. . We use XLM-R Large in this example, but XLM-R Base will also work well . model = XLMRobertaModel.from_pretrained(&#39;xlm-roberta-large&#39;) model.cuda() tokenizer = XLMRobertaTokenizer.from_pretrained(&#39;xlm-roberta-base&#39;) pad_idx=tokenizer.pad_token_id . Set up Tokenizer and Padding . As part of setting up our dataloader we&#39;ll need to define our: . Tokenizer Transform, which is a wrapper around the HuggingFace tokenizer | Function to add padding to each sample so that we can batch our samples | Function to create a padding mask so that our model doesn&#39;t attend over the padding tokens | . Create our Dataloader . Now we are ready to create our dataloader so that we can quickly iterate through our samples. I&#39;m using fastai here, however you could also use either a PyTorch or Tensorflow dataloader here, the rest of this notebook doesn&#39;t have a fastai dependency. . To start, we&#39;ll look at a random sample of 40,000 samples, about 5% of our data. Remember we are only retrieving embeddings for our English samples for now. . You can see the English sample below, including the special tokens &lt; s &gt; and &lt; /s &gt; used by XLM-R to denote the start and end of the sequence. You can also see all the padding needed for this sample. . samp_dls = get_dls(samp_texts, bs, sl, show=True) . &lt;s&gt; (i) in subsection (1) by deleting the definition of ‚Äúdependant‚Äù, and&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; . Get Embeddings . Now we&#39;re ready to retrieve our embeddings from the pretrained multi-lingual model. We do this by simply passing our samples to the model and saving the output activations from the last layer of the model. From this we can generate an embedding of size (40000, 1024) which we can then pass to our dimensionality reduction algorithm. . Processing 40k samples with UMAP takes about 5 minutes. After a little testing with n_neighbors and min_dist I found the defaults work quite well although its always worth playing around with the parameters and distance metric used. . samp_embs = torch.load(&#39;models/xlm-r_large_embs_en_samp_2020-06-29&#39;).to(&#39;cpu&#39;) . Scaling your Embeddings . One thing to consider might be whether you should scale your embeddings. The embeddings used here had mean 0 and standard deviation of 0.5, normalising them to (0,1) didn&#39;t seem to have much of an impact on the UMAP visualisation so it is not done here. Scikit-Learn has a wide variety of scaling functions if you do need to scale your data. . Remove similar samples from the entire dataset . We will remove items by: . Visually identifying similar noisey embeddings . | Taking the average of these embeddings . | Calculating a distance between this average embedding and all of the embeddings in our entire dataset . | Removing embeddings that are within a certain distance to the average embeddings . | Visualise: Dimensionality Reduction with UMAP . samp_mapper = umap.UMAP(random_state=42).fit(samp_embs.cpu().numpy()) . Identify clusters in a sample of the data that seem to be of low translation quality . Can we identify suspect looking clusters of data? Yes! . We can see the orange &quot;islands&quot; that we have labelled all contain text related to &quot;lighting&quot;, &quot;LED&quot;, &quot;Lamps&quot; etc, some of which are not even in English. . | The sparse cluster of green blue points and their neighbours are also full of language related to pornography . | We don&#39;t see so many Cryllic points, however this was also the least common of our labels, comprising only (0.07% of our labels) . | . Other &quot;islands&quot; that can identified by hovering over them include: . Arabic texts | Website footers | Text from jewellery sites (e.g. Pandora) and also clothing sites | . Below you can see some of the Arabic texts in our main dataset we have discovered: . ga en noise_type . 572564 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ÿßŸÑŸÄ riyadh | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ..ÿßŸäŸÄ ÿ¨ŸÜŸàÿ® ŸÄŸÄÿ±ÿßŸÜ.. | na | . 570189 ÿ±ÿØ: ÿØÿ±ÿ≥..ÿßÿ≥ÿ™ÿÆÿØÿ¢ŸÖ content-aware scale..~ | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ÿ∑Ÿäÿ®ŸáŸÄ ÿßŸÑÿ∑Ÿäÿ®ŸáŸÄ | na | . 567067 ŸÖŸÑÿπŸÇÿ© ÿ∑ÿπÿßŸÖ ÿÆŸÑ ÿßÿ®Ÿäÿ∂ | ŸÇŸÑÿπÿ© ŸÅŸÜ ÿßŸÑÿ®ŸÉÿ≥ŸÑ ( pixels art ) | na | . 573439 ŸáŸÄŸÄŸÄÿ∞ÿßŸÉ ŸÑŸÄŸÄŸÄŸà ÿßŸÜŸÄŸÄŸÄŸä ŸÖŸÄŸÄŸÄŸÜ ÿßŸÑŸÄŸàŸÇŸÄŸÄÿ™ ŸÖŸÄŸáŸÄŸÄÿ≤ŸàŸÖ | ŸÇŸÑÿπÿ© ÿØÿ±Ÿàÿ≥ ÿßŸÑÿßŸäŸÖŸäÿ¨ ÿ±ŸäÿØŸä | na | . 571181 ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ adigatalostan | ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ nimrow | na | . I am fairly confident that none of the text in some these islands contain valuable translations and are likely the result of automated translations of suspect quality that we would like to remove. . Remove similar samples from the full dataset . To remove similar samples we will calculate a distance metric between each of our selected embeddings and each of the embeddings in the full dataset. Alternativly we could also calculate the &quot;average embedding&quot; of all of our selected datapoints, and then calculate the distance between this average and each of the embeddings in the main dataset. . From experimentation I found that the first option is more effective at identifying more of the noisy data we are looking for. In addtion, because our nms algorithm is super fast at retrieval there isn&#39;t any significant overhead to this approach over using the average embedding. . First of all, we&#39;ll need to generate embeddings for all of our 780k text samples. This might take a little while depending on the size of your dataset so kick off the extraction and grab a coffee. . # #hide # full_dls = get_dls(all_texts, bs, show=False) # full_emb_nm = f&#39;models/xlm-r_large_embs_en_full_{today}&#39; # full_emb = get_text_embeddings(model, full_dls, emb_nm=full_emb_nm, n_files=6).to(&#39;cpu&#39;) # full_emb.size() full_emb = torch.load(&#39;models/xlm-r_large_embs_en_full_2020-06-19_full&#39;, &#39;cpu&#39;) full_emb.size() . torch.Size([784606, 1024]) . Sci-kit Learn&#39;s cosine_similarity . By calculating the average embedding of our selected datapoints we can calculate the cosine similarity between it and all of the other embeddings in our full dataset, providing decent results! . sns.distplot(cs_df.score, kde=False) plt.title(&#39;Cosine similarity between the average selection embedding and the full datset&#39;); . We could stop at this point and continue to loop through selecting new datapoints of interest and removing them from our full dataset, but for fun lets look at another way to do similarity search, using nmslib . Creating a nmslib Index . . Note: As an alternative to Sci-kit Learn&#8217;s cosine similarity function we can also use nmslib to calculate our similarity. Note this method is slower given our needs in this example, but its always fun to work with a new technology üòÄ Once we have all of our embeddings we can create our index with nmslib. This is the most time-consuming part of this process, it took 47minutes for the index to be created in this example, although there are other nmslib settings thatn can reduce this to ~15minutes. . We create an index for our entire dataset only once. We create an index for the entire dataset because we will likely have multiple queries we do not want to re-create a new index each time as it will really slow down how fast we can identify and remove low quality data. . Note: (This index will now include our sample datapoints, which are the same datapoints that we use in our query to the index. Therefore we will have to exclude these sample datapoints from our query result) . Querying the Index . Below are the sampled results from querying the full dataset, found through similarity search with an average embedding of the selection from the bokeh plot: . Arabic . 1691 ids returned from query . ga en noise_type . 566807 ŸÇŸÑÿπÿ© ŸÅŸÜ ÿßŸÑÿ®ŸÉÿ≥ŸÑ ( pixels art ) | ÿ∑ÿ±ŸäŸÇŸá ÿπŸÖŸÑ ÿßÿ∑ÿßÿ± ŸÖŸÜŸÇÿ∑ | na | . 571586 ŸÖŸàÿßŸÇÿπ ÿßŸÑÿßÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÖÿ¨ÿßŸÜŸâ free sms ( 1 2) | ŸÑÿß ÿ™ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ±ÿØ ÿπŸÑŸâ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ | na | . 571682 ŸÇŸÑÿπÿ© ÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅŸäŸÉÿ™Ÿàÿ± illustrator Ÿà coreldraw | ÿßŸÑŸÑŸáŸÖ ÿµŸÑŸä Ÿàÿ≥ŸÑŸÖ ÿπŸÑŸâ ŸÜÿ®ŸäŸÜÿß ŸÖÿ≠ŸÖÿØ | na | . 570557 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÇŸÑÿπÿ© ÿßŸÑÿµŸàÿ± ÿßŸÑÿÆÿßÿµÿ© ÿ®ÿßŸÑÿ™ÿµŸÖŸäŸÖ | na | . 566810 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ŸÅŸä ÿßŸÑŸÅŸàÿ™Ÿàÿ¥Ÿàÿ® | na | . Lighting . 1699 ids returned from query . ga en noise_type . 690718 an ts√≠n bh√≠ guangdong soilse linn sn√°mha faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3x5 watts led-e27 manufacturers and listed here are sourced by the karnar lighting. | lighting | . 477330 foinse do gaird√≠n lawn monar√≥ir√≠ ag soilsi√∫ karnar zhongshan &amp; leictreon mhonarcha. | source for high power led wall washer 144w led wall washer manufacturers at zhongshan karnar lighting &amp; electron factory. | lighting | . 479274 an ts√≠n bh√≠ guangdong cumhacht ard-√©adrom lawn faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3 watts led under ground lights round type manufacturers and listed here are sourced by the karnar lighting. | lighting | . 636797 an ts√≠n bh√≠ guangdong faoi sti√∫ir tube monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong led spot light flash lamp and fancy ball manufacturers and listed here are sourced by the karnar lighting. | lighting | . 692603 an ts√≠n bh√≠ guangdong faoi sti√∫ir connectable soilse na nollag monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong high-power led colorful 500w led wall washer manufacturers and listed here are sourced by the karnar lighting. | lighting | . Website Footer . 1825 ids returned from query . ga en noise_type . 696159 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 02:43:19. | ¬©2005-2010 karnarsambandlink okkurveftr√©last modified: august 08 2016 04:06:45. | na | . 478396 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 00:39:06. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 30 2016 22:04:01. | na | . 538977 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 14 2016 20:20:33. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 15 2016 01:50:20. | na | . 684675 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 01:19:15. | ¬©2005-2010 karnarcontactez-nousnous lierplan du sitelast modified: august 08 2016 03:54:30. | na | . 534904 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 11:57:09. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 31 2016 09:46:30. | na | . Clothing and Jewelery . 1938 ids returned from query . ga en noise_type . 641483 home:: uairead√≥ir√≠ hublot:: sraith fusion classic:: sraith 45mm fusion classic:: macasamhail hublot classic comhle√° sraith faire 45mm 511.zx.1170 | home:: hublot watches:: classic fusion series:: classic fusion 45mm series:: replica hublot classic fusion 45mm watch series 401.mx.0123.gr [da02] | na | . 79453 pandora bead √≥ir ivy √≥ir - ‚Ç¨10.23 : jewelry pandora saor, pandoraaustraliabracelets.com | pandora gold bead ivy gold - $11.00 : cheap pandora jewelry, pandoraaustraliabracelets.com | na | . 615744 clrip036a 925 sterling silver b√°n crystal ring - ‚Ç¨28.83 : jewelry pandora saor , pandoraforyou.com | clrip036a 925 sterling silver white crystal ring - $31.00 : cheap pandora jewelry, pandoraforyou.com | na | . 67287 ard-chumhacht t√°irg√≠ faoi sti√∫ir &gt; cumhacht ard-threoraithe colorful &gt; product-list | led lighting &gt; high-power led colorful &gt; product-list lww-10 lww-10-108p lww-10-206p lww-8c-108p | lighting | . 635957 prada m√°la√≠ l√°imhe p - br4692 caife leathar : seaic√©ad spyder, pradahandbags.top | prada borse p - br4692 coffee leather : spyder giacca , pradahandbags.top | na | . Removal . If we like we can limit our removal to only very similar embeddings in the full dataset by only selecting the datapoints in the full dataset that are sufficiently close (lower score) to the average embedding, plotting the score distribution can help us decide on a suitable threshold . Summary . Hopefully this gives you a sense of how you can explore and clean up large, noisy text datasets. You can open this notebook on github and test it for your own text dataset. . As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter: @mcgenergy .",
            "url": "https://www.ntentional.com/2020/06/29/Text-Cleaning-With-Clustering.html",
            "relUrl": "/2020/06/29/Text-Cleaning-With-Clustering.html",
            "date": " ‚Ä¢ Jun 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing nlp.irish!",
            "content": "tl;dr . Looking through papers to track down the Irish-English parallel corpora they used was a real pain, so I built nlp.irish to document where to find them and how to process them easily | . What? . The intention behind nlp.irish is to make NLP for folks new to working with Irish a little easier by documenting the datasets that are available out there, where to find them and how to load them to a pandas dataframe. | . The site is hosted on github here with the intention that it will grow via a collaborative effort of those working in Irish NLP. | . Where? . üáÆüá™ nlp.irish üáÆüá™ | . Why? . Irish is a low-resource language and every piece of data out there is valuable. | . Current Data . As of writing, 5 commonly used Irish-English parallel corpora have been documented, with instructions on where to find them and code on how to process them: . ParaCrawl, v6 | DGT-TM, DGT-Translation Memory | DCEP, Digital Corpus of the European Parliament | ELRC, European Language Resource Coordination | Tatoeba | . | . Contributing . Contributing is as easy as submitting a pull request on Github. Alternatively you can find me on twitter at @mcgenergy and I can help update the site with your contibution. | . .",
            "url": "https://www.ntentional.com/irish/translation/nmt/mt/nlp/2020/06/12/introducing-nlp-irish.html",
            "relUrl": "/irish/translation/nmt/mt/nlp/2020/06/12/introducing-nlp-irish.html",
            "date": " ‚Ä¢ Jun 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "ICLR 2020: Efficient Deep Learning and More",
            "content": "I was lucky enough to volunteer and attend (virtual) ICLR 2020. It delivered a huge amount of learning for me and I was fortunate to join some really great discussions. . Efficient Deep Learning was big focus of many of the papers and in this second ICLR2020 article* I will focus on techniques presented that either enable more efficient training and/or inference from the papers I managed to see. There are also a couple of bonus papers I really liked at the end of this article. . *In my first ICLR2020 article I highlight some of the new, more efficient transformer achitectures presented such as ELECTRA, Reformer and more. . Note: ICLR Videos Now Online! . All of the ICLR paper talks and slides are now online, I highly recommend watching the 5 to 15minutes videos accompanying each of the papers below for some excellent summaries and additional understanding . Efficient Deep Learning . Training methods and architecture changes that can make Deep Learning models smaller/more efficient . &#9889; Reducing Transformer Depth on Demand with Structured Dropout &#9889; . The introduction of LayerDrop in this paper was super exciting to read as it makes a (transformer) model much more robust to pruning while only having to train it once, unlike finding lottery tickets for example | . Essentially the idea is simple, just randomly remove/skip different layers during the forward pass in training like so: | . . LayerDrop can be implemented like so (see the link below for the author&#39;s full codebase) layer_drop = 0.2 # The authors dropped the layers with a 20% probability in all of their experiments for layer in transformer.layers: if random(0,1) &gt; layer_drop and self.training: x = layer(x) . | This training setup confers 3 benefits: Increased Training Speed (training less layers) in training the percentage increase in words per second increased almost linearly with the percentage of layers dropped | . | Strong regulariser NLP models trained with layerDrop seem to perform better than baseline models trained without it (e.g. EN-DE Transformer performance improvement) | Increased robustness of deeper models which enables you to increase the number of layers in your model. The authors doubled the encoder depth in their WMT14 EN-DE transformer translation model for a new SOTA BLEU score. | Increases model stability | Note the authors also reduced DropOut slightly when training to compensate for the additional regularisation of LayerDrop | . | Reduction in model size A model trained with LayerDrop can be pruned to any desired depth for inference and still maintain robust performance without additional fine-tuning | The specific type of pruning used for inference also did not seem to matter although dropping every other layer seemd to offer strong performance while being straightforward to implement | . | | . Unfortunatley when I asked during the Q&amp;A whether this could be applied to when fine-tuning existing pre-trained transformer models, such as those in HuggingFace&#39;s library, one of the authors replied that they had tried it but it didn&#39;t have great results. Their theory was that these transformers had learned so much during pre-training that a little bit of fine-tuning using LayerDrop wasn&#39;t able to have enough of an influence on the model weights to confer this robustness. | . Code for LayerDrop and models pre-trained with LayerDrop can be found here. If you want to use RoBERTa but find it too large/slow for inference then you should give the models here a go! | . | . &#9889; Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP &#9889; . This work is a follow on from The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks from Jonathan Frankle, Michael Carbin at FAIR, who&#39;s research codebase has just been released by the way: I just open-sourced my codebase for research on neural network pruning, the Lottery Ticket Hypothesis, and other topics in deep learning. It&#39;s written in PyTorch and designed to make it easy to add new models, datasets, and experiments. Check it out: https://t.co/JyTGT8RRZW . &mdash; Jonathan Frankle (@jefrankle) May 7, 2020 | At ICLR 2020 they present how the lottery ticket phenomenon, which previously was only explored for vision models, applies more generally to deep neural networks across NLP and reinforcement learning. | . They test it with NLP models, LSTMs and Transformer, as well as reinforcement learning models and found that the lottery ticket sub-networks performed better than randomly pruned networks, as was found in their previous work on vision models. | . . The authors used Iterative Pruning with Late Resetting (aka Late Rewinding): The trick is that the subnetworks don&#39;t always emerge at initialization. Instead, we found that training these subnetworks from an iteration slightly after initialization (between a few iterations and a few epochs) often works much better. We term this technique &quot;late resetting.&quot; . &mdash; Jonathan Frankle (@jefrankle) March 6, 2019 | Currently the downside to discovering lottery tickets is that they are very computationally expensive to discover. Here the authors trained the models to convergence, before pruning ~20%, reinitializing and training again. Several cycles of this requires significant computational resources for large models such as transformers and reinforcement learning frameworks. However once a lottery ticket is found it can be trained quickly due to its reduced size whilst still maintaining almost the same performance of the original full network. | . I&#39;d also recommend watching the authors second ICLR 2020 paper, &quot;The Early Phase of Neural Network Training&quot;, which explored how the &quot;Early Phase&quot; of the network training, i.e. the point at which lottery ticket sub-networks emerge (and the point at which Late Resetting would reset to) was impacted by variations to the input data and weight distributions. | . &#9889; Dynamic Model Pruning with Feedback &#9889; . The paper introduces a dynamic way to prune weights (Dynamic Model Pruning with Feedback, or DPF) that allows previously pruned model weights to be re-activated when needed, resulting in lottery-ticket peformance of the pruned models while only needing to be trained once (unlike lottery tickets which need multiple rounds of training) | . . They achieve state-of-the-art top-1 accuracy for pruning on CIFAR-10 and Imagenet for unstructured weight pruning | . The gradient is evaluated for the pruned model and then applied to the dense model. The binary mask is periodically updated to reallocate the weights. The intuition is that the gradient is used to measure the &quot;error&quot; and then the dense model is used to correct this error | . Unstructured magnitude pruning was used | . The authors say that the code will be released in June | . &#9889; Once for All: Train One Network and Specialize it for Efficient Deployment &#9889; . The idea here is that a single large model can be trained the contains a multitude of high performant sub-networks. These sub-networks can be pruned for use in a wide variety of edge device types and sizes without additional training. The author&#39;s focussed on training efficient vision models for this paper. | . . This enables strong performance on a wide variety of devices, without incurring the computational expence (and CO2 footprint) of searching for specialised architectures for each device | . . The key to training this model is a technique called Progresive Shrinking which is a:a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution) . | . . They achieved 1.5x lower latency for MobileNet-V3 and 2.6x for EfficientNet in ImageNet mobile setting while maintaining the same accuracy | . Their Code and 50 pre-trained models (for many devices &amp; many latency constraints) can be found in their gihub | . Other Great Papers You Should Absolutely Checkout &#128175; . There were many other super interesting papers I couldn&#39;t cover there, some of my favorites are below . Optimisation . Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization . Introduces Moving Average Batch Normalization (MABN) for training with small batches | Restores BatchNorm-like performance when training with small batches, down to bs=1 (BatchNorm tends to suffer when training with small batches) | Code here | . Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models . Useful for stabilising training on fine-tuning (BERT for downstream task for example) | Motivated by DropOut (which is a special case of DropConnect) | Replaces a randomly selected parameter with a &quot;target&quot; parameter, instead of zero as in DropOut, from a previously memorised state | Code here | . Vision . Network Deconvolution . Correlations between pixels and between channels can make image recognition more difficult, the authors propose network deconvolution to solve this | Achieves impressive performance gains across ResNet, ResNeXt, EfficientNet, VGG (and more) in both image classification and semantic segmentation tasks, even when BatchNorm is removed | Network Deconvolution seems to hold promise beyond vision models too:Also, the same deconvolution procedure for 1 √ó 1 convolutions can be used for non-convolutional layers, which makes it useful for the broader machine learning community. . | Code here- Network Deconvolution has also been discussed and implemented in the fastai forums | . . How much Position Information Do Convolutional Neural Networks Encode? . Adding zero-padding (widely used already) implicitly delivers positional information and imrproves vision performance | Deeper models can better encode positional information | . Thanks for Reading &#128515; . As always, I would love to hear if you have any comments, thoughts or criticisms at @mcgenergy .",
            "url": "https://www.ntentional.com/papers/nlp/efficient-nlp/transformers/2020/05/09/iclr-highlights-2.html",
            "relUrl": "/papers/nlp/efficient-nlp/transformers/2020/05/09/iclr-highlights-2.html",
            "date": " ‚Ä¢ May 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "ICLR 2020: Efficient NLP - Transformers",
            "content": "I was lucky enough to volunteer and attend (virtual) ICLR 2020. It delivered a huge amount of learning for me and I was fortunate to join some really great discussions. . Efficient NLP was big focus of many of the papers and here I will focus on a few of the more well known transformer architectures proposed over the past year or so; Reformer, ELECTRA, Lite Transformer and ALBERT. Towards the end of this article I also mention additional ICLR summaries that are worth reading üôÇ . Note: ICLR Videos Now Online! . All of the ICLR paper talks and slides are now online, I highly recommend watching the 5 to 15minutes videos accompanying each of the papers below for some excellent summaries and additional understanding #ICLR2020 Public Archive - https://t.co/EpXWIK0ujS * ~700 short talks with synced slides, papers, and code* 8 keynotes with moderated QA * 15 workshops on topics ranging from climate change to AfricaNLP. pic.twitter.com/FVX2JJUYVZ . &mdash; Sasha Rush (@srush_nlp) May 4, 2020 . Efficient NLP - Transformers . New transformer architectures that promise less compute-intense NLP training, in order of my excitement to use them: . &#9889; Reformer: The Efficient Transformer &#9889; . Reformer enables training on much longer sequences than BERT-like models (e.g. document-length sequences instead of 512 token length sequences) much more efficiently | Reformer introduces a couple of techniques that improve both time and memory efficiency: . | Technique 1: Reversible residual connection layers (originally used in computer vision in RevNets) instead of the standard residual layers improves memory efficiency: . | . . Technique 2: Locality-Sensitive Hashing (LSH) based attention replaces dot-product attention (and is much faster) which reduces the time complexity: | . The 15 minute ICLR paper presentation video linked above really helps better understand these concepts | A PyTorch Reformer implementation can be found here | . &#9889; ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators &#9889; . ELECTRA brings a couple of novelties, resulting in a much more computationally efficient transformer to train. It is trained with: a Generator-Discriminator setup and | a new pre-training task called Replaced Token Detection | . | The Generator is trained to replace masked tokens (as per the standard MLM task), the Discriminator then tries to identify the token that has been replaced | . . One subtle thing to note is that if the generator happens to generate the correct token then that token is considered &quot;real&quot; instead of &quot;fake&quot; | ELECTRA-small can be trained on a single V100 GPU (4 days) | It is slower per epoch than other transformers, but it converges faster resulting in an overall faster training:the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient . | Very strong results and it&#39;s performance scales up as the architecture is made larger | Lots more interesting results and experiment discussion can be found in the paper | A HuggingFace ELECTRA Implementation is here | . &#9889; Lite Transformer with Long-Short Range Attention &#9889; . Introduces Long-Short Range Attention (LRSA) which results in a reduction in model computation between 2.5x and 3x compared to original Transformer. . | The new architecture enables 2 different perspectives on the input sequence: . ...one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention)... . | The LSRA architecture and where the attention is focussed can be seen here: . | Lite Transformer performs well against the original Transformer for translation, summarisation and language modelling | One thing I liked is that Lite Transformer looks at performance under mobile-like constraints, defined by the authros as 10M parameters and 1G FLOPs | Lite Transformer code (PyTorch) is available from the authors here | . &#9889; ALBERT: A Lite BERT for Self-supervised Learning of Language Representations &#9889; . ALBERT is 18x smaller model than BERT-large and can be trained 1.7x faster while still outperforming it | The two techniques used to reduce its size are: Reduce the vocabulary embedding size; they reduce the matrix size by projecting it to a lower dimension. e.g. an input one-hot encoded matrix of size 30,000 is reduced to a much smaller sized matix which is then used | Cross-layer parameter sharing; they use the same operations and repeat them multiple times. This helps the parameter size of the network growing as layers as added | . | ALBERT uses 3 training tricks to further improve its performance: Uses MLM and Sentence Order Prediction (SOP), a self-supervised loss that focuses on modeling inter-sentence coherence | Does not use dropout (due to the huge amount of data available) | Uses 10x more data than BERT-Base | | HuggingFace PyTorch ALBERT code can be found here | . . Other Great Summaries to Read . Other great summaries from ICLR attendees are below, the Google Doc in Yacine&#39;s tweet below gives brief summaries to even more papers that I haven&#39;t covered here . Marija Stanojevic on mentorship tips for aspiring ML Researchers, @mstanojevic118 . | Yacine Jernite with additional paper summaries, @YRnite . | Analytics Vidhya with a summary of the event and what the most used opensource tools were . | . To Close . Research work on efficient NLP is moving rapidly and it was fascinating to see so many different approaches on display at ICLR this year, myself and my single GPU are super excited to see how fast things will develop this year üòÜ . This was also the first ML conference I attended and found the (covid-caused) virtual format to work exceptionally well, my huge congrates to all of the organisers involved in pulling off a massive amount of work in such a short amount of time! . As always, I would love to hear if you have any comments, thoughts or criticisms at @mcgenergy .",
            "url": "https://www.ntentional.com/papers/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "relUrl": "/papers/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "date": " ‚Ä¢ May 5, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "FastHugs: Language Modelling with Tranformers and Fastai",
            "content": "This aims to be an end-to-end description with code of how to train a transformer language model using fastai (v2) and HuggingFace, enjoy! . TL;DR . Main interesting bits in this notebook: . Provides full code to train a transformer (RoBERTa) using a Masked Language Model task | Utilise&#39;s many of HuggingFace&#39;s tokenizer features within fastai | Make predictions of masked tokens like this: | . . Before we get started . First off, huge thanks as always to both the Fastai and HuggingFace teams for giving so much back to the community by open-sourcing so much | For an example of text sequence classification using HuggingFace and fastai, have a look at my previous notebook here . | This tutorial is heavily based on HuggingFace&#39;s &quot;How to train a new language model from scratch using Transformers and Tokenizers&quot; tutorial, I highly recommend checking that out too. I try and highlight throughout where code has been used, borrowed or inspired by HuggingFace&#39;s code. . | . MLM Tranform . I feel the most useful thing in this notebook is the MLMTokensLabels transform*. This carries out the Masked Language Model task that RoBERTa was originally trained on. . This will take tokens ids (tokens after the have been numericalized), select a subset and either mask a certain amount of them (for prediction) or replace them with other random token ids (for regularisation). This transform also creates our labels by copying the input token ids and masking the tokens that do not need to be predicted, so that no loss is calculated on them. . Note the if you wish to train BERT or other transformer language models you will probably need to use a different task, e.g. BERT was trained on 2 tasks simultaneously, MLM and Next Sentence Prediction (NSP). Have a look at any blog posts or arxiv paper of the transformer of interest to find which task was used to pretrain it. . *This transform code is a re-write of the mask_tokens function used in HugginFace&#39;s tutorial, code here . Pretraining + Fine-Tuning: . As shown in ULMFit, MultiFiT, and elsewhere, you will get better results on your downstream task if you first fine-tune your pretrained model with the text of the same domain as your pretrained task. e.g. training an IMDB movie review classifier who&#39;s language model was trained on wikipedia text. 1/ Really excited about this one! &quot;Don&#39;t Stop Pretraining: Adapt Language Models to Domains and Tasks&quot; is live! With @anmarasovic, @swabhz , @kylelostat , @i_beltagy , Doug Downey, and @nlpnoah, to appear at ACL2020. Paper: https://t.co/hVbSQYnclk Code: https://t.co/7wKgE1mUme . &mdash; Suchin Gururangan (@ssgrn) April 24, 2020 . Using a Custom Tokenizer? . This code has not been tested using a custom tokenizer. You may want to do so if your text is very specific to a certain domain. If so then you&#39;ll have to add a number of attributes to your tokenzier to be able to use the code here. I really recommend the HuggingFace language model tutorial linked above for an example of training your own tokenizer with your own dataset . Data . We&#39;ll use the IMDB_SAMPLE here, pretending we are fine-tuning our transformer model before doing sentiment classification on IMDB. If you are pretraining a language model from scratch you&#39;d aim to use a larger, more generic source like a wikipedia dataset. fastai have the full WikiText103 (100 million tokens) dataset available for easy download here if you&#39;d like to train an enligh language model from scratch: . path = untar_data(URLs.WIKITEXT) . HuggingFace Auto Classes . HuggingFace have a numer of useful &quot;Auto&quot; classes that enable you to create different models and tokenizers by changing just the model name. . AutoModelWithLMHead will define our Language model for us. This can either be a pretrained model or a randomly initialised model | AutoTokenizer will load our tokenizer and enable us grab our vocab | AutoConfig will define the model architecture and settings, note that we use the pretrained config here for ease of use, but one can easily modify this config if needed | model_name is the model architecture (and optionally model weights) you&#39;d like to use. Language Models tested so far with this notebook: roberta-base | You can find all of HuggingFace&#39;s models at https://huggingface.co/models, most, but not all of them are supported by AutoModel,AutoConfig and AutoTokenizer | . | . We can now easily call whichever transformer we like as below: . model_name = &#39;roberta-base&#39; lm_model_class = AutoModelWithLMHead config_dict = AutoConfig.from_pretrained(model_name) . HuggingFace Tokenizer &amp; Vocab . We use AutoTokenizer to generate our pretrained tokenizer. HuggingFace&#39;s get_vocab returns a token : index dict however Fastai expects vocab to be a list. Therefore we need to convert this dict to a list to be able to use it in fastai . #collapse tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer_vocab=tokenizer.get_vocab() tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])] print(f&#39;Tokenizer &quot;{tokenizer.__class__}&quot; vocab length is : {len(tokenizer_vocab_ls)}&#39;) . . Tokenizer &#34;&lt;class &#39;transformers.tokenization_roberta.RobertaTokenizer&#39;&gt;&#34; vocab length is : 50265 . Special Tokens . Its always good to know what special tokens your tokenizer takes, lets have a look: . tokenizer.special_tokens_map . {&#39;bos_token&#39;: &#39;&lt;s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;, &#39;sep_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;, &#39;cls_token&#39;: &#39;&lt;s&gt;&#39;, &#39;mask_token&#39;: &#39;&lt;mask&gt;&#39;} . FastHugs Tokenizer . This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used. . class FastHugsTokenizer(): &quot;&quot;&quot; transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class model_name : model type set by the user max_seq_len : override default sequence length, typically 512 for bert-like models. `transformer_tokenizer.max_len_single_sentence` and `transformer_tokenizer.max_len_sentences_pair` both account for the need to add additional special tokens, i.e. for RoBERTa-base max_len_single_sentence==510, leaving space for the 2 additional special tokens to be added for the model&#39;s default 512 positional embeddings pair : whether a single sentence (sequence) or pair of sentences are used Returns: - Tokenized text, up to the max sequence length set by the user or the tokenzier default &quot;&quot;&quot; def __init__(self, transformer_tokenizer=None, model_name=&#39;roberta&#39;, max_seq_len=None, pretrained=True, pair=False, **kwargs): self.model_name, self.tok, self.max_seq_len=model_name, transformer_tokenizer, max_seq_len if pretrained: if self.max_seq_len: if pair: assert self.max_seq_len&lt;=self.tok.max_len_sentences_pair, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_sentences_pair&#39; else: assert self.max_seq_len&lt;=self.tok.max_len_single_sentence, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_single_sentence&#39; else: if pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence) def do_tokenize(self, o:str): &quot;&quot;&quot;Returns tokenized text, adds prefix space if needed, limits the maximum sequence length&quot;&quot;&quot; if &#39;roberta&#39; in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len] else: tokens = self.tok.tokenize(o)[:self.max_seq_len] return tokens def de_tokenize(self, o): &quot;&quot;&quot;Return string from tokens&quot;&quot;&quot; text=self.tok.convert_tokens_to_string(o) return text def __call__(self, items): for o in items: yield self.do_tokenize(o) . The Fastai bit . fasthugstok and our tok_fn . Lets incorporate the tokenizer from HuggingFace into fastai-v2&#39;s framework by specifying a function called fasthugstok that we can then pass on to Tokenizer.from_df. (Note .from_df is the only method I have tested) . Max Seqence Length . max_seq_len is the longest sequece our tokenizer will output. We can also the max sequence length for the tokenizer by changing max_seq_len. It uses the tokenizer&#39;s default, typically 512. 1024 or even 2048 can also be used depending on your GPU memory. Note when using pretrained models you won&#39;t be able to use a max_seq_len larger than the default. . max_seq_len = None sentence_pair=False fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, max_seq_len=max_seq_len, sentence_pair=sentence_pair) . We create a MLMTokenizer class which inherits from fastai&#39;s Tokenizer in order to fully decode . #collapse class MLMTokenizer(Tokenizer): def __init__(self, tokenizer, rules=None, counter=None, lengths=None, mode=None, sep=&#39; &#39;, **kwargs): super().__init__(tokenizer, rules, counter, lengths, mode, sep) def _detokenize1(self, o):return self.tokenizer.de_tokenize(o) def decodes(self, o): return TitledStr(str(self._detokenize1(o))) . . Set up fastai&#39;s Tokenizer.from_df, we pass rules=[fix_html] to clean up some of HTML messiness in our text. If you do not want any rules then you sould pass rules=[] to override fastai&#39;s default text processing rules . #collapse fastai_tokenizer = MLMTokenizer.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, tok_func=fasthugstok, rules=[fix_html], post_rules=[]) fastai_tokenizer.rules . . [&lt;function fastai2.text.core.fix_html(x)&gt;] . Add Special Tokens . BERT-like transformers require special tokens to be added to the sequence, depending on the task, so we need a transform for those too . class AddSpecialTokens(Transform): &quot;Add special token_ids to the numericalized tokens for Sequence Classification&quot; def __init__(self, tokenizer): self.tok=tokenizer def encodes(self, o): return(TensorText(self.tok.build_inputs_with_special_tokens(list(o)))) . Create MLM Dataset . #collapse class MLMTokensLabels(Transform): &#39;&#39;&#39; MLM task - Select subset of input token ids, given by `mlm_probability` - Mask a subset of these, `mask_token_prob` - Replace half of the first subset with random tokens - This code most comes from the `mask_tokens` function here https://github.com/huggingface/transformers/blob/a21d4fa410dc3b4c62f93aa0e6bbe4b75a101ee9/examples/run_language_modeling.py#L66 Returns: input ids and labels &#39;&#39;&#39; def __init__(self, tokenizer=None, mlm_probability=0.15, mask_token_prob=0.8): self.tok, self.mlm_probability, self.mask_token_prob=tokenizer, mlm_probability, mask_token_prob def _gen_probability_matrix(self, labels): # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability, defaults to 0.15 in Bert/RoBERTa) probability_matrix = torch.full(labels.shape, self.mlm_probability) special_tokens_mask = self.tok.get_special_tokens_mask(labels.tolist(), already_has_special_tokens=True) probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0) if self.tok._pad_token is not None: padding_mask = labels.eq(self.tok.pad_token_id) probability_matrix.masked_fill_(padding_mask, value=0.0) return probability_matrix def _replace_with_mask(self, inputs, labels, masked_indices): # for `mask_token_prob`% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK]) indices_replaced = torch.bernoulli(torch.full(labels.shape, self.mask_token_prob)).bool() &amp; masked_indices inputs[indices_replaced] = self.tok.convert_tokens_to_ids(self.tok.mask_token) return inputs, indices_replaced def _replace_with_other(self, inputs, labels, masked_indices, indices_replaced): # 1-`mask_token_prob`)/210% of the time, we replace masked input tokens with random word indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced random_words = torch.randint(len(self.tok), labels.shape, dtype=torch.long) inputs[indices_random] = random_words[indices_random] return inputs def encodes(self, inputs): if self.tok.mask_token is None: raise ValueError(&quot;This tokenizer does not have a mask token which is necessary for masked language modeling.&quot;) labels = inputs.clone() # Get probability of whether a token will be masked probability_matrix = self._gen_probability_matrix(labels) # Create random mask indices according to probability matrix masked_indices = torch.bernoulli(probability_matrix).bool() # Mask the labels for indices that are NOT masked, we only compute loss on masked tokens labels[~masked_indices] = -100 # Randomly replace with mask token inputs, indices_replaced = self._replace_with_mask(inputs, labels, masked_indices) # Randomly replace with mask token inputs = self._replace_with_other(inputs, labels, masked_indices, indices_replaced) # The rest of the time (10% of the time) we keep the masked input tokens unchanged return (inputs,labels) . . We change decodes in our Numericalize class to deal with the &lt;loss_mask&gt; tokens . # collapse @Numericalize def decodes(self,o): &#39;Add the ability to parse masks for the loss function, set as `-100`&#39; if isinstance(o, tuple): o=o[0] tmp_vocab=self.vocab.copy() tmp_vocab.append(&#39;&lt;loss_mask&gt;&#39;) o=[-1 if o_ == -100 else o_ for o_ in o] return L(tmp_vocab[o_] for o_ in o if tmp_vocab[o_] != PAD) . . And we modify Datasets so as to not wrap out tuple in another tuple . # collapse @delegates(Datasets) class Datasets(Datasets): &quot;Doesn&#39;t create a tuple in __getitem__ as x is already a tuple&quot; def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): super().__init__(items=items, tfms=tfms, tls=tls, n_inp=n_inp, dl_type=dl_type, **kwargs) def __getitem__(self, it): # same as Datasets.__getitem__ but not wrapped in a tuple res = [tl[it] for tl in self.tls] return res[0] if is_indexer(it) else list(zip(*res)) . . Our dataset is now ready to be created, lets look at an some of our (x,y) that will be passed to the model. When -100 is passed to our loss function (nn.CrossEntropyLoss) it will be ignored in the calculation. Our model will also ignore any padding tokens (usually defined as 1) when passed to it. . #collapse-hide splits = ColSplitter()(df) tfms=[attrgetter(&quot;text&quot;), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls), AddSpecialTokens(tokenizer), MLMTokensLabels(tokenizer)] dsets = Datasets(df, splits=splits, tfms=[tfms], dl_type=SortedDL) dsets[0][0][:20], dsets[0][1][:20] . . (tensor([ 0, 1890, 12, 5225, 24320, 12, 8494, 18421, 50264, 328, 14938, 1774, 630, 75, 190, 356, 69, 50264, 32819, 784]), tensor([ -100, -100, -100, 5225, -100, -100, -100, 18421, -100, -100, -100, -100, -100, 75, -100, -100, -100, 4505, -100, 784])) . Dataloader . Padding . We need to make sure our padding is done correctly as some transformer models prefer padding on the left while others prefer it on the right. tokenizer.padding_side will tell us which side is correct. e.g., BERT, Roberta prefers padding to the right, so we set pad_first=False . #collapse def pad_mlm_input(samples, pad_idx=1, pad_fields=[0,1], pad_first=False, max_seq_len=None, backwards=False): &quot;Function that collect `samples` and adds padding, modified `max_len_l` in fastai&#39;s `pad_input`&quot; pad_fields = L(pad_fields) #max_len_l = ifnone(max_seq_len, pad_fields.map(lambda f: max([len(s[f]) for s in samples]))) max_len_l = pad_fields.map(lambda f: max_seq_len) if backwards: pad_first = not pad_first def _f(field_idx, x): if isinstance(x, tuple): x=(x[0]) ## Added this line too, removes tuple if present if field_idx not in pad_fields: return x idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x)) pad = x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx x1 = torch.cat([pad, x] if pad_first else [x, pad]) if backwards: x1 = x1.flip(0) return retain_type(x1, x) return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples] def transformer_mlm_padding(tokenizer=None, max_seq_len=None, sentence_pair=False): &#39;Uses `pad_fields=[0,1]` to pad both input and label&#39; if tokenizer.padding_side == &#39;right&#39;: pad_first=False else: pad_first=True max_seq_len = ifnone(max_seq_len, tokenizer.max_len) return partial(pad_mlm_input, pad_fields=[0,1], pad_first=pad_first, pad_idx=tokenizer.pad_token_id, max_seq_len=max_seq_len) . . #collapse padding=transformer_mlm_padding(tokenizer) bs=4 dls = dsets.dataloaders(bs=bs, before_batch=[padding]) . . Check our batch . We can see our special RoBERTa tokens (&#39;&lt;s&gt;&#39;, &#39;&lt;/s&gt;&#39;), which translate to 0, 2 in its vocab, have been added to the start and end of each sequence in the batch. Your can look at these indices in tokenizer.get_vocab() to confirm this. We can also see that most of the tokens in our target (text_) are masked out as we only want to calculate the loss on the ~15% of the text tokens that have been masked. . #collapse b=dls.one_batch() b[0].size(), b[1].size() . . (torch.Size([4, 512]), torch.Size([4, 512])) . #collapse dls.show_batch() . . text text_ . 0 &lt;s&gt; I&lt;mask&gt; fortunate enough to meet&lt;mask&gt; Pal segregatedand still have my DS:TMlishing&lt;mask&gt; autographed by&lt;mask&gt;&lt;mask&gt; at a convention shortly&lt;mask&gt; the release, and asked him why he chose to do the film &quot;camp&quot;. Before&lt;mask&gt; could answer, two studio flacks intercepted and lectured me on how the studio &quot;knew best&quot; and how &quot;no one will take such&lt;mask&gt; film seriously&quot;. I had been reading the Bantam reprints for&lt;mask&gt; couple of years thanks&lt;mask&gt; a&lt;mask&gt; (ComiCon attendees of the 1970s will recall 357hawk and his band? I was in&lt;mask&gt; couple&lt;mask&gt; years of that withnd), and had higher hopes than what we&lt;mask&gt;.&lt;mask&gt; nThe flacks insisted that no high adventure would ever be&lt;mask&gt; seriously, and so doing &#39;camp&lt;mask&gt; was the&lt;mask&gt; way. Several other fans jumped in gap my&lt;mask&gt;, with Pal listening as best he could. At the end of the little event, Pal&lt;mask&gt; up to&lt;mask&gt; and apologized,&lt;mask&gt; he could have done more and better. n nSTAR WARS put the lie to | &lt;loss_mask&gt;&lt;loss_mask&gt; was&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt; (&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;OB poster&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him)&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; after&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; he&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; &quot;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; friend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Black&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hopes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; got&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; done&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;&lt;loss_mask&gt;&lt;loss_mask&gt; only&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt; side&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; came&lt;loss_mask&gt;&lt;loss_mask&gt; us&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; wishing&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; that&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rating as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt; n&lt;loss_mask&gt; destroying the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; still&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; we&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;hero&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, there&lt;loss_mask&gt;&lt;loss_mask&gt; second&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; serious&lt;loss_mask&gt; Yes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt; And&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;sheet&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; leaping&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bronze&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;AV&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Next&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; If&lt;loss_mask&gt; knows&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; San&lt;loss_mask&gt; for the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 1 &lt;s&gt;&lt;mask&gt; is another one of those &#39;humans vs insects/eco-horror&#39; features; a theme that was popular in the late 70&#39;s.&lt;mask&gt; you can&#39;t really call it horror. There&#39;s zero suspense and no&lt;mask&gt; events.&lt;mask&gt; other words: this movie&lt;mask&gt; pretty lame. It&#39;s not that it&lt;mask&gt; really bad or&lt;mask&gt;; it&#39;s just very boring. A construction site near&lt;mask&gt; hotel uncovers a big nest of&lt;mask&gt;. Later on we learn that, probably due to&lt;mask&gt; sorts&lt;mask&gt; pesticides Lounge in the past, their&lt;mask&gt; became poisonous. Some people get bitten and rushed to&lt;mask&gt; hospital and it takes ages for&lt;mask&gt;&lt;mask&gt; Vanity the&lt;mask&gt; to figure out what&#39;s going on.&lt;mask&gt; Foxworth figures&lt;mask&gt; out first and then you can&lt;mask&gt; him go berserk with a digging machine for what seems like several hours.&lt;mask&gt; they&lt;mask&gt; in the house, waiting&lt;mask&gt; get rescued. And, man, you should see all the efforts they make for&lt;mask&gt; them.&lt;mask&gt; won&#39;t spoil too much, but at&lt;mask&gt; point they even use a big&lt;mask&gt;. All the | &lt;loss_mask&gt; This&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Only&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; gruesome&lt;loss_mask&gt;&lt;loss_mask&gt; In&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; something&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; different&lt;loss_mask&gt; of&lt;loss_mask&gt; used&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bite&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the residents of&lt;loss_mask&gt; hospital&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Robert&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; see&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Then&lt;loss_mask&gt; flee&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; all&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rescuing&lt;loss_mask&gt;&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; one&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helicopter&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt; thinking&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; you&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; building&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; lots of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt; are shown&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; garbage&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; straw&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt; wider shots&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; designers&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; near&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; do&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; IT&lt;loss_mask&gt;&lt;loss_mask&gt;EN&lt;loss_mask&gt; AT&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; my&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; title&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;K&lt;loss_mask&gt;&lt;loss_mask&gt; MAN&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;ll&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Now&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 2 &lt;s&gt; The&lt;mask&gt;&lt;mask&gt; saw no fewer than 3 filmed productions&lt;mask&gt; H. G. Wells&#39; great novel, &quot;War of&lt;mask&gt; Worlds&quot;. This&lt;mask&gt; perhaps the least well-known and very probably the best of&lt;mask&gt;&lt;mask&gt; No other&lt;mask&gt;&lt;mask&gt; W&lt;mask&gt;W has ever attempted not only to present the story very much as Wells wrote&lt;mask&gt;, but also Burton create the atmosphere of the time&lt;mask&gt; which it was supposed to take place: the last year of&lt;mask&gt; 19th Century, 1900 ¬Ö using Wells&#39; original setting, in and near Woking&lt;mask&gt;&lt;mask&gt;. n nIMDb&lt;mask&gt; unfFlyingly to what they regard as &quot;spoilers&quot;. That might apply&lt;mask&gt; some&lt;mask&gt;, where the ending might actually be a&lt;mask&gt;, but with regard to one of the most famous novels in&lt;mask&gt;&lt;mask&gt;, it seems positively silly. I have&lt;mask&gt; sympathy&lt;mask&gt; people who have neglected to&lt;mask&gt; one&lt;mask&gt; the seminal works&lt;mask&gt; English literature,&lt;mask&gt; let&#39;s get right to the chase. The aliens are destroyed through catching an Earth disease,&lt;mask&gt; hits&lt;mask&gt; have no immunity. If that wo a spoiler, so be | &lt;loss_mask&gt;&lt;loss_mask&gt; year 2005&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; them.&lt;loss_mask&gt;&lt;loss_mask&gt; version of&lt;loss_mask&gt;ot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, England&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; seems unfriend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; films&lt;loss_mask&gt; where&lt;loss_mask&gt;&lt;loss_mask&gt; might&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; surprise&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the world&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have no&lt;loss_mask&gt; for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; read&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; so&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; against which they&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; 1953 classic&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39; plot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ÔøΩ&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; way&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; off due to&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Century&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;ides&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; some&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; an&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; than&lt;loss_mask&gt;).&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; are typical of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;/white and&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt; described them&lt;loss_mask&gt; have a more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;feel&quot;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; destruction&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; period&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; particularly&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; or brilliant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facial&lt;loss_mask&gt; | . 3 &lt;s&gt;&lt;mask&gt; watched Grend&lt;mask&gt; the&lt;mask&gt; night and am compelled&lt;mask&gt; evangelical&lt;mask&gt; a Public Service Announcement. n nGrendel is another version of&lt;mask&gt;owulf, the thousand- resulted-&lt;mask&gt; Anglo-Saxon epic poem.&lt;mask&gt; SciFi channeluture a growing catalog of inoffensive&lt;mask&gt; uninterestingxs,&lt;mask&gt; the previews promised an&lt;mask&gt;authentic low-budget mini-epic, but this one refused to&lt;mask&gt;&lt;mask&gt; switch channels.&lt;mask&gt; was staggeringly, overwhelmingly, bad&lt;mask&gt; I watched in fascination and horror at the train wreck you&lt;mask&gt;&#39;t tear your eyes away from&lt;mask&gt; I reached for a notepad and managed to capture part of what I was seeing.&lt;mask&gt; following may contain spoilers or might just save your sanity&lt;mask&gt; You&#39;ve been warned. n n- Just to&lt;mask&gt; it over with, Beow&lt;mask&gt;&lt;mask&gt; warriors wore horned&lt;mask&gt;.&lt;mask&gt;&lt;mask&gt;ial issue compared to what came after. It also appears that the helmets were in a bin and handed&lt;mask&gt; whichever actor wandered by next. Fit,&lt;mask&gt; and function&lt;mask&gt; apparently irrelevant. n n- Marina Sirtis&lt;mask&gt;&lt;mask&gt; been blackmailed into doing the&lt;mask&gt; by&lt;mask&gt; Ringling Brothers, Barnum and&lt;mask&gt;&lt;mask&gt;.&lt;mask&gt; managed to avoid a red rubber nose, but the | &lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;el&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to put together&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;year&lt;loss_mask&gt;old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; has&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt; movies&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; let me&lt;loss_mask&gt;&lt;loss_mask&gt;. It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; couldn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt; contain&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; save&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; get&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;ulf&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helmets&lt;loss_mask&gt; Triv&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; actor&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; appearance&lt;loss_mask&gt;&lt;loss_mask&gt; were&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; had obviously&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Bailey circus&lt;loss_mask&gt; She&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ben&lt;loss_mask&gt;&lt;loss_mask&gt; not&lt;loss_mask&gt; be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; H&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; must have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; hadn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facilitate&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hairst&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; sideburn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;.&lt;loss_mask&gt; prove&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;shaped&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tradition&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; volume&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt; unintended focus&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; bolts&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; recoil&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . Model . Our model can be instantiated with either pretrained or random weights. We also need to be careful to pass the model the attention_mask so that the model ignores padding tokens when training. . class LMModel(nn.Module): def __init__(self, lm_model_class=None, tokenizer=None, model_name=None, config_dict=None, pretrained=False): super().__init__() self.tok=tokenizer if pretrained: self.model = lm_model_class.from_pretrained(model_name) else: self.model = lm_model_class.from_config(config_dict) self.model = self.model.module if hasattr(self.model, &quot;module&quot;) else self.model self.model.resize_token_embeddings(len(tokenizer)) def forward(self, input_ids): attention_mask = (input_ids!=self.tok.pad_token_id).type(input_ids.type()) return self.model(input_ids, attention_mask=attention_mask)[0] # only return the prediction_scores (and not hidden states and attention) . Pretrained Language Model . Lets fine-tune our pretrained Language Model. We would typically do this before training the model on our specific text. Note that here we are not training the language model head before we train the full model, but we could do so if we created a splitter and passed it to our learner . To load the pretrained HuggingFace model just use pretrained=True when calling your model: . model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=True) . Training . From here we train our model as usual using fastai. Note that we use Perplexity as our metric as it is a good measure of how well a language model is training . #collapse opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, #splitter=model_splitter, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . We check our learning rate finder . #collapse-hide learn.lr_find(suggestions=True, stop_div=False) . . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.2089296132326126) . We do some training . #collapse-hide learn.fit_one_cycle(10, lr_max=1e-4) . . epoch train_loss valid_loss accuracy perplexity time . 0 | 13.052832 | 11.317341 | 0.052725 | 82235.375000 | 00:32 | . 1 | 8.539399 | 5.935386 | 0.049121 | 378.185822 | 00:32 | . 2 | 1.660586 | 0.785814 | 0.493350 | 2.194192 | 00:32 | . 3 | 0.731211 | 0.768679 | 0.493125 | 2.156916 | 00:32 | . 4 | 0.732979 | 0.772890 | 0.492373 | 2.166016 | 00:32 | . 5 | 0.681202 | 0.695503 | 0.493711 | 2.004716 | 00:33 | . 6 | 0.660206 | 0.681334 | 0.494063 | 1.976512 | 00:33 | . 7 | 0.469388 | 0.641964 | 0.495615 | 1.900209 | 00:33 | . 8 | 0.512519 | 0.612524 | 0.494834 | 1.845082 | 00:33 | . 9 | 0.545736 | 0.625833 | 0.495205 | 1.869804 | 00:33 | . And we see how our loss progressed . Lets Look at the model&#39;s predictions . Manually checking how well our model makes predictions for masked tokens is a simple way to see how it is training . Here function get_mask_pred takes masked string given by the user and returns the topk predictions given by the model for that masked token. With it we can sanity check that our model has learned something useful! . *Note that get_mask_pred is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . #collapse def get_mask_pred(model, masked_text:str, topk:int=5): &quot;Code lightly modified from `FillMaskPipeline` in the HuggingFace Transformers library&quot; aa=fastai_tokenizer.encodes(masked_text) bb=Numericalize(vocab=tokenizer_vocab_ls)(aa) cc=AddSpecialTokens(tokenizer)(bb) outs=model(cc.unsqueeze(0).cuda()) masked_index = (cc == tokenizer.mask_token_id).nonzero().item() logits = outs[0, masked_index, :] probs = logits.softmax(dim=0) values, predictions = probs.topk(topk) result=[] for i, vv in enumerate(zip(values.tolist(), predictions.tolist())): v, p =vv tokens = cc.numpy() if i == 0: result.append({&quot;word&quot;:&quot;Input text&quot;, &quot;score&quot;: 0., &quot;token&quot;: 0, &quot;sequence&quot;: tokenizer.decode(tokens)}) tokens[masked_index] = p tokens = tokens[np.where(tokens != tokenizer.pad_token_id)] w = tokenizer.decode(p) result.append({&quot;word&quot;:w, &quot;score&quot;: v, &quot;token&quot;: p, &quot;sequence&quot;: tokenizer.decode(tokens)}) return pd.DataFrame(result) . . Here we can input our own masked sentence and see how the model does. Note that even without fine-tuning the performance below will still be very strong as the pretrained RoBERTa model is very strong. . text2 = &#39;I was walking to &lt;mask&gt; when I came across a cat on the road&#39; pred2 = get_mask_pred(model1, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I came across a cat on the road&lt;/s&gt; | . 1 school | 0.791473 | 334 | &lt;s&gt; I was walking to school when I came across a cat on the road&lt;/s&gt; | . 2 church | 0.068957 | 2352 | &lt;s&gt; I was walking to church when I came across a cat on the road&lt;/s&gt; | . 3 work | 0.068007 | 173 | &lt;s&gt; I was walking to work when I came across a cat on the road&lt;/s&gt; | . 4 breakfast | 0.007202 | 7080 | &lt;s&gt; I was walking to breakfast when I came across a cat on the road&lt;/s&gt; | . Not bad at all! Now lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model1, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 would | 0.809723 | 74 | &lt;s&gt; This very funny British comedy shows what would happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 2 might | 0.131539 | 429 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 3 could | 0.042638 | 115 | &lt;s&gt; This very funny British comedy shows what could happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 will | 0.009556 | 40 | &lt;s&gt; This very funny British comedy shows what will happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br... | . Boom, pretty darn good! Lets try the same example, replacing ancient in discovery of ancient documents . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 historical | 0.585666 | 4566 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of historical documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /... | . 2 old | 0.086817 | 793 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of old documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 3 obscure | 0.040825 | 23732 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of obscure documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 ancient | 0.035504 | 8178 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . Again, pretty solid predictions! . Train a Language Model from Scratch! . We can follow the same procedure to train a language model from scratch by using pretrained=False when seeing up our model . #collapse model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=False) opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . Training . Untrained . Again, lets look at the predictions: . model2=learn.model . text2 = &#39;I was walking to &lt;mask&gt; when I cam across a cat on the road&#39; pred2 = get_mask_pred(model2, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I cam across a cat on the road&lt;/s&gt; | . 1 the | 0.037963 | 5 | &lt;s&gt; I was walking to the when I cam across a cat on the road&lt;/s&gt; | . 2 . | 0.036504 | 4 | &lt;s&gt; I was walking to. when I cam across a cat on the road&lt;/s&gt; | . 3 , | 0.033266 | 6 | &lt;s&gt; I was walking to, when I cam across a cat on the road&lt;/s&gt; | . 4 of | 0.024381 | 9 | &lt;s&gt; I was walking to of when I cam across a cat on the road&lt;/s&gt; | . Pretty bad üëé, and see how the unconfident it is in its predictions! This doesn&#39;t perform well because we have only used 800 movie reviews to train our model, we&#39;ll need a lot more text to get decent results! . Again, just for fun, lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 , | 0.044226 | 6 | &lt;s&gt; This very funny British comedy shows what, happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 2 the | 0.035027 | 5 | &lt;s&gt; This very funny British comedy shows what the happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 3 . | 0.028172 | 4 | &lt;s&gt; This very funny British comedy shows what. happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 4 and | 0.025764 | 8 | &lt;s&gt; This very funny British comedy shows what and happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . Ewww.. . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 the | 0.036510 | 5 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of the documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 2 , | 0.035627 | 6 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of, documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . 3 and | 0.029176 | 8 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of and documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 4 . | 0.029063 | 4 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of. documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . Yuck! . Notes &amp; Hacky Bits . Notes . The validation set will change slightly due to random masking. While the data in the validaion set remains constant, different tokens will be masked each time the validation dataloader is called due to MLMTokensLabels calling a random probability each time. . If a perfectly reproducable validation set is needed then you&#39;ll probably have to create a separate transform for it&#39;s masking and set it&#39;s split_idx = 1. | . | I didn&#39;t have time to get learn.predict working. One issue that needs to be fixed is that MLMTokensLabels transform shouldn&#39;t be called on your masked input text as it will add more masks, which you don&#39;t want. . | FastHugsTokenizer will have to be modified to: . enable sequence lengths larger than the tokenizer default | to use a non-pretrained tokenizer (e.g. one you trained yourself) | . | The HuggingFace encode_plus or batch_encode_plus functions are great and I would have used them, but don&#39;t play nice with fastai multiprocessiing . | . Hacks . I had to overwrite __getitem__ in the Datasets class so that it wouldn&#39;t return a tuple as what it thinks is our x is actually our (x,y). Wrapping this tuple in anoother tuple causes headaches down the line. Creating a custom Datasets class and inheriting from it didn&#39;t work as learn.predict calls on Datasets and not the custom dataset class. . | The function get_mask_pred (used to view predictions of masked text) is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . | . Give me a shout &#128227; . Thats it for this, I hope you found it useful and learned a thing or two. If you have any questions or would like to get in touch you can find me on Twitter @mcgenergy .",
            "url": "https://www.ntentional.com/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "relUrl": "/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "date": " ‚Ä¢ Apr 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "FastHugs: Sequence Classification with Transformers and Fastai",
            "content": "All FastHugs code can be found in my FastHugs GitHub . Things You Might Like (&#10084;&#65039; ?) . FastHugsTokenizer: A tokenizer wrapper than can be used with fastai-v2&#39;s tokenizer. . FastHugsModel: A model wrapper over the HF models, more or less the same to the wrapper&#39;s from HF fastai-v1 articles mentioned below . Padding: Padding settings for the padding token index and on whether the transformer prefers left or right padding . Model Splitters: Functions to split the classification head from the model backbone in line with fastai-v2&#39;s new definition of Learner (in splitters.py . Housekeeping . Pretrained Transformers only for now &#128528; . Initially, this notebook will only deal with finetuning HuggingFace&#39;s pretrained models. It covers BERT, DistilBERT, RoBERTa and ALBERT pretrained classification models only. These are the core transformer model architectures where HuggingFace have added a classification head. HuggingFace also has other versions of these model architectures such as the core model architecture and language model model architectures. . If you&#39;d like to try train a model from scratch HuggingFace just recently published an article on How to train a new language model from scratch using Transformers and Tokenizers. Its well worth reading to see how their tokenizers library can be used, independent of their pretrained transformer models. . Read these first &#128071; . This notebooks heavily borrows from this notebook , which in turn is based off of this tutorial and accompanying article. Huge thanks to Melissa Rajaram and Maximilien Roberti for these great resources, if you&#39;re not familiar with the HuggingFace library please given them a read first as they are quite comprehensive. . fastai-v2 &#9996;&#65039;2&#65039;&#8419; . This paper introduces the v2 version of the fastai library and you can follow and contribute to v2&#39;s progress on the forums. This notebook uses the small IMDB dataset and is based off the fastai-v2 ULMFiT tutorial. Huge thanks to Jeremy, Sylvain, Rachel and the fastai community for making this library what it is. I&#39;m super excited about the additinal flexibility v2 brings. üéâ . Dependencies &#128229; . If you haven&#39;t already, install HuggingFace&#39;s transformers library with: pip install transformers . #collapse path = untar_data(URLs.IMDB_SAMPLE) model_path = Path(&#39;models&#39;) df = pd.read_csv(path/&#39;texts.csv&#39;) . . FastHugs Tokenizer . This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used. . #collapse class FastHugsTokenizer(): &quot;&quot;&quot; transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class model_name : model type set by the user max_seq_len : override default sequence length, typically 512 for bert-like models sentence_pair : whether a single sentence (sequence) or pair of sentences are used &quot;&quot;&quot; def __init__(self, transformer_tokenizer=None, model_name = &#39;roberta&#39;, max_seq_len=None, sentence_pair=False, **kwargs): self.tok, self.max_seq_len=transformer_tokenizer, max_seq_len if self.max_seq_len: if self.max_seq_len&lt;=self.tok.max_len: print(&#39;WARNING: max_seq_len is larger than the model default transformer_tokenizer.max_len&#39;) if sentence_pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence) self.model_name = model_name def do_tokenize(self, o:str): &quot;&quot;&quot;Limits the maximum sequence length and add the special tokens&quot;&quot;&quot; CLS, SEP=self.tok.cls_token, self.tok.sep_token # Add prefix space, depending on model selected if &#39;roberta&#39; in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len] else: tokens = self.tok.tokenize(o)[:self.max_seq_len] # order of &#39;tokens&#39;, &#39;SEP&#39; and &#39;CLS&#39; if &#39;xlnet&#39; in model_name: return tokens + [SEP] + [CLS] else: return [CLS] + tokens + [SEP] def __call__(self, items): for o in items: yield self.do_tokenize(o) . . FastHugs Model . This nn.module wraps the pretrained transformer model and initialises it with its config file. . The forward of this module is taken straight from Melissa&#39;s notebook above and its purpose is to create the attention mask and grab only the logits from the output of the model (as the HappyFace transformer models also output the loss). . #collapse class FastHugsModel(nn.Module): &#39;Inspired by https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers/data&#39; def __init__(self, transformer_cls, config_dict, n_class, pretrained=True): super(FastHugsModel, self).__init__() self.config = config_dict self.config._num_labels = n_class # load model if pretrained: self.transformer = transformer_cls.from_pretrained(model_name, config=self.config) else: self.transformer = transformer_cls.from_config(config=self.config) def forward(self, input_ids, attention_mask=None): attention_mask = (input_ids!=1).type(input_ids.type()) logits = self.transformer(input_ids, attention_mask = attention_mask)[0] return logits . . The HuggingFace bit . Define HuggingFace Model + Config . AutoModelForSequenceClassification will define our model. When this is padded to the FastHugsModel class below then model will be instantiated and the weights downloaded (if you are using a pretrained model) | AutoConfig will define the model architecture and settings | model_name is the model architecture (and optionally model weights) you&#39;d like to use. Models tested: bert-base-uncased, roberta-base, distilbert-base-cased, albert-base-v2 | You can find all of HuggingFace&#39;s models at https://huggingface.co/models, although not all of them are supported by AutoModel,AutoConfig and AutoTokenizer | . | . model_name = &#39;roberta-base&#39; model_class = AutoModelForSequenceClassification config_dict = AutoConfig.from_pretrained(model_name) . HuggingFace Config changes . Some config settings can be changed even when using pretrained weights. For example in the FastHugsModel class below _num_labels is set when the model (pretrained or not) is instantiated, depending on how many classes you have in your dataloader. . When creating a non-pretrained model you can load a config with: . config_dict = AutoConfig.for_model(model_name) . Alternatively you could load a pretrained config and modify that. For example if your are not using a pretrained model you can change the size of your input embeddings by changing config_dict.max_position_embeddings = 1024. (This won&#39;t work when using pretrained models as the pre-trained weights need the default max_position_embeddings size). . HuggingFace Tokenizer &amp; Vocab . AutoTokenizer will load our tokenizer and enable us grab our vocab | . fastai expects vocab to be a list, however HuggingFace&#39;s get_vocab returns a token : index dict. We need to convert this dict to a list to be able to use it in fastai . tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer_vocab=tokenizer.get_vocab() tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])] len(tokenizer_vocab_ls) . 50265 . The Fastai bit . Get fastai model splitter function . In order to be able to fine-tune our classifier head we need to first split the HuggingFace model&#39;s classifier head from the body. These functions are dependent on the specific architecture and can be found in splitter.py of this repo . splitter_nm = model_name.split(&#39;-&#39;)[0] + &#39;_cls_splitter&#39; model_splitter = splitters[splitter_nm] . fasthugstok and our tok_fn . Lets incorporate the tokenizer from HuggingFace into fastai-v2&#39;s framework by specifying a function called fasthugstok that we can then pass on to Tokenizer.from_df. (Note .from_df is the only method I have tested) . Max Seqence Length . max_seq_len is the longest sequece our tokenizer will output. We can also the max sequence length for the tokenizer by changing max_seq_len. It uses the tokenizer&#39;s default, typically 512. 1024 or even 2048 can also be used depending on your GPU memory. Note when using pretrained models you won&#39;t be able to use a max_seq_len larger than the default. . max_seq_len = None sentence_pair=False fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, max_seq_len=max_seq_len, sentence_pair=sentence_pair) . Set up fastai&#39;s Tokenizer.from_df, we pass rules=[] to override fastai&#39;s default text processing rules . fastai_tokenizer = Tokenizer.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, tok_func=fasthugstok, rules=[]) . Setup Dataloaders . Create Dataset . Lets add our custom tokenizer function (tok_fn) and transformer_vocab here . splits = ColSplitter()(df) x_tfms = [attrgetter(&quot;text&quot;), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls)] dsets = Datasets(df, splits=splits, tfms=[x_tfms, [attrgetter(&quot;label&quot;), Categorize()]], dl_type=SortedDL) . Padding . We need to make sure our padding is done correctly as some transformer models prefer padding on the left while others prefer it on the right. tokenizer.padding_side will tell us which side is correct. e.g., BERT, Roberta prefers padding to the right, so we set pad_first=False . #collapse def transformer_padding(tokenizer=None, max_seq_len=None, sentence_pair=False): if tokenizer.padding_side == &#39;right&#39;: pad_first=False else: pad_first=True max_seq_len = ifnone(max_seq_len, tokenizer.max_len) return partial(pad_input_chunk, pad_first=pad_first, pad_idx=tokenizer.pad_token_id, seq_len=max_seq_len) . . Dataloaders . bs = 4 padding=transformer_padding(tokenizer) dls = dsets.dataloaders(bs=bs, before_batch=[padding]) . dls.show_batch(max_n=3, trunc_at=60) . text category . 0 &lt;s&gt; ƒ†I ƒ†was ƒ†fortunate ƒ†enough ƒ†to ƒ†meet ƒ†George ƒ†Pal ƒ†( and ƒ†still ƒ†have ƒ†my ƒ†DS : TM OB ƒ†poster ƒ†aut ographed ƒ†by ƒ†him ) ƒ†at ƒ†a ƒ†convention ƒ†shortly ƒ†after ƒ†the ƒ†release , ƒ†and ƒ†asked ƒ†him ƒ†why ƒ†he ƒ†chose ƒ†to ƒ†do ƒ†the ƒ†film ƒ†&quot; camp &quot;. ƒ†Before ƒ†he ƒ†could ƒ†answer , ƒ†two ƒ†studio ƒ†fl acks ƒ†intercepted ƒ†and ƒ†lect ured ƒ†me ƒ†on | negative | . 1 &lt;s&gt; ƒ†D ressed ƒ†to ƒ†Kill ƒ†starts ƒ†off ƒ†with ƒ†Kate ƒ†Miller ƒ†( Ang ie ƒ†Dickinson ) ƒ†having ƒ†a ƒ†sexually ƒ†explicit ƒ†nightmare , ƒ†later ƒ†on ƒ†that ƒ†day ƒ†she ƒ†visits ƒ†her ƒ†psychiatrist ƒ†Dr . ƒ†Robert ƒ†Elliott ƒ†( Michael ƒ†C aine ) ƒ†for ƒ†a ƒ†session ƒ†in ƒ†which ƒ†she ƒ†admits ƒ†to ƒ†be ƒ†sexually ƒ†frustrated ƒ†&amp; ƒ†un ful filled ƒ†in ƒ†her ƒ†current ƒ†marriage . ƒ†Kate ƒ†then | positive | . 2 &lt;s&gt; ƒ†SHALL OW ƒ†G RA VE ƒ†begins ƒ†with ƒ†either ƒ†a ƒ†tribute ƒ†or ƒ†a ƒ†rip ƒ†off ƒ†of ƒ†the ƒ†shower ƒ†scene ƒ†in ƒ†PS Y CHO . ƒ†( I &#39;m ƒ†leaning ƒ†toward ƒ†rip ƒ†off .) ƒ†After ƒ†that ƒ†it ƒ†gets ƒ†worse ƒ†and ƒ†then ƒ†surprisingly ƒ†gets ƒ†better , ƒ†almost ƒ†to ƒ†the ƒ†point ƒ†of ƒ†being ƒ†original . ƒ†Bad ƒ†acting ƒ†and ƒ†amateur ish ƒ†directing ƒ†bog ƒ†down ƒ†a | negative | . (Alternatively) Factory dataloader . Here we set: . tok_tfm=tok_fn to use our HF tokenizer | text_vocab=transformer_vocab to load our pretrained vocab | before_batch=transformer_padding(transformer_tokenizer) to use our custom padding function | . fct_dls = TextDataLoaders.from_df(df, text_col=&quot;text&quot;, tok_tfm=fastai_tokenizer, text_vocab=tokenizer_vocab_ls, before_batch=[padding], label_col=&#39;label&#39;, valid_col=&#39;is_valid&#39;, bs=bs) . fct_dls.show_batch(max_n=3, trunc_at=60) . text category . 0 &lt;s&gt; ƒ†I ƒ†was ƒ†fortunate ƒ†enough ƒ†to ƒ†meet ƒ†George ƒ†Pal ƒ†( and ƒ†still ƒ†have ƒ†my ƒ†DS : TM OB ƒ†poster ƒ†aut ographed ƒ†by ƒ†him ) ƒ†at ƒ†a ƒ†convention ƒ†shortly ƒ†after ƒ†the ƒ†release , ƒ†and ƒ†asked ƒ†him ƒ†why ƒ†he ƒ†chose ƒ†to ƒ†do ƒ†the ƒ†film ƒ†&quot; camp &quot;. ƒ†Before ƒ†he ƒ†could ƒ†answer , ƒ†two ƒ†studio ƒ†fl acks ƒ†intercepted ƒ†and ƒ†lect ured ƒ†me ƒ†on | negative | . 1 &lt;s&gt; ƒ†**** Don &#39;t ƒ†read ƒ†this ƒ†review ƒ†if ƒ†you ƒ†want ƒ†the ƒ†shocking ƒ†conclusion ƒ†of ƒ†&quot; The ƒ†Cr ater ƒ†Lake ƒ†Monster &quot; ƒ†to ƒ†be ƒ†a ƒ†total ƒ†surprise **** &lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; A ƒ†clay m ation ƒ†pl es ios aur ƒ†rises ƒ†from ƒ†the ƒ†depths ƒ†of ƒ†Cr ater ƒ†Lake ƒ†to ƒ†wre ak ƒ†havoc ƒ†on ƒ†a ƒ†group ƒ†of ƒ†local ƒ†red ne | negative | . 2 &lt;s&gt; ƒ†This ƒ†is ƒ†the ƒ†last ƒ†of ƒ†four ƒ†sw ash buck lers ƒ†from ƒ†France ƒ†I &#39;ve ƒ†scheduled ƒ†for ƒ†viewing ƒ†during ƒ†this ƒ†Christmas ƒ†season : ƒ†the ƒ†others ƒ†( in ƒ†order ƒ†of ƒ†viewing ) ƒ†were ƒ†the ƒ†un inspired ƒ†THE ƒ†BLACK ƒ†T UL IP ƒ†( 1964 ; ƒ†from ƒ†the ƒ†same ƒ†director ƒ†as ƒ†this ƒ†one ƒ†but ƒ†not ƒ†nearly ƒ†as ƒ†good ), ƒ†the ƒ†surprisingly ƒ†effective ƒ†L | positive | . Create our learner . opt_func = partial(Adam, decouple_wd=True) loss = LabelSmoothingCrossEntropy() fasthugs_model = FastHugsModel(transformer_cls=model_class, config_dict=config_dict, n_class=dls.c, pretrained=True) learn = Learner(dls, fasthugs_model, opt_func=opt_func, splitter=model_splitter, loss_func=loss, metrics=[accuracy]).to_fp16() . Stage 1 training . Lets freeze the model backbone and only train the classifier head. freeze_to(1) means that only the classifier head is trainable . learn.freeze_to(1) . Lets find a learning rate to train our classifier head . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=9.999999747378752e-07, lr_steep=0.10000000149011612) . learn.recorder.plot_lr_find() plt.vlines(9.999e-7, 0.65, 1.1) plt.vlines(0.10, 0.65, 1.1) . &lt;matplotlib.collections.LineCollection at 0x7f7582802450&gt; . learn.fit_one_cycle(3, lr_max=1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.692172 | 0.653464 | 0.550000 | 00:07 | . 1 | 0.573368 | 0.591558 | 0.635000 | 00:07 | . 2 | 0.522324 | 0.533852 | 0.810000 | 00:07 | . learn.save(&#39;roberta-fasthugs-stg1-1e-3&#39;) . learn.recorder.plot_loss() . Stage 2 training . And now lets train the full model with differential learning rates . learn.unfreeze() . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=0.03981071710586548) . learn.recorder.plot_lr_find() plt.vlines(6.30e-8, 0.6, 1.2) plt.vlines(0.039, 0.6, 1.2) . &lt;matplotlib.collections.LineCollection at 0x7f7582464490&gt; . learn.fit_one_cycle(3, lr_max=slice(1e-5, 1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.425518 | 0.354511 | 0.910000 | 00:31 | . 1 | 0.278425 | 0.372734 | 0.910000 | 00:32 | . 2 | 0.272590 | 0.366681 | 0.925000 | 00:31 | . learn.save(&#39;roberta-fasthugs-stg2-3e-5&#39;) . learn.recorder.plot_loss() . Lets Look at the model&#39;s predictions . learn.predict(&quot;This was a really good movie, i loved it&quot;) . (&#39;positive&#39;, tensor(1), tensor([0.1498, 0.8502])) . from fastai2.interpret import * #interp = Interpretation.from_learner(learn) interp = ClassificationInterpretation.from_learner(learn) . interp.plot_top_losses(3) . input target predicted probability loss . 0 &lt;s&gt; ƒ†This ƒ†movie ƒ†is ƒ†horrible - ƒ†in ƒ†a ƒ†&#39; so ƒ†bad ƒ†it &#39;s ƒ†good &#39; ƒ†kind ƒ†of ƒ†way .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; The ƒ†storyline ƒ†is ƒ†re h ashed ƒ†from ƒ†so ƒ†many ƒ†other ƒ†films ƒ†of ƒ†this ƒ†kind , ƒ†that ƒ†I &#39;m ƒ†not ƒ†going ƒ†to ƒ†even ƒ†bother ƒ†describing ƒ†it . ƒ†It &#39;s ƒ†a ƒ†sword / s or cery ƒ†picture , ƒ†has ƒ†a ƒ†kid ƒ†hoping ƒ†to ƒ†realize ƒ†how ƒ†important ƒ†he ƒ†is ƒ†in ƒ†this ƒ†world , ƒ†has ƒ†a ƒ†&quot; nom adic &quot; ƒ†adventurer , ƒ†an ƒ†evil ƒ†aide / s orce rer , ƒ†a ƒ†princess , ƒ†a ƒ†hairy ƒ†creature .... you ƒ†get ƒ†the ƒ†point .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; The ƒ†first ƒ†time ƒ†I ƒ†caught ƒ†this ƒ†movie ƒ†was ƒ†during ƒ†a ƒ†very ƒ†harsh ƒ†winter . ƒ†I ƒ†don &#39;t ƒ†know ƒ†why ƒ†I ƒ†decided ƒ†to ƒ†continue ƒ†watching ƒ†it ƒ†for ƒ†an ƒ†extra ƒ†five ƒ†minutes ƒ†before ƒ†turning ƒ†the ƒ†channel , ƒ†but ƒ†when ƒ†I ƒ†caught ƒ†site ƒ†of ƒ†Gulf ax | positive | negative | 0.970687747001648 | 3.354750156402588 | . 1 &lt;s&gt; ƒ†In ƒ†17 th ƒ†Century ƒ†Japan , ƒ†there ƒ†lived ƒ†a ƒ†samurai ƒ†who ƒ†would ƒ†set ƒ†the ƒ†standard ƒ†for ƒ†the ƒ†ages . ƒ†His ƒ†name ƒ†was ƒ†May eda . ƒ†He ƒ†is ƒ†sent ƒ†on ƒ†an ƒ†epic ƒ†journey ƒ†across ƒ†the ƒ†world ƒ†to ƒ†acquire ƒ†5 , 000 ƒ†mus cats ƒ†from ƒ†the ƒ†King ƒ†of ƒ†Spain . ƒ†Whilst ƒ†at ƒ†sea ƒ†a ƒ†violent ƒ†storm ƒ†swall ows ƒ†their ƒ†precious ƒ†gold ƒ†intended ƒ†to ƒ†buy ƒ†the ƒ†weapons ƒ†and ƒ†almost ƒ†takes ƒ†their ƒ†lives . ƒ†May eda ƒ†must ƒ†battle ƒ†all ƒ†odds ƒ†to ƒ†survive ƒ†and ƒ†the ƒ†secure ƒ†the ƒ†fate ƒ†of ƒ†his ƒ†beloved ƒ†Japan . ƒ†Shogun ƒ†May eda ƒ†is ƒ†a ƒ†multi ƒ†million ƒ†dollar ƒ†action ƒ†adventure ƒ†epic ƒ†set ƒ†across ƒ†three ƒ†continents .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; Star ring ƒ†cinema ƒ†legends ƒ†Sho ƒ†Kos ugi ƒ†( T ench u : ƒ†Stealth ƒ†Assassins ), ƒ†Christopher ƒ†Lee ƒ†( Star ƒ†Wars , ƒ†Lord ƒ†of ƒ†the ƒ†Rings ƒ†Trilogy ), ƒ†John ƒ†Rh ys ƒ†Davies ƒ†( Lord ƒ†of ƒ†the ƒ†Rings ƒ†Trilogy , ƒ†Indiana ƒ†Jones | negative | positive | 0.9588471055030823 | 3.033039093017578 | . 2 &lt;s&gt; ƒ†&quot; How ƒ†To ƒ†Lose ƒ†Friends ƒ†&amp; ƒ†Alien ate ƒ†People &quot; ƒ†is ƒ†not ƒ†based ƒ†on ƒ†Tiger ƒ†Woods &#39; ƒ†inf idel ities . ƒ†It ƒ†is ƒ†a ƒ†mediocre ƒ†romantic ƒ†comedy ƒ†based ƒ†on ƒ†Toby ƒ†Young &#39;s ƒ†book ƒ†on ƒ†his ƒ†experiences ƒ†working ƒ†as ƒ†a ƒ†journalist ƒ†covering ƒ†celebrities . ƒ†The ƒ†film ƒ†stars ƒ†Simon ƒ†Pe gg ƒ†as ƒ†Sidney ƒ†Young , ƒ†a ƒ†z any ƒ†British ƒ†journalist ƒ†who ƒ†takes ƒ†a ƒ†job ƒ†in ƒ†an ƒ†illustrious ƒ†celebrity ƒ†magazine ƒ†in ƒ†New ƒ†York . ƒ†Young ƒ†is ƒ†restless ƒ†in ƒ†getting ƒ†caught ƒ†up ƒ†all ƒ†type ƒ†of ƒ†shenanigans ƒ†to ƒ†alien ate ƒ†all ƒ†around ƒ†him , ƒ†hence ƒ†movie ƒ†title . ƒ†He ƒ†is ƒ†upro arious , ƒ†daring , ƒ†and ƒ†mor onic . ƒ†But ƒ†nevertheless ƒ†for ƒ†some ƒ†very ƒ†bizarre ƒ†reason , ƒ†he ƒ†is ƒ†a ƒ†somewhat ƒ†lik able ƒ†character . ƒ†Sidney ƒ†be friends ƒ†a ƒ†fellow ƒ†journalist , ƒ†the ƒ†composed ƒ†Alison ƒ†Olsen , ƒ†played ƒ†quite ƒ†adm ir ably ƒ†by ƒ†Kirst en ƒ†Dun st . ƒ†However , ƒ†Sidney ƒ†is ƒ†primarily ƒ†longing | positive | negative | 0.942081868648529 | 2.7092723846435547 | .",
            "url": "https://www.ntentional.com/nlp/training%20technique/classification/2020/04/17/fasthugs_seq_classification.html",
            "relUrl": "/nlp/training%20technique/classification/2020/04/17/fasthugs_seq_classification.html",
            "date": " ‚Ä¢ Apr 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "FixRes: 'Fixing the train-test resolution discrepancy'",
            "content": "TL;DR . The paper outlines two easy-to-implement tips to improve your image classification test results: . Do your inference on the test set at a higher resolution than your train set | Fine-tune the last layers of your CNN classifier (i.e. the linear layer(s) after your pooling layer) at the higher test resolution | Overview . This article is a quick summary of &#39;Fixing the train-test resolution discrepancy&#39; from Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv√© J√©gou from Facebook AI Research, presented at NeurIPS 2019, with additional data from the note &#39;Fixing the train-test resolution discrepancy: FixEfficientNet&#39; from the same authors . Results . Facebook AI Research (FAIR) used this technique to achieve a new SOTA result on Imagenet (88.5% top-1 accuracy) using EfficientNet (using extra data) | . . The authors also claim that it can enable faster training by training at a lower resolution while still attaining similr/better results | . Our FixEfficientNet-L2 obtains a new state-of-the-art performance on ImageNet!You can find all our new results on the FixRes additional note (https://t.co/mvY3EkGucR) and also on @paperswithcode and @sotabench(In case you missed the FixRes paper : https://t.co/2NgQcrGDk5) pic.twitter.com/WiQtJQxdgT . &mdash; Hugo Touvron (@HugoTouvron) March 23, 2020 . But Why? . Using typical training transforms such as RandomResizedCrop result in objects in training images appearing larger than they do in the test set. Have a look at the example from the paper below. . Our original image is resized to 224 x 224 before it is shown to the model. RandomResizedCrop is used to resize our training image (and add a little regularisation) while for the test image a simple center crop is taken. As a result of these different resizing methods, the size of the white horse in the top left training image is much larger than what would be shown to the model in the test set. It is this difference in object (e.g. horse) size that the authors say that their FixRes technique addresses . . In other words: . ...resizing the input images in pre-processing changes the distribution of objects sizes. Since different pre-processing protocols are used at training and testing time, the size distribution differs in the two cases. . How? - Two Tricks . Test at a Higher Resolution | Simply testing at a higher resolution should yield a performance improvement. Here, the authors show ImageNet top-1 test set accuracy trained at 224 x 224, you can see that the optimal test resolution was 288 x 288: (This behaviour was previously been shown in 2016 in &quot;Identity Mappings in Deep Residual Networks&quot;). Alternatively if you don&#39;t want to/cannot test at higher resolution, then training at a lower resolution is said to deliver the same accuracy, while enabling you to train faster (as you will be able to use a larger batch size with your smaller image resolutions) . Fine-tuning of later (classifier) layers of your CNN model | For the convolutional part of the CNN, comprising linear convolution, subsampling, ReLU, and similar layers, changing the input crop size is approximately transparent because the receptive field is unaffected by the input size. However, for classification the network must be terminated by a pooling operator (usually average pooling) in order to produce a fixed-size vector. Changing the size of the input crop strongly affects the activation statistics of this layer. . When fine-tuning, the authors recommend using test-time augmentation, not the previous training augmentation as it is simplest and performs well. Using training augmentations gave only slightly better results. . Similarity to Fast.ai&#39;s Progressive Resizing . Interestingly this technique is a little similar to Progressive Resizing, first espoused in the fast.ai deep learning course. The idea behind Progressive Resizing is that you first train at a lower resolution before increasing resolution and training again, albeit you&#39;re always training the entire network as opposed fine-tuning the classifier layers as described above. Nevertheless, it makes me wonder if both the FixRes and Progressive Resizing training techniques work via correcting for the same Train/Test object size mis-match? . Any thoughts, comments, suggestions I&#39;d love to hear from you @mcgenergy on Twitter üòÉ .",
            "url": "https://www.ntentional.com/papers/training%20technique/classification/2020/04/15/fixres.html",
            "relUrl": "/papers/training%20technique/classification/2020/04/15/fixres.html",
            "date": " ‚Ä¢ Apr 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Morgan (Me)",
          "content": "Current on a self-directed learning journey, diving deep into ML and loving it . Previously at Facebook and before then working in electricity trading at Gaelectric and Danske Commodities . I really enjoy ML talk, give me a shout on @mcgenergy on Twitter or on LinkedIn or have a look at my latest work (such as HuggingFace, a bridge between between fastai and the HuggingFace library) on my Github . .",
          "url": "https://www.ntentional.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": ". Welcome! My latest articles are below, if you‚Äôd like to get in touch, find me at @mcgenergy on Twitter . Latest Articles üëá .",
          "url": "https://www.ntentional.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://www.ntentional.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}