{
  
    
        "post0": {
            "title": "Comprehensive Language Model Fine Tuning, Part 1: ü§ó Datasets library [Updated]",
            "content": "This post has been updated to show how to use HuggingFace&#39;s normalizers functions for your text pre-processing . In the following post, I&#39;ll cover the following using the HuggingFace Datasets libray: . Loading data, single or multiple files, csv, txt or dataframes, train/test splits | Processing data with 11 text processing functions | Tokenizing data for use with MobileBERT | Saving processed data to disk | Datasets tips and tricks along the way . Note: Click the colab button to open this notebook in Google Colab and run it end to end. This script was written with Transformers 3.3.1, Datasets 1.1 and Pytorch 1.6 | . I would love to hear your feedback, what could have been written better or clearer, let me know what you think on twitter: @mcgenergy . !transformers-cli env . Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points. - `transformers` version: 3.3.1 - Platform: Darwin-19.6.0-x86_64-i386-64bit - Python version: 3.7.5 - PyTorch version (GPU?): 1.4.0 (False) - Tensorflow version (GPU?): not installed (NA) - Using GPU in script?: &lt;fill in&gt; - Using distributed or parallel set-up in script?: &lt;fill in&gt; . HuggingFace Datasets Library . Why Should I use this &quot;Datasets&quot; library? . Lets see what the docs have to say: . Built-in interoperability with Numpy, Pandas, PyTorch and Tensorflow 2 Lightweight and fast with a transparent and pythonic API Strive on large datasets . | ü§óDatasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped on drive by default. . | Smart caching:never wait for your data to process several times &gt; - ü§óDatasets currently provides access to ~100 NLP datasets and ~10 evaluation metrics and is designed to let the community easily add and share new datasets and evaluation metrics. . | You can browse the full set of datasets with the live ü§óDatasets viewer . | . My fav . For me personally, I am irrationally fond of this library. It just has so many useful features for handling your text data! I have really enjoyed the speed of data processing and the fact that caching means that running your processing a second time is lightening fast! I&#39;ve spent about 6 weeks working with it and I feel I&#39;ve only scratched the surface of what it can do in some areas. . So, huge kudos to the team working on Datasets, the library and docs are now really great! But enough of what I think, lets get stuck in some data processing woop woop! . Lets Go &#128678; . Lets start our guide to using the Datasets library to get your data ready to train. Note that a couple of the examples in this post are taken from the ü§ó Datasets docs, becasue &quot;why fix it if it ain&#39;t broken!&quot;. . To start, lets install the library with a handy to remember pip install: . !pip install datasets --upgrade . Loading our Data . Now we have the library, lets load a dataset. If we are loading from one or more .txt or .csv files we can load like so: . # Single file dataset = load_dataset(&#39;text&#39;, data_files=&#39;my_file.txt&#39;) # Multiple files dataset = load_dataset(&#39;csv&#39;, data_files=[&#39;my_file_1.csv&#39;, &#39;my_file_2.csv&#39;, &#39;my_file_3.csv&#39;]) . Train/Test Split . Train/Test Split by File . If we would like to define our Train/Test split there are a few differant ways to do that. If your training data is already split by file we can do the following: . dataset = load_dataset(&#39;csv&#39;, data_files={&#39;train&#39;: [&#39;my_train_file_1.csv&#39;, &#39;my_train_file_2.csv&#39;], &#39;test&#39;: &#39;my_test_file.csv&#39;}) . Splitting a Single FIle . Alternatively we can split a single file ourselves. Lets grab some Shakespeare text from Andrej Karpathy. Because this is a sinlge file, lets do a 80/20 train/test split . # collapse-hide !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt . . We can see that after loading, this dataset contains a DatasetDict with a sinlge key called train, which in turn has Dataset object with a sinlge column called text, with 32,777 rows of text: . #collapse-hide full_ds = datasets.load_dataset(&#39;text&#39;, data_files=&#39;input.txt&#39;) full_ds . . DatasetDict({&#39;train&#39;: Dataset(features: {&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)}, num_rows: 40000)}) . We&#39;ll have to index into the dictionary with the train key and the name of the column(s) we&#39;d like to inspect the text . full_ds[&#39;train&#39;][:10][&#39;text&#39;] . [&#39;First Citizen:&#39;, &#39;Before we proceed any further, hear me speak.&#39;, &#39;&#39;, &#39;All:&#39;, &#39;Speak, speak.&#39;, &#39;&#39;, &#39;First Citizen:&#39;, &#39;You are all resolved rather to die than to famish?&#39;, &#39;&#39;, &#39;All:&#39;] . . Tip: You can specify the cache_dir when loading a dataset if the default cache in your root directory has limited disk space, for example when procesing large files on Kaggle your working directory has a 5GB limit, however ../../tmp has a much higher limit which you can use for your active session . Loading only a small section of our data file . If we only want to take a small part of the dataset to enable us to develop rapidly we can specify the number of rows we would like to load, lets take 400 rows for example. Here we use the ReadInstruction method, have a look through the docs for even more interesting ways to use this. . mini_ds = load_dataset(&#39;text&#39;, data_files=&#39;input.txt&#39;, split=ReadInstruction(&#39;train&#39;, from_=0, to=400, unit=&#39;abs&#39;)) mini_ds . Dataset(features: {&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)}, num_rows: 400) . 80/20 Split . Since this is a single block of text lets create an 80/20 train/test split for ourselves by specifying a split when loading the data, like so: split=[&#39;train[:80%]&#39;]. There are additional useful examples of splits such as, K-fold cross validation, in the docs here . train_ds = datasets.load_dataset(&#39;text&#39;, data_files=&#39;input.txt&#39;, split=[&#39;train[:80%]&#39;])[0] val_ds = datasets.load_dataset(&#39;text&#39;, data_files=&#39;input.txt&#39;, split=[&#39;train[80%:]&#39;])[0] train_ds, val_ds . (Dataset(features: {&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)}, num_rows: 32000), Dataset(features: {&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)}, num_rows: 8000)) . Selecting Specific Row Indices . If we like, we can also specify the exact rows we would like to extract using select() on an already-loaded dataset. Here we select 50 random indices from the full dataset . r = np.random.rand(50).tolist() rand_dataset = full_ds[&#39;train&#39;].select(r) rand_dataset . Dataset(features: {&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None)}, num_rows: 50) . This covers some typical ways one might want to load data, however there are many more options to explore, including loading from pandas dataframes and creating your own loading script, see the docs for more . Processing our Data . [UPDATE] See the next section below for how to use the normalizers from the HuggingFace tokenizers library to do some of this pre-processing event faster! . Now we have data loaded lets take a look at some processing options. .map() will be the main tool we&#39;ll use to apply processing functions our text. Note here are additional modifications you can make including shuffling and sorting with .shuffle() and .sort() respectively, but I&#39;ll leave those to you to explore in the docs üîé . The map Function . map applies a function to our dataset. Below you can see how to lowercase our data by passing the lower_case function to map. When applying map you can choose to feed your function a batch of items (with batched=True or a single item. You can also adjust this batch size, the default is 1000. Feeding batches can be handy when using functions like tokenizers that can efficiently processes batches. Note that the structure of the processing . def lower_case(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(e.lower()) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} train_ds = train_ds.map(lower_case, batched=True) print(&#39; &#39;.join(train_ds[&#39;text&#39;][200:203])) . note me this, good friend; your most grave belly was deliberate, not rash like his accusers, and thus answer&#39;d: . 11 Processing Functions Ready to Use with Datasets . Below are 11 useful text processing functions that you might need as part of your workflow; from html removal, to punctuation fixes, replacing username handles (e.g. twitter handles), dealing with emojis and more . #collapse-hide &#39;&#39;&#39; Below are a selection of often useful processing functions to apply to your text. As currently written, these functions require that your text column in your dataset is called &quot;text&quot; The functions are written to be able to deal with either a batch of samples being passed or a single sample being passed. Most pre-processing functions are taken from the covid-twitter-bert processing file, here: https://github.com/digitalepidemiologylab/covid-twitter-bert/blob/d5a87550bb9d2424672d1ea56c84786f462321a3/utils/preprocess.py or else from fastai&#39;s processing rules here: https://docs.fast.ai/text.core#Preprocessing-rules &#39;&#39;&#39; # compile regexes username_regex = re.compile(r&#39;(^|[^@ w])@( w{1,15}) b&#39;) url_regex = re.compile(r&#39;((www .[^ s]+)|(https?://[^ s]+)|(http?://[^ s]+))&#39;) control_char_regex = re.compile(r&#39;[ r n t]+&#39;) # Get unk character from your tokenizer of choice tokenizer = AutoTokenizer.from_pretrained(&#39;google/mobilebert-uncased&#39;) unk = tokenizer.special_tokens_map[&#39;unk_token&#39;] # processing functions def standardise_punc(example): transl_table = dict([(ord(x), ord(y)) for x, y in zip(u&quot;‚Äò‚Äô¬¥‚Äú‚Äù‚Äì-&quot;, u&quot;&#39;&#39;&#39; &quot; &quot;--&quot;)]) tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(e.translate(transl_table)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def remove_control_char(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(re.sub(control_char_regex, &#39; &#39;, e)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def remove_remaining_control_chars(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(&#39;&#39;.join(ch for ch in e if unicodedata.category(ch)[0] != &#39;C&#39;)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def remove_multi_space(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(&#39; &#39;.join(e.split())) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def remove_accented_characters(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(unidecode.unidecode(e)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def remove_unicode_symbols(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(&#39;&#39;.join(ch for ch in e if unicodedata.category(ch)[0] != &#39;So&#39;)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def lower_case(example): tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(e.lower()) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def replace_usernames(example): filler,tmp_ls = &#39;&lt;user&gt;&#39;,[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: occ = e.count(&#39;@&#39;) for _ in range(occ): e = e.replace(&#39;@&lt;user&gt;&#39;, f&#39;{filler}&#39;) e = re.sub(username_regex, filler, e) # replace other user handles by filler e = e.replace(filler, f&#39; {filler} &#39;) # add spaces between, and remove double spaces again e = &#39; &#39;.join(e.split()) tmp_ls.append(e) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def replace_urls(example): filler,tmp_ls = &#39;&lt;url&gt;&#39;,[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: occ = e.count(&#39;www.&#39;) + e.count(&#39;http:&#39;) + e.count(&#39;https:&#39;) for _ in range(occ): e = re.sub(url_regex, filler, e) # replace other urls by filler e = e.replace(filler, f&#39; {filler} &#39;) # add spaces between, and remove double spaces again e = &#39; &#39;.join(e.split()) tmp_ls.append(e) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def asciify_emojis(example): &quot;&quot;&quot; Converts emojis into text aliases. E.g. üëç becomes :thumbs_up: For a full list of text aliases see: https://www.webfx.com/tools/emoji-cheat-sheet/ &quot;&quot;&quot; tmp_ls = [] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(emoji.demojize(e)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} def fix_html(example): &quot;From fastai: &#39;Fix messy things we&#39;ve seen in documents&#39;&quot; tmp_ls = [] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: e = e.replace(&#39;#39;&#39;, &quot;&#39;&quot;).replace(&#39;amp;&#39;, &#39;&amp;&#39;).replace(&#39;#146;&#39;, &quot;&#39;&quot;).replace(&#39;nbsp;&#39;, &#39; &#39;).replace( &#39;#36;&#39;, &#39;$&#39;).replace(&#39; n&#39;, &quot; n&quot;).replace(&#39;quot;&#39;, &quot;&#39;&quot;).replace(&#39;&lt;br /&gt;&#39;, &quot; n&quot;).replace( &#39; &quot;&#39;, &#39;&quot;&#39;).replace(&#39;&lt;unk&gt;&#39;,unk).replace(&#39; @.@ &#39;,&#39;.&#39;).replace(&#39; @-@ &#39;,&#39;-&#39;).replace(&#39;...&#39;,&#39; ‚Ä¶&#39;) tmp_ls.append(html.unescape(e)) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls[0]} else: return {&#39;text&#39;: tmp_ls} . . Tip :To keep your code a little cleaner you could compose your processing functions together into a single list, so that you would then only have to apply map once, instead of calling it multiple times. In the example below I use the compose function from the fastcore library. . # Lets add &quot;yo!&quot; to the beginning of each of our items def add_yo(example): &#39;&#39;&#39;Add &quot;yo! &quot; to each example&#39;&#39;&#39; tmp_ls=[] example[&#39;text&#39;] = _listify(example[&#39;text&#39;]) for e in example[&#39;text&#39;]: tmp_ls.append(&#39;yo! &#39; + e) if len(tmp_ls) == 1: return {&#39;text&#39;: tmp_ls} else: return {&#39;text&#39;: tmp_ls} # Compose our lower_case and add_yo functions my_processing_funcs = compose(*[lower_case, add_yo]) # Apply both functions with map train_ds = train_ds.map(my_processing_funcs, batched=True) # We have lowercased and added &quot;yo!&quot; to to each item in a single call to map! train_ds[&#39;text&#39;][200:203] . [&#39;yo! yo! note me this, good friend;&#39;, &#39;yo! yo! your most grave belly was deliberate,&#39;, &#34;yo! yo! not rash like his accusers, and thus answer&#39;d:&#34;] . #collapse-hide &#39;&#39;&#39; Do processing of the train and validation set &#39;&#39;&#39; do_batched = True train_ds = train_ds.map(fix_html, batched=do_batched) train_ds = train_ds.map(lower_case, batched=do_batched) train_ds = train_ds.map(standardise_punc, batched=do_batched) train_ds = train_ds.map(remove_control_char, batched=do_batched) train_ds = train_ds.map(remove_remaining_control_chars, batched=do_batched) train_ds = train_ds.map(remove_multi_space, batched=do_batched) train_ds = train_ds.map(remove_accented_characters, batched=do_batched) train_ds = train_ds.map(remove_unicode_symbols, batched=do_batched) train_ds = train_ds.map(replace_usernames, batched=do_batched) train_ds = train_ds.map(replace_urls, batched=do_batched) train_ds = train_ds.map(asciify_emojis, batched=do_batched) # 3-4x slower than the others val_ds = val_ds.map(fix_html, batched=do_batched) val_ds = val_ds.map(lower_case, batched=do_batched) val_ds = val_ds.map(standardise_punc, batched=do_batched) val_ds = val_ds.map(remove_control_char, batched=do_batched) val_ds = val_ds.map(remove_remaining_control_chars, batched=do_batched) val_ds = val_ds.map(remove_multi_space, batched=do_batched) val_ds = val_ds.map(remove_accented_characters, batched=do_batched) val_ds = val_ds.map(remove_unicode_symbols, batched=do_batched) val_ds = val_ds.map(replace_usernames, batched=do_batched) val_ds = val_ds.map(replace_urls, batched=do_batched) val_ds = val_ds.map(asciify_emojis, batched=do_batched) . . [UPDATE] Using normalizers from the tokenizers library for your preprocessing . The day I originally published this article, Sylvain Gugger at HuggingFace also tweeted that the tokenizers library had been updated, including updated docs Well there go half of the processing functions I mentioned üòÖ Stoked about the Normalizers and Pre-Tokenizers though, was one of the things I thought was missing (or maybe it was there and I missed it)https://t.co/gSheva9I2p . &mdash; Morgan McGuire (@mcgenergy) October 9, 2020 The new docs outline a number of &quot;normalizer&quot; functions similar to the preprocessing functions above such as lowercasing, removing white spaces etc. Turns out they were already in the library but not documented! So here is a quick update on how to use these functions as part of your pre-processing workflow. . . Available Normalizers . As of writing the normalizers available, according to the docs, are: . NFD, NFKD, NFC: NFD, NFKD and NFC unicode normalization algorithms ** . | Lowercase: Replaces all uppercase to lowercase . | Strip: Removes all whitespace characters on the specified sides (left, right or both) of the input . | StripAccents: Removes all accent symbols in unicode (to be used with NFD for consistency) . | Replace: Replaces a custom string or regexp and changes it with given content . | . ** I&#39;m not familiar with these normalizers, but if it is any help, the documentation uses NFD in their BERT tokenizer example . Applying a Normalizer to a string . We can apply a normalizer to a string by instantiating it and then calling .normalize_str, like so: . from tokenizers.normalizers import Lowercase lc = Lowercase() lc.normalize_str(&#39;ho wy iii KKKK&#39;) . &#39;ho wy iii kkkk&#39; . Applying normalizers to Datasets . Applying normalizers with .map is also quite straightforward. Note that we do not use map&#39;s batching functionality here as normalize_str requires that a string be passed to it. . tmp = train_ds.map(lambda e: {&#39;text&#39; : lc.normalize_str(e[&#39;text&#39;])}, batched=False) . Composing Normalizers and Applying to Datasets . Below we compose multiple normalizers; NFD Unicode normalization, StripAccents and Lowercase. To do this we use simply pass a list of our normalizers to tokenizers.normalizers.Sequence which applies each of the given normalizers in the order given. . import tokenizers from tokenizers.normalizers import Lowercase, NFD, StripAccents # Compose our normalizers normalizer = tokenizers.normalizers.Sequence([Lowercase(), NFD(), StripAccents()]) # Apply to string (example shamelessly copied from the tokenizers docs) print(normalizer.normalize_str(&quot;H√©ll√≤ h√¥w are √º?&quot;)) # Apply to Dataset tmp = train_ds.map(lambda e: {&#39;text&#39; : normalizer.normalize_str(e[&#39;text&#39;])}, batched=False) . hello how are u? . We can even then append this normalizer to our tokenzier! . tokenizer = AutoTokenizer.from_pretrained(&#39;google/mobilebert-uncased&#39;) tokenizer.normalizer = normalizer tokenizer.normalizer.normalize_str(&quot;H√©ll√≤ h√¥w are √º?&quot;) . &#39;hello how are u?&#39; . After processing our data with all the pre-processing/normalizer function above (click the button to show all funcs used) we&#39;re now ready for tokenization! . Tokenization . Combining HuggingFaces &quot;Fast&quot; tokenizers with the Datasets library is a real dream, the speed is something else! Here we&#39;ll instantiate a tokenizer compatible with the MobileBERT model. . Tip: HuggingFace&#8217;s AutoTokenizer class makes loading tokenizers super simple, removing the need to import the specific tokenizer class for each different model you use. AutoModel is the equivalent for model loading and we&#8217;ll use that in the next part of this series . tokenizer = AutoTokenizer.from_pretrained(&#39;google/mobilebert-uncased&#39;, return_dict=True) . &#39;lambda&#39; and &#39;map&#39; . Here we use a lambda function with map to apply the tokenizer to the train and validation sets. With HuggingFace tokenizers we have map options such as adding padding, truncating the text and setting a max_length and more. We use batched=True to take full advantages of our tokenizers ability to handle batches . Tip: In order to save precious GPU memory when training some of the -large transformer models I found that truncating the training text and setting a max length to be really useful. It worth experimenting with, if your text has very long sequences then truncation might degrade performance to an unacceptable level. In my case I was dealing with tweet data so I knew I wasn&#8217;t chopping too much from my texts. I didn&#8217;t truncate the validation text as the evaluation phases is generally less memory intensive than the training phase, so the model could handle the full text. You&#8217;ll want to consider when pursuing this strategy if you want to validate against the full text or truncated text. . Given the above, lets do our tokenization like so: . train_ds = train_ds.map(lambda e: tokenizer(e[&#39;text&#39;], padding=False, truncation=True, max_length=200), batched=True) val_ds = val_ds.map(lambda e: tokenizer(e[&#39;text&#39;], padding=True, truncation=False), batched=True) . Set Format . After tokenization, our tokenized data are all in lists. To be able to use them in our model we need to encode the data as either Pytorch or Tensorflow tensors. Here we convert the relevant columns to pytorch tensors, we can set type = &quot;tensorflow&quot; (or &quot;tf&quot;) if we are using Tensorflow here. You can see here we also specify only a subset of our columns as that is all that is needed for training our model. . train_ds.set_format(type=&#39;torch&#39;, columns=[&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) val_ds.set_format(type=&#39;torch&#39;, columns=[&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) . Sweet, Now Let Me Go Training! . Our data has been loaded, processed, tokenized and formatted, you are now go for training right? Well, one more thing you might want to think about before jumping into your modelling is if you need to use your data on different machines... . Saving and Loading Data . If you typically only use one machine consistently there is probably no need to save your data as Datasets keeps a cache of everything you have done to it. . However if your processing takes a significant amount of time and you need to move your data between machines, if you are using Kaggle notebooks, then I recommend saving your data for easy loading like so: . train_ds.save_to_disk(&#39;20M_processed_tokenized_pt_train_dataset&#39;) . You can then easily load your data again like so: . train_ds = load_from_disk(&#39;20M_processed_tokenized_pt_train_dataset&#39;) . Ready to Train &#127881; . Now that our data is loaded, processed, tokenized and formatted we are ready to train! Check out the next part in this series too see how how we fine-tune our Transformer Language Model! . Coming Up in Post 2: Training your Language Model Transformer with &#129303; Trainer . Coming up in Post 2: . Getting your data collator | Setting up all Training Arguments | Make sure Weights and Biases is tracking what you need | Training a MobileBERT model | Training on TPUs | Saving your model model | . Thanks for Reading This Far &#128591; . As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter: @mcgenergy .",
            "url": "https://www.ntentional.com/nlp/datasets/tokenization/processing/2020/10/09/comprehensive-datasets.html",
            "relUrl": "/nlp/datasets/tokenization/processing/2020/10/09/comprehensive-datasets.html",
            "date": " ‚Ä¢ Oct 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "YouTube Talk: AdaHessian Optimizer, With The Authors",
            "content": "I recently hosted a presentation with the authors of the AdaHessian optimizer paper, Zhewei Yao and Amir Gholami and, to the fastai community. . AdaHessian is a promising new optimizer and one of the first second-order optimizers that is becoming practical to use. I&#39;ve been working on a port of it to fastai which I hope to release shortly, I;ll post up here once I have it released. Until then, I hope you enjoy the presentation and Q&amp;A afterwards! . (Click here of the embedded link below doesn&#39;t work) . Useful links . Paper link | AdaHessian Paper repo | My Fastai Implementation, basic working version, mixed precision coming | Fastai discussion | @lessw2020, Best-Deep-Learning-Optimizers Code | @davda54 pytorch implementation | . Paper Citation . @article{yao2020adahessian, title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning}, author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W}, journal={arXiv preprint arXiv:2006.00719}, year={2020} } . Thanks for Reading &#128515; . As always, I would love to hear if you have any comments, thoughts or criticisms, you can find me on Twitter at @mcgenergy .",
            "url": "https://www.ntentional.com/papers/research/optimization/optimizer/youtube/nlp/vision/2020/09/27/adahessian_with_the_authors.html",
            "relUrl": "/papers/research/optimization/optimizer/youtube/nlp/vision/2020/09/27/adahessian_with_the_authors.html",
            "date": " ‚Ä¢ Sep 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Speed-testing HuggingFace nlp Datasets vs Fastai",
            "content": "tl;dr . Fastai&#39;s Textdataloader is well optimised and appears to be faster than nlp Datasets in the context of setting up your dataloaders (pre-processing, tokenizing, sorting) for a dataset of 1.6M tweets. However nlp Datasets caching means that it will be faster when repeating the same setup. . Speed . I started playing around with HuggingFace&#39;s nlp Datasets library recently and was blown away by the speed at which you can iterate through the data, thanks to some PyArrow wizardry its seriously fast! There is a bit of magic in the new ü§ónlp library besides giving dead-simple access to 120+ datasetsüßô‚Äç‚ôÇÔ∏èWe&#39;ve tested it with @qlhoest and loading a 17GB+ dataset like English Wikipedia only takes... 9MB in RAMüê£And you can iterate over the data at 2-3 Gbit/süöÄTry it yourselfüëá pic.twitter.com/wx7x7fzhqf . &mdash; Thomas Wolf (@Thom_Wolf) June 15, 2020 So I wondered if there was a significant speed-up to be gained by doing as much text processing as I could with this library as opposed to Fastai&#39;s default text processing. Also, after previously discovering Fastai&#39;s functionality to do faster text loading I was in the mood for further speed-ups! üí® . nlp Datasets . The nlp Datasets library is incredibly memory efficient; from the docs: . It provides a very efficient way to load and process data from raw files (CSV/JSON/text) or in-memory data (python dict, pandas dataframe) with a special focus on memory efficency and speed. As a matter of example, loading a 18GB dataset like English Wikipedia allocate 9 MB in RAM and you can iterate over the dataset at 1-2 GBit/s in python. . It also hosts 130+ common nlp research datasets AND (thanks to this pointer from Thomas Wolf on the new HuggingFace forums) I also learned that you can also easily load your own CSVs (or jsons, pandas dataframes) and bask in all of that speedy goodness, for example like below: . from nlp import load_dataset dataset = load_dataset(&#39;csv&#39;, data_files=&#39;my_file.csv&#39;) . Caching . Another great thing about nlp Datasets&#39; speed is that even if processing all of your data turns out to be slower than your current conventional processing method, the results of its processing are cached, which means that the second time around it will be much faster than your current processing method! This also applies for a new python session, i.e. if you restart your jupyter notebook! . By the way, if you&#39;re curious to learn more about PyArrow then I highly recommend Dejan Simic&#39;s post about it Hot off the press - the last few days I did some research, explored #ApacheArrow and figured out how to read &amp; write Arrow files - here is everything I learned so far:https://t.co/6E174oQ5eT . &mdash; Dejan Simic (@simicdds) June 26, 2020 . Note: If you love the sound of laptop fans spinning like sonic the hedgehog ü¶î, redhot battery packs üî• and the adrenaline üò∞ of living on the edge of pandas&#8217; capabilities as you explore, plot and manipulate your giant text datasets, then the nlp Datasets library probably isn&#8217;t for you. . Otherwise, regardless about using it for your final DL pipeline or not, nlp Datasets is definitely worth using just for the shear speed at which it can apply functions to GB&#39;s of data. . So, is it faster? Lets see! . The Setup . To find out we&#39;ll be comparing Fastai&#39;s high-level TextDataloders class to a custom dataprocessing pipeline using HuggingFace&#39;s nlp Datasets datasets library. . Fastai . This Fastai class does a bunch of different things, all by calling just 1 line of code, including: . Pre and Post Processing | Tokenization with Spacy&#39;s tokenizer, including creating a vocabulary and parallelising the tokenization | Speed optimizations including sorting data by text sample length and padding only to the longest item in the sequence, similar what was described here | Creating the train and validation dataloaders and putting them onto the GPU | . HuggingFace nlp Datasets . The nlp Datasets pipeline I wrote tries to replicate all of the core functionality of TextDataloaders as best I could. . Note: I couldn&#8217;t figure out how to parallelise the text processing with nlp Datasets, although this is probably down to my lack of experience with parallelism as opposed to a limitation of the library . Sentiment Dataset . For this experiment I used the Sentiment140 dataset, a sentiment classifcation dataset of Twitter data with 1,600,000 tweets and ~120M space-separated tokens. . For our experiment we&#39;ll test with both 10% and 100% of the dataset with a 80/20 train/val split . Experiment Settings . The timings will consist of 2 elements, an &quot;init&quot; and &quot;1 epoch&quot;. The former will covering the entire process from loading the data (already downloaded) to creating the dataloaders. The second element will simply consist of iterating through the entire training dataset once. . Init Details . The initialisation consists of: . 0) Reading the data from disk, from a csv for fastai and from a PyArrow file for nlp Datasets . Note that for nlp Datasets a train set and a validation set were downloaded separately. Previously I had used .select or .train_test_split to generate a train and val set, however using either of these methods added over 7minutes to the pre-processing time. | . 1) Applying fastai&#39;s default text pre-processing functions. These will: . - Fix various messy bits of html sometimes seen in documents - Replace repetitions at the character level, e.g. `cccc` becomes: `TK_REP 4 c` - Replace word repetitions, e.g. `cow cow cow cow` becomes: `TK_WREP 4 cow` - Add spaces around / and # - Remove multiple spaces - Replace tokens in ALL CAPS by their lower version and add TK_UP before. - Replace characters in ALL CAPS by their lower version and add TK_UP before. - Lowercases everything . 2) Tokenizing based on Spacy&#39;s tokenizer (fastai&#39;s default) . 3) Applying a post-processing rule which replaces embedded spaces in a token with unicode line char to allow for split/join . 4) Adding padding and sorting the samples by length for more efficient gpu usage . 5) Creation of the train and validation dataloaders . Results . 10% Data . Results are...mixed! While the Fastai convienience function had a faster init (48s vs 71s), the PyArrow-backed nlp run through a single epoch was significantly faster (11s vs 14s). . 160K ROWS: Init (s) 1 epoch (s) 1 mini-batch [bs=64] (ms) . Fastai | 124 | 14.3 | 7.4 | . Fastai w/sorted | 48.1 | 14.3 | 7.4 | . nlp | 71.2 | 11.3 | 5.6 | . 100% Data . For the full dataset of 1.6M tweets, Fastai dramatically outperforms nlp Datasets. . 1.6M ROWS: Init (s) 1 epoch (s) . Fastai w/sorted | 484 | 142 | . nlp | 1034 | 323 | . But maybe nlp Datasets might be faster? . Given the large difference in speed on the full dataset, I am suspicious about some parts of my implementation, specifically sorting the entire dataset which takes takes 416s. Do I need to sort the full dataset? Maybe there is a smarter way to serve up a sorted dataset similar to how Fastai achieves it? Removing sorting brings nlp Datasets timing down to 618s, still slower than Fastai&#39;s 484s. Possibly with parallelism Fastai could be matched? . In addition, nlp Datasets&#39;s caching means that the second time around you do your pre-processing it will be significantly faster. . RAM Usage . Note that Fastai was faster on the full 1.6M row dataset, but I also had to delete the pandas dataframe used to calculate the text lengths as it was taking up too much RAM and causing my dataloaders to fail. On the other hand, nlp Datasets won&#39;t incur this issue as it is reading directly from disk. So even if Fastai is faster, nlp Datasets could still save you a few headaches when deadling with large datasets. . To End . While not as definitive as I would like, it appears Fastai&#39;s TextDataloaders convenience function is faster than nlp Datasets for datasets of this scale for an intial setup. The question of parallelism remains. nlp Datasets caching will mean that if going through the same setup a second time it will be significantly faster. . I still plan on using the nlp Datasets library for one-off processing and experimentation as I think it offers incredible speed and flexiblility, the team at HuggingFace have done amazing work here. . Thanks for Reading &#128515; . As always, I would love to hear if you have any comments, thoughts or criticisms, you can find me on Twitter at @mcgenergy . [Appendix] . Code: . For those curious, you can peek the code used in testing this below üëá . #collapse-hide # Imports %reload_ext autoreload %autoreload 2 from fastai2.basics import * from fastai2.text.all import * # from fastai2.callback.all import * # from fastai2.data.transforms import RandomSplitter from fastai2.text.core import defaults from nlp import load_dataset import spacy,html from spacy.symbols import ORTH import timeit import gc . . Fastai Testing . Init timing: . #collapse-hide #%%timeit -n 1 -r 3 # Download data and save as csv # senti_dataset = load_dataset(&#39;sentiment140&#39;, split=&#39;train[:100%]&#39;, download_mode=&#39;reuse_cache_if_exists&#39;) # df = senti_dataset.data.to_pandas() # df.to_csv(&#39;sentiment140.csv&#39;) # Read data; the first 10% of the sentiment140 dataset, extraced from the `nlp` library and saved as a csv #fn_10pct = &#39;sentiment140_10pct.csv&#39; fn = &#39;sentiment140.csv&#39; df = pd.read_csv(fn, index_col=None) # SORT: Calculate text sample lengths df[&#39;word_count&#39;] = df[&#39;text&#39;].str.split().map(len) res=df[&#39;word_count&#39;].values # Create Dataloaders dls = TextDataLoaders.from_csv(path=&#39;.&#39;, csv_fname=fn, valid_pct=0.2, bs=64, text_col=&#39;text&#39;, label_col=&#39;sentiment&#39; , res=res) . . 1 epoch timing . #collapse-hide del df, res gc.collect() # Do 1 pass of the training dataloader s = &quot;&quot;&quot;for b in dls.train: pass &quot;&quot;&quot; time = timeit.timeit(stmt=s, number=1, globals=globals()); time time, time / len(dls.train) . . (143.04122078799992, 0.007152061039399996) . HuggingFace nlp Datasets Testing . Tokenizer, Numericalizer and Padding functions, modified from Fastai&#39;s functions . #collapse-hide class SpacyTokenizerNLP(): &quot;Spacy tokenizer for `lang`&quot; def __init__(self, lang=&#39;en&#39;, special_toks=None, buf_sz=5000): self.special_toks = ifnone(special_toks, defaults.text_spec_tok) nlp = spacy.blank(lang, disable=[&quot;parser&quot;, &quot;tagger&quot;, &quot;ner&quot;]) for w in self.special_toks: nlp.tokenizer.add_special_case(w, [{ORTH: w}]) self.pipe,self.buf_sz = nlp.pipe,buf_sz def encodes(self, items): tmp = [list(doc) for doc in self.pipe(items, batch_size=self.buf_sz)] return {&#39;tok_text_pre&#39;: [list(str(t) for t in l) for l in tmp]} def make_vocab(count, min_freq=3, max_vocab=60000, special_toks=None): &quot;Create a vocab of `max_vocab` size from `Counter` `count` with items present more than `min_freq`&quot; vocab = [o for o,c in count.most_common(max_vocab) if c &gt;= min_freq] special_toks = ifnone(special_toks, defaults.text_spec_tok) for o in reversed(special_toks): #Make sure all special tokens are in the vocab if o in vocab: vocab.remove(o) vocab.insert(0, o) vocab = vocab[:max_vocab] return vocab + [f&#39;xxfake&#39; for i in range(0, 8-len(vocab)%8)] class NumericalizeNLP(Transform): &quot;Reversible transform of tokenized texts to numericalized ids&quot; def __init__(self, dsets=None, vocab=None, min_freq=3, max_vocab=60000, special_toks=None, pad_tok=None): store_attr(self, &#39;vocab,min_freq,max_vocab,special_toks,pad_tok&#39;) self.vocab, self.special_toks, self.min_freq, self.max_vocab = vocab, special_toks, min_freq, max_vocab self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)}) if self.vocab is None: count = Counter(p for o in dsets for p in o) self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab, special_toks=self.special_toks) self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != &#39;xxfake&#39;}) def encodes_nlp(self, o): return TensorText(tensor([self.o2i [o_] for o_ in o])) def encodes_nlp(self, b): return {&#39;toks&#39; : [[self.o2i[o_] for o_ in oo] for oo in b[&#39;tok_text&#39;]]} # Padding functions def pad_seq(x, max_batch_len, pad_idx): pad = x.new_zeros(max_batch_len-x.size(0))+pad_idx return torch.cat([x, pad]) # Pad up to longest item in the batch and put batch on the GPU def pad_batch(batch=None, pad_token_id=1): batch_inputs = list() max_size = max([len(item[&#39;toks&#39;]) for item in batch]) for item in batch: batch_inputs += [pad_seq(item[&#39;toks&#39;], max_size, pad_token_id)] return torch.stack(batch_inputs).cuda() . . Download and define processing functions . #collapse-hide # Download text, a clean version of the dataset is downloaded (not included in the timings) train_senti_dataset = load_dataset(&#39;sentiment140&#39;, split=&#39;train[:80%]&#39;, download_mode=&#39;reuse_cache_if_exists&#39;) val_senti_dataset = load_dataset(&#39;sentiment140&#39;, split=&#39;train[80:100%]&#39;, download_mode=&#39;reuse_cache_if_exists&#39;) spacy_tok = SpacyTokenizerNLP(lang=&#39;en&#39;, special_toks=defaults.text_spec_tok) def preproc_and_tok(b): return spacy_tok.encodes(list(maps(*defaults.text_proc_rules, b[&#39;text&#39;]))) def postproc(b): return {&#39;tok_text&#39;: [list(maps(*defaults.text_postproc_rules, _b)) for _b in b[&#39;tok_text_pre&#39;]]} def get_tok_lengths(example_batch): return {&#39;tok_lens&#39;: [len(e) for e in example_batch[&#39;toks&#39;]]} def prepare_dataset(dataset, vocab=None, is_train=True): &#39;&#39;&#39; Takes a raw nlp dataset and returns a processed, tokenized, numericalised dataset &#39;&#39;&#39; # Apply processing rules and tokenize print(&#39;pre-proc and tokenize&#39;) dataset = dataset.map(preproc_and_tok, batched=True) # Apply post-processing rules print(&#39;post=proc&#39;) dataset = dataset.map(postproc, batched=True) # Init Numericalizer and create vocab print(&#39;init numericalizer&#39;) if is_train: numeric = NumericalizeNLP(dsets=dataset[&#39;tok_text_pre&#39;], special_toks=defaults.text_spec_tok, pad_tok=1) else: numeric = NumericalizeNLP(dsets=dataset[&#39;tok_text_pre&#39;], vocab=vocab, special_toks=defaults.text_spec_tok, pad_tok=1) # Numericalize print(&#39;numericalizing&#39;) dataset = dataset.map(numeric.encodes_nlp, batched=True) # Get sample lengths for sorting dataset=dataset.map(get_tok_lengths, batched=True) print(&#39;sorting&#39;) # Sort dataset from small to large dataset = dataset.sort(&#39;tok_lens&#39;) return dataset, numeric . . Downloading and preparing dataset sentiment140/sentiment140 (download: 77.59 MiB, generated: 214.21 MiB, total: 291.81 MiB) to /home/morgan/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0... Dataset sentiment140 downloaded and prepared to /home/morgan/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0. Subsequent calls will reuse this data. . Init Timing . #collapse-hide s = &quot;&quot;&quot; # Load train and validation datasets from downloaded files train_senti_dataset = load_dataset(&#39;sentiment140&#39;, split=&#39;train[:80%]&#39;) val_senti_dataset = load_dataset(&#39;sentiment140&#39;, split=&#39;train[80%:100%]&#39;) # Get processed tokens train_senti, numeric = prepare_dataset(train_senti_dataset) val_senti, numeric = prepare_dataset(val_senti_dataset, vocab=numeric.vocab) # Set as Pytorch type print(&#39;setting format&#39;) columns = [&#39;toks&#39;,&#39;sentiment&#39;] train_senti.set_format(type=&#39;torch&#39;, columns=columns) val_senti[0].set_format(type=&#39;torch&#39;, columns=columns) # Instantiate out PyTorch Dataloaders print(&#39;init dataloaders&#39;) train_dataloader = torch.utils.data.DataLoader(train_senti, batch_size=64, collate_fn=pad_batch) val_dataloader = torch.utils.data.DataLoader(val_senti, batch_size=64, collate_fn=pad_batch) &quot;&quot;&quot; time = timeit.timeit(stmt=s, number=1, globals=globals()) time . . Time nlp 1 epoch . #collapse-hide s = &quot;for b in train_dataloader: pass&quot; time = timeit.timeit(stmt=s, number=1, globals=globals()); time time, time / (len(train_senti)/64) . . (323.86245587099984, 0.01619312279354999) .",
            "url": "https://www.ntentional.com/nlp/fastai/dataloader/2020/07/21/speed_test_fastai_vs_hf_nlp_datasets.html",
            "relUrl": "/nlp/fastai/dataloader/2020/07/21/speed_test_fastai_vs_hf_nlp_datasets.html",
            "date": " ‚Ä¢ Jul 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Text Data Cleanup - Dynamic Embedding Visualisation",
            "content": "tl;dr . If there is one thing I would like you to take away from this article it is the ability to use the Bokeh library to dynamically visualise, select and extract text embeddings of interest directly from a plot into a list for further processing with pandas, numpy etc: . . In order for Machine Translation to be useful in the real world, we should should strive to train it on high quality translation data. This is doubly true for lower-resource languages such as Irish, where clean training data is relatively limited. In this article we will try and identify and remove clusters of dirty/noisey samples in our parallalel dataset. The stages we will go through are: . Generate embeddings from a pre-trained multi-lingual model, XLM-RoBERTa | Visualise these embeddings using a dimensionality technique via UMAP | Identify clusters in a sample of the data that seem to be of low translation quality via Bokeh | Remove similar samples from the main dataset via nmslib | . . Warning: Spoiler alert, check out the weird text I found in my Irish-English dataset below üòÖ . Arabic ‚ÅâÔ∏è . 1691 ids returned from query . ga en noise_type . 566807 ŸÇŸÑÿπÿ© ŸÅŸÜ ÿßŸÑÿ®ŸÉÿ≥ŸÑ ( pixels art ) | ÿ∑ÿ±ŸäŸÇŸá ÿπŸÖŸÑ ÿßÿ∑ÿßÿ± ŸÖŸÜŸÇÿ∑ | na | . 571586 ŸÖŸàÿßŸÇÿπ ÿßŸÑÿßÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÖÿ¨ÿßŸÜŸâ free sms ( 1 2) | ŸÑÿß ÿ™ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ±ÿØ ÿπŸÑŸâ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ | na | . 571682 ŸÇŸÑÿπÿ© ÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅŸäŸÉÿ™Ÿàÿ± illustrator Ÿà coreldraw | ÿßŸÑŸÑŸáŸÖ ÿµŸÑŸä Ÿàÿ≥ŸÑŸÖ ÿπŸÑŸâ ŸÜÿ®ŸäŸÜÿß ŸÖÿ≠ŸÖÿØ | na | . 570557 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÇŸÑÿπÿ© ÿßŸÑÿµŸàÿ± ÿßŸÑÿÆÿßÿµÿ© ÿ®ÿßŸÑÿ™ÿµŸÖŸäŸÖ | na | . 566810 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ŸÅŸä ÿßŸÑŸÅŸàÿ™Ÿàÿ¥Ÿàÿ® | na | . Lighting üí° . 1699 ids returned from query . ga en noise_type . 690718 an ts√≠n bh√≠ guangdong soilse linn sn√°mha faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3x5 watts led-e27 manufacturers and listed here are sourced by the karnar lighting. | lighting | . 477330 foinse do gaird√≠n lawn monar√≥ir√≠ ag soilsi√∫ karnar zhongshan &amp; leictreon mhonarcha. | source for high power led wall washer 144w led wall washer manufacturers at zhongshan karnar lighting &amp; electron factory. | lighting | . 479274 an ts√≠n bh√≠ guangdong cumhacht ard-√©adrom lawn faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3 watts led under ground lights round type manufacturers and listed here are sourced by the karnar lighting. | lighting | . 636797 an ts√≠n bh√≠ guangdong faoi sti√∫ir tube monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong led spot light flash lamp and fancy ball manufacturers and listed here are sourced by the karnar lighting. | lighting | . 692603 an ts√≠n bh√≠ guangdong faoi sti√∫ir connectable soilse na nollag monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong high-power led colorful 500w led wall washer manufacturers and listed here are sourced by the karnar lighting. | lighting | . Website Footers üìá . 1825 ids returned from query . ga en noise_type . 696159 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 02:43:19. | ¬©2005-2010 karnarsambandlink okkurveftr√©last modified: august 08 2016 04:06:45. | na | . 478396 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 00:39:06. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 30 2016 22:04:01. | na | . 538977 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 14 2016 20:20:33. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 15 2016 01:50:20. | na | . 684675 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 01:19:15. | ¬©2005-2010 karnarcontactez-nousnous lierplan du sitelast modified: august 08 2016 03:54:30. | na | . 534904 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 11:57:09. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 31 2016 09:46:30. | na | . Clothing and Jewelery üëë . 1938 ids returned from query . ga en noise_type . 641483 home:: uairead√≥ir√≠ hublot:: sraith fusion classic:: sraith 45mm fusion classic:: macasamhail hublot classic comhle√° sraith faire 45mm 511.zx.1170 | home:: hublot watches:: classic fusion series:: classic fusion 45mm series:: replica hublot classic fusion 45mm watch series 401.mx.0123.gr [da02] | na | . 79453 pandora bead √≥ir ivy √≥ir - ‚Ç¨10.23 : jewelry pandora saor, pandoraaustraliabracelets.com | pandora gold bead ivy gold - $11.00 : cheap pandora jewelry, pandoraaustraliabracelets.com | na | . 615744 clrip036a 925 sterling silver b√°n crystal ring - ‚Ç¨28.83 : jewelry pandora saor , pandoraforyou.com | clrip036a 925 sterling silver white crystal ring - $31.00 : cheap pandora jewelry, pandoraforyou.com | na | . 67287 ard-chumhacht t√°irg√≠ faoi sti√∫ir &gt; cumhacht ard-threoraithe colorful &gt; product-list | led lighting &gt; high-power led colorful &gt; product-list lww-10 lww-10-108p lww-10-206p lww-8c-108p | lighting | . 635957 prada m√°la√≠ l√°imhe p - br4692 caife leathar : seaic√©ad spyder, pradahandbags.top | prada borse p - br4692 coffee leather : spyder giacca , pradahandbags.top | na | . Text Embeddings for Fun and Profit . Text embeddings can be incredibly useful for a variety of tasks, a couple of interesting examples include @er214&#39;s post and notebook demonstrating how they used GloVe word embeddings to create a spell-checker for their dataset. They also created a &quot;pretentiousness&quot; embedding to score news outlets ü§£. While these examples used word embeddings, we will generate embeddings for chunks of text, but the concept remains the same. . Pre-Trained XLM-RoBERTa Model . We derive our text embedding by passing a text sample though XLM-RoBERTa Large (XLM-R), a multilingual model which should work reasonably well for both Irish and English. For our embeddings we will extract the values in final hidden layer. Note that some research done on BERT shows that by concatenating the final 4 layers of the model one gets even richer contextual embeddings. In the interest of simplicity we will stick to the final layer only for the moment, although using additional layers would be an interesting area of exploration! . Dimension Reduction with UMAP . Dimensionality reduction techniques attempt to find the latent features in your data. Uniform Manifold Approximation and Projection or &quot;UMAP&quot; is a dimension reduction technique published in 2018 by McInnes and Healy that can be used for visualisation of high dimensional data, in a similar manner to t-SNE. Its main advantage is that it is fast (faster than t-SNE when dealing with large datasets) and also better maintains the global structure of the data. . UMAP also has beautifully well-written documentation. To learn more, McInnes gives a helpful overview of UMAP at the SciPy 2018 conference and Pair Code has an excellent explanation complete with many different visualisations that aid in understanding. . Similarity Search . In order to calculate the similarity between our embeddings there are a multitude of tools and techniques we can try. In this article we will demonstrate both Sci-kit Learn&#39;s cosine_similarity as well as nmslib&#39;s functionality. See the end of this article for the similarity search resources I found useful. . ParaCrawl Data . First lets load our raw ParaCrawl data been crawled from the internet and contains a few different types of artifacts including: . Non-latin scripts | Non alphanumeric characters (e.g. &#39;¬©&#39;,&#39;¬≥&#39;,&#39;¬∫&#39;) | Text samples that are unlikely to have been translated to Irish by a human, including text from Porn sites | Lighting equipment sites (who knows why?) | . | . Lets see how much we can find by turning our samples into embeddings and using dimenstionality reduction to visualise them. For fun, we&#39;ll label texts that contain &quot;sex&quot;, &quot;lighting&quot; and cyrillic characters like &quot;–∏&quot;, &quot;–∑&quot; or &quot;–ª&quot; see if they get clustered together when we visualise our word embeddings later. We will also lowercae our entire dataset; many of the legal texts here are fully written in uppercase, however right now we care more about the content of the text as opposed to the style . Dataset has : 784606 rows [en] : green dog walkers is a regional programme involving most of the councils&#39; in the leinster area. [ga] : is cl√°r r√©igi√∫nach √© si√∫l√≥ir√≠ glasa madra√≠ a bhaineann le formh√≥r na gcomhairl√≠ i gc√∫ige laighin. . Grab the Embeddings . After creating our dataloaders you can see the English sample below, including the special tokens &lt; s &gt; and &lt; /s &gt; used by XLM-R to denote the start and end of the sequence. You can also see all the padding needed for this sample. . samp_dls = get_dls(samp_texts, bs, sl, show=True) . &lt;s&gt; (i) in subsection (1) by deleting the definition of ‚Äúdependant‚Äù, and&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; . Now we&#39;re ready to retrieve our embeddings from the pretrained multi-lingual model. We do this by simply passing our samples to the model and saving the output activations from the last layer of the model. From this we can generate an embedding of size (40000, 1024) which we can then pass to our dimensionality reduction algorithm. . Processing 40k samples with UMAP takes about 5 minutes. After a little testing with the n_neighbors and min_dist parameters I actually found the defaults work quite well, although its always worth playing around with them and distance metric used for your particular dataset. . Scaling your Embeddings . One thing to consider might be whether you should scale your embeddings. The embeddings used here had mean 0 and standard deviation of 0.5, normalising them to (0,1) didn&#39;t seem to have much of an impact on the UMAP visualisation so it is not done here. Scikit-Learn has a wide variety of scaling functions if you do need to scale your data. . Remove similar samples from the entire dataset . We will remove items by: . Visually identifying similar noisey embeddings . | Taking the average of these embeddings . | Calculating a distance between this average embedding and all of the embeddings in our entire dataset . | Removing embeddings that are within a certain distance to the average embeddings . | Visualise: Dimensionality Reduction with UMAP . First we reduce the embedding space down from 1024 to 2 dimensions using UMAP, then we plot the embedding using the Bokeh visualisation library, which enables us to dynamically select different regions of interest, enabling us to interactively explore our dataset. . samp_mapper = umap.UMAP(random_state=42).fit(samp_embs.cpu().numpy()) . nb_url=&quot;http://0.0.0.0:8888&quot; # optional, depending on where you are running the notebook show(plot_emb_app, notebook_url=nb_url) . Below we can see how we can explore how our data has been clustered together and more interestingly select and extract datapoints of interest directly from the plot into a list for further processing with pandas, numpy etc . Identify clusters in a sample of the data that seem to be of low translation quality . Can we identify suspect looking clusters of data? Yes! . We can see the orange &quot;islands&quot; that we have labelled all contain text related to &quot;lighting&quot;, &quot;LED&quot;, &quot;Lamps&quot; etc, some of which are not even in English. . | The sparse cluster of green blue points and their neighbours are also full of language related to pornography . | We don&#39;t see so many Cryllic points, however this was also the least common of our labels, comprising only (0.07% of our labels) . | . Other &quot;islands&quot; that can identified by hovering over them include: . Arabic texts | Website footers | Text from jewellery sites (e.g. Pandora) and also clothing sites | . It Worked! . Below you can see additional Arabic texts in our main dataset we have discovered: . ga en noise_type . 572564 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ÿßŸÑŸÄ riyadh | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ..ÿßŸäŸÄ ÿ¨ŸÜŸàÿ® ŸÄŸÄÿ±ÿßŸÜ.. | na | . 570189 ÿ±ÿØ: ÿØÿ±ÿ≥..ÿßÿ≥ÿ™ÿÆÿØÿ¢ŸÖ content-aware scale..~ | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ÿ∑Ÿäÿ®ŸáŸÄ ÿßŸÑÿ∑Ÿäÿ®ŸáŸÄ | na | . 567067 ŸÖŸÑÿπŸÇÿ© ÿ∑ÿπÿßŸÖ ÿÆŸÑ ÿßÿ®Ÿäÿ∂ | ŸÇŸÑÿπÿ© ŸÅŸÜ ÿßŸÑÿ®ŸÉÿ≥ŸÑ ( pixels art ) | na | . 573439 ŸáŸÄŸÄŸÄÿ∞ÿßŸÉ ŸÑŸÄŸÄŸÄŸà ÿßŸÜŸÄŸÄŸÄŸä ŸÖŸÄŸÄŸÄŸÜ ÿßŸÑŸÄŸàŸÇŸÄŸÄÿ™ ŸÖŸÄŸáŸÄŸÄÿ≤ŸàŸÖ | ŸÇŸÑÿπÿ© ÿØÿ±Ÿàÿ≥ ÿßŸÑÿßŸäŸÖŸäÿ¨ ÿ±ŸäÿØŸä | na | . 571181 ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ adigatalostan | ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ nimrow | na | . I am fairly confident that none of the text in some these islands contain valuable translations and are likely the result of automated translations of suspect quality that we would like to remove. . Remove similar samples from the full dataset . To remove similar samples we will calculate a distance metric between each of our selected embeddings and each of the embeddings in the full dataset. Alternativly we could also calculate the &quot;average embedding&quot; of all of our selected datapoints, and then calculate the distance between this average and each of the embeddings in the main dataset. . From experimentation I found that the first option is more effective at identifying more of the noisy data we are looking for. In addtion, because our nms algorithm is super fast at retrieval there isn&#39;t any significant overhead to this approach over using the average embedding. . First of all, we&#39;ll need to generate embeddings for all of our 780k text samples. This might take a little while depending on the size of your dataset so kick off the extraction and grab a coffee. . Sci-kit Learn&#39;s cosine_similarity . By calculating the average embedding of our selected datapoints we can calculate the cosine similarity between it and all of the other embeddings in our full dataset, providing decent results! . 2724 rows with a similarity score greater than : 0.9995 . ga en noise_type . 570624 ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖÿ¥ÿßÿ±ŸÉÿßÿ™ ÿßŸÑÿ™Ÿä ŸÉÿ™ÿ®Ÿáÿß …¢–ΩœÉ“ì—èŒ±–∏ | ŸÇŸÑÿπÿ© ÿ®ÿ±ÿßŸÖÿ¨ ÿßŸÑŸÅŸäŸÉÿ™Ÿàÿ± illustrator Ÿà coreldraw | na | . 571940 safari ŸÖÿ¥ÿßŸáÿØÿ© ŸÖŸÑŸÅŸá ÿßŸÑÿ¥ÿÆÿµŸä | ŸÖÿ¥ŸÉŸàÿ± ÿπŸÑŸâ ÿßŸÑÿßÿ≥ÿ±ÿßÿ± ŸäÿßÿπŸÖ | na | . 567896 ÿ±ÿ≥ÿßŸÑÿ© ÿ•ÿØÿßÿ±Ÿäÿ© ÿ±ÿ≥ÿßŸÑÿ© ÿ•ÿØÿßÿ±Ÿäÿ© | ÿØŸàÿ±ÿ© ÿßŸÑŸÅŸàÿ™Ÿàÿ¥Ÿàÿ® cs5 ŸÑŸÑŸÖÿ®ÿ™ÿØÿ¶ŸäŸÜ | na | . 573774 ÿßŸÑŸÑŸá ŸäÿπÿßŸÅŸäŸÉ ÿ£ÿÆŸä ÿßŸÑŸÅÿßÿ∂ŸÑ | ŸÖŸÖŸÉŸÜ ÿ™ÿπŸÑŸÖŸäŸÜŸä ctrl+f ÿßÿÆÿ™ÿµÿßÿ± ŸÑŸÄ ÿ•Ÿäÿ¥ ÿ®ÿßŸÑÿ∏ÿ®ÿ∑ | na | . 568595 ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ fahooody | ÿ•ÿ±ÿ≥ÿßŸÑ ÿ±ÿ≥ÿßŸÑÿ© ÿÆÿßÿµÿ© ÿ•ŸÑŸâ ÿ±ÿ¨ŸÑ ÿ£ÿπŸÖÿßŸÑ | na | . We could stop at this point and continue to loop through selecting new datapoints of interest and removing them from our full dataset, but for fun lets look at another way to do similarity search, using nmslib . Creating a nmslib Index . . Note: As an alternative to Sci-kit Learn&#8217;s cosine similarity function we can also use nmslib to calculate our similarity. Note this method is slower given our needs in this example, but its always fun to work with a new technology üòÄ . Once we have all of our embeddings we can create our index with nmslib. This is the most time-consuming part of this process, it took 47minutes for the index to be created in this example, although there are other nmslib settings thatn can reduce this to ~15minutes. . We create an index for our entire dataset only once. We create an index for the entire dataset because we will likely have multiple queries we do not want to re-create a new index each time as it will really slow down how fast we can identify and remove low quality data. . Note: (This index will now include our sample datapoints, which are the same datapoints that we use in our query to the index. Therefore we will have to exclude these sample datapoints from our query result) . Querying the Index . Below are the sampled results from querying the full dataset, found through similarity search with an average embedding of the selection from the bokeh plot: . Arabic . 1691 ids returned from query . ga en noise_type . 566807 ŸÇŸÑÿπÿ© ŸÅŸÜ ÿßŸÑÿ®ŸÉÿ≥ŸÑ ( pixels art ) | ÿ∑ÿ±ŸäŸÇŸá ÿπŸÖŸÑ ÿßÿ∑ÿßÿ± ŸÖŸÜŸÇÿ∑ | na | . 571586 ŸÖŸàÿßŸÇÿπ ÿßŸÑÿßÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÖÿ¨ÿßŸÜŸâ free sms ( 1 2) | ŸÑÿß ÿ™ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ±ÿØ ÿπŸÑŸâ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ | na | . 571682 ŸÇŸÑÿπÿ© ÿØÿ±Ÿàÿ≥ ÿßŸÑŸÅŸäŸÉÿ™Ÿàÿ± illustrator Ÿà coreldraw | ÿßŸÑŸÑŸáŸÖ ÿµŸÑŸä Ÿàÿ≥ŸÑŸÖ ÿπŸÑŸâ ŸÜÿ®ŸäŸÜÿß ŸÖÿ≠ŸÖÿØ | na | . 570557 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÇŸÑÿπÿ© ÿßŸÑÿµŸàÿ± ÿßŸÑÿÆÿßÿµÿ© ÿ®ÿßŸÑÿ™ÿµŸÖŸäŸÖ | na | . 566810 ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: jordan - palestine | ŸÖŸÄŸÄÿ≠ŸÑ ÿßŸÑÿ•ŸÇŸÄÿßŸÖÿ©: ŸÅŸä ÿßŸÑŸÅŸàÿ™Ÿàÿ¥Ÿàÿ® | na | . Lighting . 1699 ids returned from query . ga en noise_type . 690718 an ts√≠n bh√≠ guangdong soilse linn sn√°mha faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3x5 watts led-e27 manufacturers and listed here are sourced by the karnar lighting. | lighting | . 477330 foinse do gaird√≠n lawn monar√≥ir√≠ ag soilsi√∫ karnar zhongshan &amp; leictreon mhonarcha. | source for high power led wall washer 144w led wall washer manufacturers at zhongshan karnar lighting &amp; electron factory. | lighting | . 479274 an ts√≠n bh√≠ guangdong cumhacht ard-√©adrom lawn faoi sti√∫ir monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong 3 watts led under ground lights round type manufacturers and listed here are sourced by the karnar lighting. | lighting | . 636797 an ts√≠n bh√≠ guangdong faoi sti√∫ir tube monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong led spot light flash lamp and fancy ball manufacturers and listed here are sourced by the karnar lighting. | lighting | . 692603 an ts√≠n bh√≠ guangdong faoi sti√∫ir connectable soilse na nollag monar√≥ir√≠ agus at√° liostaithe anseo a fh√°il ag an soilsi√∫ karnar. | the featured china guangdong high-power led colorful 500w led wall washer manufacturers and listed here are sourced by the karnar lighting. | lighting | . Website Footer . 1825 ids returned from query . ga en noise_type . 696159 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 02:43:19. | ¬©2005-2010 karnarsambandlink okkurveftr√©last modified: august 08 2016 04:06:45. | na | . 478396 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 00:39:06. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 30 2016 22:04:01. | na | . 538977 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 14 2016 20:20:33. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 15 2016 01:50:20. | na | . 684675 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: august 08 2016 01:19:15. | ¬©2005-2010 karnarcontactez-nousnous lierplan du sitelast modified: august 08 2016 03:54:30. | na | . 534904 ¬©2005-2010 karnard√©an teagmh√°il linnnasc linnl√©arsc√°il an l√°ithre√°inlast modified: july 31 2016 11:57:09. | ¬©2005-2010 karnarcontact uslink ussite maplast modified: july 31 2016 09:46:30. | na | . Clothing and Jewelery . 1938 ids returned from query . ga en noise_type . 641483 home:: uairead√≥ir√≠ hublot:: sraith fusion classic:: sraith 45mm fusion classic:: macasamhail hublot classic comhle√° sraith faire 45mm 511.zx.1170 | home:: hublot watches:: classic fusion series:: classic fusion 45mm series:: replica hublot classic fusion 45mm watch series 401.mx.0123.gr [da02] | na | . 79453 pandora bead √≥ir ivy √≥ir - ‚Ç¨10.23 : jewelry pandora saor, pandoraaustraliabracelets.com | pandora gold bead ivy gold - $11.00 : cheap pandora jewelry, pandoraaustraliabracelets.com | na | . 615744 clrip036a 925 sterling silver b√°n crystal ring - ‚Ç¨28.83 : jewelry pandora saor , pandoraforyou.com | clrip036a 925 sterling silver white crystal ring - $31.00 : cheap pandora jewelry, pandoraforyou.com | na | . 67287 ard-chumhacht t√°irg√≠ faoi sti√∫ir &gt; cumhacht ard-threoraithe colorful &gt; product-list | led lighting &gt; high-power led colorful &gt; product-list lww-10 lww-10-108p lww-10-206p lww-8c-108p | lighting | . 635957 prada m√°la√≠ l√°imhe p - br4692 caife leathar : seaic√©ad spyder, pradahandbags.top | prada borse p - br4692 coffee leather : spyder giacca , pradahandbags.top | na | . Removal . If we like we can limit our removal to only very similar embeddings in the full dataset by only selecting the datapoints in the full dataset that are sufficiently close (lower score) to the average embedding, plotting the score distribution can help us decide on a suitable threshold . Summary . Hopefully this gives you a sense of how you can explore and clean up large, noisy text datasets. You can open this notebook on github and test it for your own text dataset. . As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter: @mcgenergy . Appendix . Similarity Search Resources . Below are the resources I found that helped give me an overview of the similarity search tools and methods that are out there . nmslib is an &quot;efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces&quot;. In testing Ben Frederickson found that between nmslib, FAISS (Facebook) and annoy (Spotify) nmslib was the fastest nearest neighbours library for CPU. Note FAISS on GPU is blazes past nmslib and annoy, but can also difficult to set up. | . Milvus is an interesting library that was open sourced in late 2019 which offers similarity search using FAISS, nmslib or annoy as well as GPU capability. If I had had more time I would have explored this further | . **ElasticSearch and article is yet another tool we could use to carry out the similarity search between our text embeddings. | . DAIR.ai recently posted a video called &quot;101 Ways to Solve Search&quot; by Pratik Bhavsar which is a great introduction to how search (including semantic search) works in general. | . models.pratik.ai is a really great flow chart visualisation which he tries to keep up to date of the models and techniques available for semantic search and NLP in general | . Spotify&#39;s annoy: @hmendonca&#39;s EDA notebook on kaggle is an excellent example showing how to use annoy to group and display similar images (faces in this case). | .",
            "url": "https://www.ntentional.com/nlp/visualization/bokeh/clustering/2020/06/29/Text-Cleaning-With-Clustering.html",
            "relUrl": "/nlp/visualization/bokeh/clustering/2020/06/29/Text-Cleaning-With-Clustering.html",
            "date": " ‚Ä¢ Jun 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introducing nlp.irish!",
            "content": "tl;dr . Looking through papers to track down the Irish-English parallel corpora they used was a real pain, so I built nlp.irish to document where to find them and how to process them easily | . What? . The intention behind nlp.irish is to make NLP for folks new to working with Irish a little easier by documenting the datasets that are available out there, where to find them and how to load them to a pandas dataframe. | . The site is hosted on github here with the intention that it will grow via a collaborative effort of those working in Irish NLP. | . Where? . üáÆüá™ nlp.irish üáÆüá™ | . Why? . Irish is a low-resource language and every piece of data out there is valuable. | . Current Data . As of writing, 5 commonly used Irish-English parallel corpora have been documented, with instructions on where to find them and code on how to process them: . ParaCrawl, v6 | DGT-TM, DGT-Translation Memory | DCEP, Digital Corpus of the European Parliament | ELRC, European Language Resource Coordination | Tatoeba | . | . Contributing . Contributing is as easy as submitting a pull request on Github. Alternatively you can find me on twitter at @mcgenergy and I can help update the site with your contibution. | . .",
            "url": "https://www.ntentional.com/irish/translation/nmt/mt/nlp/2020/06/12/introducing-nlp-irish.html",
            "relUrl": "/irish/translation/nmt/mt/nlp/2020/06/12/introducing-nlp-irish.html",
            "date": " ‚Ä¢ Jun 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "ICLR 2020: Efficient Deep Learning and More",
            "content": "I was lucky enough to volunteer and attend (virtual) ICLR 2020. It delivered a huge amount of learning for me and I was fortunate to join some really great discussions. . Efficient Deep Learning was big focus of many of the papers and in this second ICLR2020 article* I will focus on techniques presented that either enable more efficient training and/or inference from the papers I managed to see. There are also a couple of bonus papers I really liked at the end of this article. . *In my first ICLR2020 article I highlight some of the new, more efficient transformer achitectures presented such as ELECTRA, Reformer and more. . Note: ICLR Videos Now Online! . All of the ICLR paper talks and slides are now online, I highly recommend watching the 5 to 15minutes videos accompanying each of the papers below for some excellent summaries and additional understanding . Efficient Deep Learning . Training methods and architecture changes that can make Deep Learning models smaller/more efficient . &#9889; Reducing Transformer Depth on Demand with Structured Dropout &#9889; . The introduction of LayerDrop in this paper was super exciting to read as it makes a (transformer) model much more robust to pruning while only having to train it once, unlike finding lottery tickets for example | . Essentially the idea is simple, just randomly remove/skip different layers during the forward pass in training like so: | . . LayerDrop can be implemented like so (see the link below for the author&#39;s full codebase) layer_drop = 0.2 # The authors dropped the layers with a 20% probability in all of their experiments for layer in transformer.layers: if random(0,1) &gt; layer_drop and self.training: x = layer(x) . | This training setup confers 3 benefits: Increased Training Speed (training less layers) in training the percentage increase in words per second increased almost linearly with the percentage of layers dropped | . | Strong regulariser NLP models trained with layerDrop seem to perform better than baseline models trained without it (e.g. EN-DE Transformer performance improvement) | Increased robustness of deeper models which enables you to increase the number of layers in your model. The authors doubled the encoder depth in their WMT14 EN-DE transformer translation model for a new SOTA BLEU score. | Increases model stability | Note the authors also reduced DropOut slightly when training to compensate for the additional regularisation of LayerDrop | . | Reduction in model size A model trained with LayerDrop can be pruned to any desired depth for inference and still maintain robust performance without additional fine-tuning | The specific type of pruning used for inference also did not seem to matter although dropping every other layer seemd to offer strong performance while being straightforward to implement | . | | . Unfortunatley when I asked during the Q&amp;A whether this could be applied to when fine-tuning existing pre-trained transformer models, such as those in HuggingFace&#39;s library, one of the authors replied that they had tried it but it didn&#39;t have great results. Their theory was that these transformers had learned so much during pre-training that a little bit of fine-tuning using LayerDrop wasn&#39;t able to have enough of an influence on the model weights to confer this robustness. | . Code for LayerDrop and models pre-trained with LayerDrop can be found here. If you want to use RoBERTa but find it too large/slow for inference then you should give the models here a go! | . | . &#9889; Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP &#9889; . This work is a follow on from The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks from Jonathan Frankle, Michael Carbin at FAIR, who&#39;s research codebase has just been released by the way: I just open-sourced my codebase for research on neural network pruning, the Lottery Ticket Hypothesis, and other topics in deep learning. It&#39;s written in PyTorch and designed to make it easy to add new models, datasets, and experiments. Check it out: https://t.co/JyTGT8RRZW . &mdash; Jonathan Frankle (@jefrankle) May 7, 2020 | At ICLR 2020 they present how the lottery ticket phenomenon, which previously was only explored for vision models, applies more generally to deep neural networks across NLP and reinforcement learning. | . They test it with NLP models, LSTMs and Transformer, as well as reinforcement learning models and found that the lottery ticket sub-networks performed better than randomly pruned networks, as was found in their previous work on vision models. | . . The authors used Iterative Pruning with Late Resetting (aka Late Rewinding): The trick is that the subnetworks don&#39;t always emerge at initialization. Instead, we found that training these subnetworks from an iteration slightly after initialization (between a few iterations and a few epochs) often works much better. We term this technique &quot;late resetting.&quot; . &mdash; Jonathan Frankle (@jefrankle) March 6, 2019 | Currently the downside to discovering lottery tickets is that they are very computationally expensive to discover. Here the authors trained the models to convergence, before pruning ~20%, reinitializing and training again. Several cycles of this requires significant computational resources for large models such as transformers and reinforcement learning frameworks. However once a lottery ticket is found it can be trained quickly due to its reduced size whilst still maintaining almost the same performance of the original full network. | . I&#39;d also recommend watching the authors second ICLR 2020 paper, &quot;The Early Phase of Neural Network Training&quot;, which explored how the &quot;Early Phase&quot; of the network training, i.e. the point at which lottery ticket sub-networks emerge (and the point at which Late Resetting would reset to) was impacted by variations to the input data and weight distributions. | . &#9889; Dynamic Model Pruning with Feedback &#9889; . The paper introduces a dynamic way to prune weights (Dynamic Model Pruning with Feedback, or DPF) that allows previously pruned model weights to be re-activated when needed, resulting in lottery-ticket peformance of the pruned models while only needing to be trained once (unlike lottery tickets which need multiple rounds of training) | . . They achieve state-of-the-art top-1 accuracy for pruning on CIFAR-10 and Imagenet for unstructured weight pruning | . The gradient is evaluated for the pruned model and then applied to the dense model. The binary mask is periodically updated to reallocate the weights. The intuition is that the gradient is used to measure the &quot;error&quot; and then the dense model is used to correct this error | . Unstructured magnitude pruning was used | . The authors say that the code will be released in June | . &#9889; Once for All: Train One Network and Specialize it for Efficient Deployment &#9889; . The idea here is that a single large model can be trained the contains a multitude of high performant sub-networks. These sub-networks can be pruned for use in a wide variety of edge device types and sizes without additional training. The author&#39;s focussed on training efficient vision models for this paper. | . . This enables strong performance on a wide variety of devices, without incurring the computational expence (and CO2 footprint) of searching for specialised architectures for each device | . . The key to training this model is a technique called Progresive Shrinking which is a:a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution) . | . . They achieved 1.5x lower latency for MobileNet-V3 and 2.6x for EfficientNet in ImageNet mobile setting while maintaining the same accuracy | . Their Code and 50 pre-trained models (for many devices &amp; many latency constraints) can be found in their gihub | . Other Great Papers You Should Absolutely Checkout &#128175; . There were many other super interesting papers I couldn&#39;t cover there, some of my favorites are below . Optimisation . Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization . Introduces Moving Average Batch Normalization (MABN) for training with small batches | Restores BatchNorm-like performance when training with small batches, down to bs=1 (BatchNorm tends to suffer when training with small batches) | Code here | . Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models . Useful for stabilising training on fine-tuning (BERT for downstream task for example) | Motivated by DropOut (which is a special case of DropConnect) | Replaces a randomly selected parameter with a &quot;target&quot; parameter, instead of zero as in DropOut, from a previously memorised state | Code here | . Vision . Network Deconvolution . Correlations between pixels and between channels can make image recognition more difficult, the authors propose network deconvolution to solve this | Achieves impressive performance gains across ResNet, ResNeXt, EfficientNet, VGG (and more) in both image classification and semantic segmentation tasks, even when BatchNorm is removed | Network Deconvolution seems to hold promise beyond vision models too:Also, the same deconvolution procedure for 1 √ó 1 convolutions can be used for non-convolutional layers, which makes it useful for the broader machine learning community. . | Code here- Network Deconvolution has also been discussed and implemented in the fastai forums | . . How much Position Information Do Convolutional Neural Networks Encode? . Adding zero-padding (widely used already) implicitly delivers positional information and imrproves vision performance | Deeper models can better encode positional information | . Thanks for Reading &#128515; . As always, I would love to hear if you have any comments, thoughts or criticisms at @mcgenergy .",
            "url": "https://www.ntentional.com/papers/nlp/efficient-nlp/transformers/2020/05/09/iclr-highlights-2.html",
            "relUrl": "/papers/nlp/efficient-nlp/transformers/2020/05/09/iclr-highlights-2.html",
            "date": " ‚Ä¢ May 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "ICLR 2020: Efficient NLP - Transformers",
            "content": "I was lucky enough to volunteer and attend (virtual) ICLR 2020. It delivered a huge amount of learning for me and I was fortunate to join some really great discussions. . Efficient NLP was big focus of many of the papers and here I will focus on a few of the more well known transformer architectures proposed over the past year or so; Reformer, ELECTRA, Lite Transformer and ALBERT. Towards the end of this article I also mention additional ICLR summaries that are worth reading üôÇ . Note: ICLR Videos Now Online! . All of the ICLR paper talks and slides are now online, I highly recommend watching the 5 to 15minutes videos accompanying each of the papers below for some excellent summaries and additional understanding #ICLR2020 Public Archive - https://t.co/EpXWIK0ujS * ~700 short talks with synced slides, papers, and code* 8 keynotes with moderated QA * 15 workshops on topics ranging from climate change to AfricaNLP. pic.twitter.com/FVX2JJUYVZ . &mdash; Sasha Rush (@srush_nlp) May 4, 2020 . Efficient NLP - Transformers . New transformer architectures that promise less compute-intense NLP training, in order of my excitement to use them: . &#9889; Reformer: The Efficient Transformer &#9889; . Reformer enables training on much longer sequences than BERT-like models (e.g. document-length sequences instead of 512 token length sequences) much more efficiently | Reformer introduces a couple of techniques that improve both time and memory efficiency: . | Technique 1: Reversible residual connection layers (originally used in computer vision in RevNets) instead of the standard residual layers improves memory efficiency: . | . . Technique 2: Locality-Sensitive Hashing (LSH) based attention replaces dot-product attention (and is much faster) which reduces the time complexity: | . The 15 minute ICLR paper presentation video linked above really helps better understand these concepts | A PyTorch Reformer implementation can be found here | . &#9889; ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators &#9889; . ELECTRA brings a couple of novelties, resulting in a much more computationally efficient transformer to train. It is trained with: a Generator-Discriminator setup and | a new pre-training task called Replaced Token Detection | . | The Generator is trained to replace masked tokens (as per the standard MLM task), the Discriminator then tries to identify the token that has been replaced | . . One subtle thing to note is that if the generator happens to generate the correct token then that token is considered &quot;real&quot; instead of &quot;fake&quot; | ELECTRA-small can be trained on a single V100 GPU (4 days) | It is slower per epoch than other transformers, but it converges faster resulting in an overall faster training:the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient . | Very strong results and it&#39;s performance scales up as the architecture is made larger | Lots more interesting results and experiment discussion can be found in the paper | A HuggingFace ELECTRA Implementation is here | . &#9889; Lite Transformer with Long-Short Range Attention &#9889; . Introduces Long-Short Range Attention (LRSA) which results in a reduction in model computation between 2.5x and 3x compared to original Transformer. . | The new architecture enables 2 different perspectives on the input sequence: . ...one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention)... . | The LSRA architecture and where the attention is focussed can be seen here: . | Lite Transformer performs well against the original Transformer for translation, summarisation and language modelling | One thing I liked is that Lite Transformer looks at performance under mobile-like constraints, defined by the authros as 10M parameters and 1G FLOPs | Lite Transformer code (PyTorch) is available from the authors here | . &#9889; ALBERT: A Lite BERT for Self-supervised Learning of Language Representations &#9889; . ALBERT is 18x smaller model than BERT-large and can be trained 1.7x faster while still outperforming it | The two techniques used to reduce its size are: Reduce the vocabulary embedding size; they reduce the matrix size by projecting it to a lower dimension. e.g. an input one-hot encoded matrix of size 30,000 is reduced to a much smaller sized matix which is then used | Cross-layer parameter sharing; they use the same operations and repeat them multiple times. This helps the parameter size of the network growing as layers as added | . | ALBERT uses 3 training tricks to further improve its performance: Uses MLM and Sentence Order Prediction (SOP), a self-supervised loss that focuses on modeling inter-sentence coherence | Does not use dropout (due to the huge amount of data available) | Uses 10x more data than BERT-Base | | HuggingFace PyTorch ALBERT code can be found here | . . Other Great Summaries to Read . Other great summaries from ICLR attendees are below, the Google Doc in Yacine&#39;s tweet below gives brief summaries to even more papers that I haven&#39;t covered here . Marija Stanojevic on mentorship tips for aspiring ML Researchers, @mstanojevic118 . | Yacine Jernite with additional paper summaries, @YRnite . | Analytics Vidhya with a summary of the event and what the most used opensource tools were . | . To Close . Research work on efficient NLP is moving rapidly and it was fascinating to see so many different approaches on display at ICLR this year, myself and my single GPU are super excited to see how fast things will develop this year üòÜ . This was also the first ML conference I attended and found the (covid-caused) virtual format to work exceptionally well, my huge congrates to all of the organisers involved in pulling off a massive amount of work in such a short amount of time! . As always, I would love to hear if you have any comments, thoughts or criticisms at @mcgenergy .",
            "url": "https://www.ntentional.com/papers/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "relUrl": "/papers/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "date": " ‚Ä¢ May 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "FastHugs: Language Modelling with Tranformers and Fastai",
            "content": "This aims to be an end-to-end description with code of how to train a transformer language model using fastai (v2) and HuggingFace, enjoy! . TL;DR . Main interesting bits in this notebook: . Provides full code to train a transformer (RoBERTa) using a Masked Language Model task | Utilise&#39;s many of HuggingFace&#39;s tokenizer features within fastai | Make predictions of masked tokens like this: | . . Before we get started . First off, huge thanks as always to both the Fastai and HuggingFace teams for giving so much back to the community by open-sourcing so much | For an example of text sequence classification using HuggingFace and fastai, have a look at my previous notebook here . | This tutorial is heavily based on HuggingFace&#39;s &quot;How to train a new language model from scratch using Transformers and Tokenizers&quot; tutorial, I highly recommend checking that out too. I try and highlight throughout where code has been used, borrowed or inspired by HuggingFace&#39;s code. . | . MLM Tranform . I feel the most useful thing in this notebook is the MLMTokensLabels transform*. This carries out the Masked Language Model task that RoBERTa was originally trained on. . This will take tokens ids (tokens after the have been numericalized), select a subset and either mask a certain amount of them (for prediction) or replace them with other random token ids (for regularisation). This transform also creates our labels by copying the input token ids and masking the tokens that do not need to be predicted, so that no loss is calculated on them. . Note the if you wish to train BERT or other transformer language models you will probably need to use a different task, e.g. BERT was trained on 2 tasks simultaneously, MLM and Next Sentence Prediction (NSP). Have a look at any blog posts or arxiv paper of the transformer of interest to find which task was used to pretrain it. . *This transform code is a re-write of the mask_tokens function used in HugginFace&#39;s tutorial, code here . Pretraining + Fine-Tuning: . As shown in ULMFit, MultiFiT, and elsewhere, you will get better results on your downstream task if you first fine-tune your pretrained model with the text of the same domain as your pretrained task. e.g. training an IMDB movie review classifier who&#39;s language model was trained on wikipedia text. 1/ Really excited about this one! &quot;Don&#39;t Stop Pretraining: Adapt Language Models to Domains and Tasks&quot; is live! With @anmarasovic, @swabhz , @kylelostat , @i_beltagy , Doug Downey, and @nlpnoah, to appear at ACL2020. Paper: https://t.co/hVbSQYnclk Code: https://t.co/7wKgE1mUme . &mdash; Suchin Gururangan (@ssgrn) April 24, 2020 . Using a Custom Tokenizer? . This code has not been tested using a custom tokenizer. You may want to do so if your text is very specific to a certain domain. If so then you&#39;ll have to add a number of attributes to your tokenzier to be able to use the code here. I really recommend the HuggingFace language model tutorial linked above for an example of training your own tokenizer with your own dataset . Data . We&#39;ll use the IMDB_SAMPLE here, pretending we are fine-tuning our transformer model before doing sentiment classification on IMDB. If you are pretraining a language model from scratch you&#39;d aim to use a larger, more generic source like a wikipedia dataset. fastai have the full WikiText103 (100 million tokens) dataset available for easy download here if you&#39;d like to train an enligh language model from scratch: . path = untar_data(URLs.WIKITEXT) . HuggingFace Auto Classes . HuggingFace have a numer of useful &quot;Auto&quot; classes that enable you to create different models and tokenizers by changing just the model name. . AutoModelWithLMHead will define our Language model for us. This can either be a pretrained model or a randomly initialised model | AutoTokenizer will load our tokenizer and enable us grab our vocab | AutoConfig will define the model architecture and settings, note that we use the pretrained config here for ease of use, but one can easily modify this config if needed | model_name is the model architecture (and optionally model weights) you&#39;d like to use. Language Models tested so far with this notebook: roberta-base | You can find all of HuggingFace&#39;s models at https://huggingface.co/models, most, but not all of them are supported by AutoModel,AutoConfig and AutoTokenizer | . | . We can now easily call whichever transformer we like as below: . model_name = &#39;roberta-base&#39; lm_model_class = AutoModelWithLMHead config_dict = AutoConfig.from_pretrained(model_name) . HuggingFace Tokenizer &amp; Vocab . We use AutoTokenizer to generate our pretrained tokenizer. HuggingFace&#39;s get_vocab returns a token : index dict however Fastai expects vocab to be a list. Therefore we need to convert this dict to a list to be able to use it in fastai . #collapse tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer_vocab=tokenizer.get_vocab() tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])] print(f&#39;Tokenizer &quot;{tokenizer.__class__}&quot; vocab length is : {len(tokenizer_vocab_ls)}&#39;) . . Tokenizer &#34;&lt;class &#39;transformers.tokenization_roberta.RobertaTokenizer&#39;&gt;&#34; vocab length is : 50265 . Special Tokens . Its always good to know what special tokens your tokenizer takes, lets have a look: . tokenizer.special_tokens_map . {&#39;bos_token&#39;: &#39;&lt;s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;, &#39;sep_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;, &#39;cls_token&#39;: &#39;&lt;s&gt;&#39;, &#39;mask_token&#39;: &#39;&lt;mask&gt;&#39;} . FastHugs Tokenizer . This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used. . class FastHugsTokenizer(): &quot;&quot;&quot; transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class model_name : model type set by the user max_seq_len : override default sequence length, typically 512 for bert-like models. `transformer_tokenizer.max_len_single_sentence` and `transformer_tokenizer.max_len_sentences_pair` both account for the need to add additional special tokens, i.e. for RoBERTa-base max_len_single_sentence==510, leaving space for the 2 additional special tokens to be added for the model&#39;s default 512 positional embeddings pair : whether a single sentence (sequence) or pair of sentences are used Returns: - Tokenized text, up to the max sequence length set by the user or the tokenzier default &quot;&quot;&quot; def __init__(self, transformer_tokenizer=None, model_name=&#39;roberta&#39;, max_seq_len=None, pretrained=True, pair=False, **kwargs): self.model_name, self.tok, self.max_seq_len=model_name, transformer_tokenizer, max_seq_len if pretrained: if self.max_seq_len: if pair: assert self.max_seq_len&lt;=self.tok.max_len_sentences_pair, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_sentences_pair&#39; else: assert self.max_seq_len&lt;=self.tok.max_len_single_sentence, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_single_sentence&#39; else: if pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence) def do_tokenize(self, o:str): &quot;&quot;&quot;Returns tokenized text, adds prefix space if needed, limits the maximum sequence length&quot;&quot;&quot; if &#39;roberta&#39; in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len] else: tokens = self.tok.tokenize(o)[:self.max_seq_len] return tokens def de_tokenize(self, o): &quot;&quot;&quot;Return string from tokens&quot;&quot;&quot; text=self.tok.convert_tokens_to_string(o) return text def __call__(self, items): for o in items: yield self.do_tokenize(o) . The Fastai bit . fasthugstok and our tok_fn . Lets incorporate the tokenizer from HuggingFace into fastai-v2&#39;s framework by specifying a function called fasthugstok that we can then pass on to Tokenizer.from_df. (Note .from_df is the only method I have tested) . Max Seqence Length . max_seq_len is the longest sequece our tokenizer will output. We can also the max sequence length for the tokenizer by changing max_seq_len. It uses the tokenizer&#39;s default, typically 512. 1024 or even 2048 can also be used depending on your GPU memory. Note when using pretrained models you won&#39;t be able to use a max_seq_len larger than the default. . max_seq_len = None sentence_pair=False fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, max_seq_len=max_seq_len, sentence_pair=sentence_pair) . We create a MLMTokenizer class which inherits from fastai&#39;s Tokenizer in order to fully decode . #collapse class MLMTokenizer(Tokenizer): def __init__(self, tokenizer, rules=None, counter=None, lengths=None, mode=None, sep=&#39; &#39;, **kwargs): super().__init__(tokenizer, rules, counter, lengths, mode, sep) def _detokenize1(self, o):return self.tokenizer.de_tokenize(o) def decodes(self, o): return TitledStr(str(self._detokenize1(o))) . . Set up fastai&#39;s Tokenizer.from_df, we pass rules=[fix_html] to clean up some of HTML messiness in our text. If you do not want any rules then you sould pass rules=[] to override fastai&#39;s default text processing rules . #collapse fastai_tokenizer = MLMTokenizer.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, tok_func=fasthugstok, rules=[fix_html], post_rules=[]) fastai_tokenizer.rules . . [&lt;function fastai2.text.core.fix_html(x)&gt;] . Add Special Tokens . BERT-like transformers require special tokens to be added to the sequence, depending on the task, so we need a transform for those too . class AddSpecialTokens(Transform): &quot;Add special token_ids to the numericalized tokens for Sequence Classification&quot; def __init__(self, tokenizer): self.tok=tokenizer def encodes(self, o): return(TensorText(self.tok.build_inputs_with_special_tokens(list(o)))) . Create MLM Dataset . #collapse class MLMTokensLabels(Transform): &#39;&#39;&#39; MLM task - Select subset of input token ids, given by `mlm_probability` - Mask a subset of these, `mask_token_prob` - Replace half of the first subset with random tokens - This code most comes from the `mask_tokens` function here https://github.com/huggingface/transformers/blob/a21d4fa410dc3b4c62f93aa0e6bbe4b75a101ee9/examples/run_language_modeling.py#L66 Returns: input ids and labels &#39;&#39;&#39; def __init__(self, tokenizer=None, mlm_probability=0.15, mask_token_prob=0.8): self.tok, self.mlm_probability, self.mask_token_prob=tokenizer, mlm_probability, mask_token_prob def _gen_probability_matrix(self, labels): # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability, defaults to 0.15 in Bert/RoBERTa) probability_matrix = torch.full(labels.shape, self.mlm_probability) special_tokens_mask = self.tok.get_special_tokens_mask(labels.tolist(), already_has_special_tokens=True) probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0) if self.tok._pad_token is not None: padding_mask = labels.eq(self.tok.pad_token_id) probability_matrix.masked_fill_(padding_mask, value=0.0) return probability_matrix def _replace_with_mask(self, inputs, labels, masked_indices): # for `mask_token_prob`% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK]) indices_replaced = torch.bernoulli(torch.full(labels.shape, self.mask_token_prob)).bool() &amp; masked_indices inputs[indices_replaced] = self.tok.convert_tokens_to_ids(self.tok.mask_token) return inputs, indices_replaced def _replace_with_other(self, inputs, labels, masked_indices, indices_replaced): # 1-`mask_token_prob`)/210% of the time, we replace masked input tokens with random word indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced random_words = torch.randint(len(self.tok), labels.shape, dtype=torch.long) inputs[indices_random] = random_words[indices_random] return inputs def encodes(self, inputs): if self.tok.mask_token is None: raise ValueError(&quot;This tokenizer does not have a mask token which is necessary for masked language modeling.&quot;) labels = inputs.clone() # Get probability of whether a token will be masked probability_matrix = self._gen_probability_matrix(labels) # Create random mask indices according to probability matrix masked_indices = torch.bernoulli(probability_matrix).bool() # Mask the labels for indices that are NOT masked, we only compute loss on masked tokens labels[~masked_indices] = -100 # Randomly replace with mask token inputs, indices_replaced = self._replace_with_mask(inputs, labels, masked_indices) # Randomly replace with mask token inputs = self._replace_with_other(inputs, labels, masked_indices, indices_replaced) # The rest of the time (10% of the time) we keep the masked input tokens unchanged return (inputs,labels) . . We change decodes in our Numericalize class to deal with the &lt;loss_mask&gt; tokens . # collapse @Numericalize def decodes(self,o): &#39;Add the ability to parse masks for the loss function, set as `-100`&#39; if isinstance(o, tuple): o=o[0] tmp_vocab=self.vocab.copy() tmp_vocab.append(&#39;&lt;loss_mask&gt;&#39;) o=[-1 if o_ == -100 else o_ for o_ in o] return L(tmp_vocab[o_] for o_ in o if tmp_vocab[o_] != PAD) . . And we modify Datasets so as to not wrap out tuple in another tuple . # collapse @delegates(Datasets) class Datasets(Datasets): &quot;Doesn&#39;t create a tuple in __getitem__ as x is already a tuple&quot; def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): super().__init__(items=items, tfms=tfms, tls=tls, n_inp=n_inp, dl_type=dl_type, **kwargs) def __getitem__(self, it): # same as Datasets.__getitem__ but not wrapped in a tuple res = [tl[it] for tl in self.tls] return res[0] if is_indexer(it) else list(zip(*res)) . . Our dataset is now ready to be created, lets look at an some of our (x,y) that will be passed to the model. When -100 is passed to our loss function (nn.CrossEntropyLoss) it will be ignored in the calculation. Our model will also ignore any padding tokens (usually defined as 1) when passed to it. . #collapse-hide splits = ColSplitter()(df) tfms=[attrgetter(&quot;text&quot;), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls), AddSpecialTokens(tokenizer), MLMTokensLabels(tokenizer)] dsets = Datasets(df, splits=splits, tfms=[tfms], dl_type=SortedDL) dsets[0][0][:20], dsets[0][1][:20] . . (tensor([ 0, 1890, 12, 5225, 24320, 12, 8494, 18421, 50264, 328, 14938, 1774, 630, 75, 190, 356, 69, 50264, 32819, 784]), tensor([ -100, -100, -100, 5225, -100, -100, -100, 18421, -100, -100, -100, -100, -100, 75, -100, -100, -100, 4505, -100, 784])) . Dataloader . Padding . We need to make sure our padding is done correctly as some transformer models prefer padding on the left while others prefer it on the right. tokenizer.padding_side will tell us which side is correct. e.g., BERT, Roberta prefers padding to the right, so we set pad_first=False . #collapse def pad_mlm_input(samples, pad_idx=1, pad_fields=[0,1], pad_first=False, max_seq_len=None, backwards=False): &quot;Function that collect `samples` and adds padding, modified `max_len_l` in fastai&#39;s `pad_input`&quot; pad_fields = L(pad_fields) #max_len_l = ifnone(max_seq_len, pad_fields.map(lambda f: max([len(s[f]) for s in samples]))) max_len_l = pad_fields.map(lambda f: max_seq_len) if backwards: pad_first = not pad_first def _f(field_idx, x): if isinstance(x, tuple): x=(x[0]) ## Added this line too, removes tuple if present if field_idx not in pad_fields: return x idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x)) pad = x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx x1 = torch.cat([pad, x] if pad_first else [x, pad]) if backwards: x1 = x1.flip(0) return retain_type(x1, x) return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples] def transformer_mlm_padding(tokenizer=None, max_seq_len=None, sentence_pair=False): &#39;Uses `pad_fields=[0,1]` to pad both input and label&#39; if tokenizer.padding_side == &#39;right&#39;: pad_first=False else: pad_first=True max_seq_len = ifnone(max_seq_len, tokenizer.max_len) return partial(pad_mlm_input, pad_fields=[0,1], pad_first=pad_first, pad_idx=tokenizer.pad_token_id, max_seq_len=max_seq_len) . . #collapse padding=transformer_mlm_padding(tokenizer) bs=4 dls = dsets.dataloaders(bs=bs, before_batch=[padding]) . . Check our batch . We can see our special RoBERTa tokens (&#39;&lt;s&gt;&#39;, &#39;&lt;/s&gt;&#39;), which translate to 0, 2 in its vocab, have been added to the start and end of each sequence in the batch. Your can look at these indices in tokenizer.get_vocab() to confirm this. We can also see that most of the tokens in our target (text_) are masked out as we only want to calculate the loss on the ~15% of the text tokens that have been masked. . #collapse b=dls.one_batch() b[0].size(), b[1].size() . . (torch.Size([4, 512]), torch.Size([4, 512])) . #collapse dls.show_batch() . . text text_ . 0 &lt;s&gt; I&lt;mask&gt; fortunate enough to meet&lt;mask&gt; Pal segregatedand still have my DS:TMlishing&lt;mask&gt; autographed by&lt;mask&gt;&lt;mask&gt; at a convention shortly&lt;mask&gt; the release, and asked him why he chose to do the film &quot;camp&quot;. Before&lt;mask&gt; could answer, two studio flacks intercepted and lectured me on how the studio &quot;knew best&quot; and how &quot;no one will take such&lt;mask&gt; film seriously&quot;. I had been reading the Bantam reprints for&lt;mask&gt; couple of years thanks&lt;mask&gt; a&lt;mask&gt; (ComiCon attendees of the 1970s will recall 357hawk and his band? I was in&lt;mask&gt; couple&lt;mask&gt; years of that withnd), and had higher hopes than what we&lt;mask&gt;.&lt;mask&gt; nThe flacks insisted that no high adventure would ever be&lt;mask&gt; seriously, and so doing &#39;camp&lt;mask&gt; was the&lt;mask&gt; way. Several other fans jumped in gap my&lt;mask&gt;, with Pal listening as best he could. At the end of the little event, Pal&lt;mask&gt; up to&lt;mask&gt; and apologized,&lt;mask&gt; he could have done more and better. n nSTAR WARS put the lie to | &lt;loss_mask&gt;&lt;loss_mask&gt; was&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt; (&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;OB poster&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him)&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; after&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; he&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; &quot;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; friend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Black&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hopes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; got&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; done&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;&lt;loss_mask&gt;&lt;loss_mask&gt; only&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt; side&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; came&lt;loss_mask&gt;&lt;loss_mask&gt; us&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; wishing&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; that&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rating as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt; n&lt;loss_mask&gt; destroying the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; still&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; we&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;hero&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, there&lt;loss_mask&gt;&lt;loss_mask&gt; second&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; serious&lt;loss_mask&gt; Yes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt; And&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;sheet&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; leaping&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bronze&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;AV&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Next&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; If&lt;loss_mask&gt; knows&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; San&lt;loss_mask&gt; for the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 1 &lt;s&gt;&lt;mask&gt; is another one of those &#39;humans vs insects/eco-horror&#39; features; a theme that was popular in the late 70&#39;s.&lt;mask&gt; you can&#39;t really call it horror. There&#39;s zero suspense and no&lt;mask&gt; events.&lt;mask&gt; other words: this movie&lt;mask&gt; pretty lame. It&#39;s not that it&lt;mask&gt; really bad or&lt;mask&gt;; it&#39;s just very boring. A construction site near&lt;mask&gt; hotel uncovers a big nest of&lt;mask&gt;. Later on we learn that, probably due to&lt;mask&gt; sorts&lt;mask&gt; pesticides Lounge in the past, their&lt;mask&gt; became poisonous. Some people get bitten and rushed to&lt;mask&gt; hospital and it takes ages for&lt;mask&gt;&lt;mask&gt; Vanity the&lt;mask&gt; to figure out what&#39;s going on.&lt;mask&gt; Foxworth figures&lt;mask&gt; out first and then you can&lt;mask&gt; him go berserk with a digging machine for what seems like several hours.&lt;mask&gt; they&lt;mask&gt; in the house, waiting&lt;mask&gt; get rescued. And, man, you should see all the efforts they make for&lt;mask&gt; them.&lt;mask&gt; won&#39;t spoil too much, but at&lt;mask&gt; point they even use a big&lt;mask&gt;. All the | &lt;loss_mask&gt; This&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Only&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; gruesome&lt;loss_mask&gt;&lt;loss_mask&gt; In&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; something&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; different&lt;loss_mask&gt; of&lt;loss_mask&gt; used&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bite&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the residents of&lt;loss_mask&gt; hospital&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Robert&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; see&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Then&lt;loss_mask&gt; flee&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; all&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rescuing&lt;loss_mask&gt;&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; one&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helicopter&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt; thinking&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; you&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; building&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; lots of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt; are shown&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; garbage&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; straw&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt; wider shots&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; designers&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; near&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; do&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; IT&lt;loss_mask&gt;&lt;loss_mask&gt;EN&lt;loss_mask&gt; AT&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; my&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; title&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;K&lt;loss_mask&gt;&lt;loss_mask&gt; MAN&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;ll&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Now&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 2 &lt;s&gt; The&lt;mask&gt;&lt;mask&gt; saw no fewer than 3 filmed productions&lt;mask&gt; H. G. Wells&#39; great novel, &quot;War of&lt;mask&gt; Worlds&quot;. This&lt;mask&gt; perhaps the least well-known and very probably the best of&lt;mask&gt;&lt;mask&gt; No other&lt;mask&gt;&lt;mask&gt; W&lt;mask&gt;W has ever attempted not only to present the story very much as Wells wrote&lt;mask&gt;, but also Burton create the atmosphere of the time&lt;mask&gt; which it was supposed to take place: the last year of&lt;mask&gt; 19th Century, 1900 ¬Ö using Wells&#39; original setting, in and near Woking&lt;mask&gt;&lt;mask&gt;. n nIMDb&lt;mask&gt; unfFlyingly to what they regard as &quot;spoilers&quot;. That might apply&lt;mask&gt; some&lt;mask&gt;, where the ending might actually be a&lt;mask&gt;, but with regard to one of the most famous novels in&lt;mask&gt;&lt;mask&gt;, it seems positively silly. I have&lt;mask&gt; sympathy&lt;mask&gt; people who have neglected to&lt;mask&gt; one&lt;mask&gt; the seminal works&lt;mask&gt; English literature,&lt;mask&gt; let&#39;s get right to the chase. The aliens are destroyed through catching an Earth disease,&lt;mask&gt; hits&lt;mask&gt; have no immunity. If that wo a spoiler, so be | &lt;loss_mask&gt;&lt;loss_mask&gt; year 2005&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; them.&lt;loss_mask&gt;&lt;loss_mask&gt; version of&lt;loss_mask&gt;ot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, England&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; seems unfriend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; films&lt;loss_mask&gt; where&lt;loss_mask&gt;&lt;loss_mask&gt; might&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; surprise&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the world&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have no&lt;loss_mask&gt; for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; read&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; so&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; against which they&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; 1953 classic&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39; plot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ÔøΩ&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; way&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; off due to&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Century&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;ides&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; some&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; an&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; than&lt;loss_mask&gt;).&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; are typical of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;/white and&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt; described them&lt;loss_mask&gt; have a more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;feel&quot;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; destruction&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; period&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; particularly&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; or brilliant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facial&lt;loss_mask&gt; | . 3 &lt;s&gt;&lt;mask&gt; watched Grend&lt;mask&gt; the&lt;mask&gt; night and am compelled&lt;mask&gt; evangelical&lt;mask&gt; a Public Service Announcement. n nGrendel is another version of&lt;mask&gt;owulf, the thousand- resulted-&lt;mask&gt; Anglo-Saxon epic poem.&lt;mask&gt; SciFi channeluture a growing catalog of inoffensive&lt;mask&gt; uninterestingxs,&lt;mask&gt; the previews promised an&lt;mask&gt;authentic low-budget mini-epic, but this one refused to&lt;mask&gt;&lt;mask&gt; switch channels.&lt;mask&gt; was staggeringly, overwhelmingly, bad&lt;mask&gt; I watched in fascination and horror at the train wreck you&lt;mask&gt;&#39;t tear your eyes away from&lt;mask&gt; I reached for a notepad and managed to capture part of what I was seeing.&lt;mask&gt; following may contain spoilers or might just save your sanity&lt;mask&gt; You&#39;ve been warned. n n- Just to&lt;mask&gt; it over with, Beow&lt;mask&gt;&lt;mask&gt; warriors wore horned&lt;mask&gt;.&lt;mask&gt;&lt;mask&gt;ial issue compared to what came after. It also appears that the helmets were in a bin and handed&lt;mask&gt; whichever actor wandered by next. Fit,&lt;mask&gt; and function&lt;mask&gt; apparently irrelevant. n n- Marina Sirtis&lt;mask&gt;&lt;mask&gt; been blackmailed into doing the&lt;mask&gt; by&lt;mask&gt; Ringling Brothers, Barnum and&lt;mask&gt;&lt;mask&gt;.&lt;mask&gt; managed to avoid a red rubber nose, but the | &lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;el&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to put together&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;year&lt;loss_mask&gt;old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; has&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt; movies&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; let me&lt;loss_mask&gt;&lt;loss_mask&gt;. It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; couldn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt; contain&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; save&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; get&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;ulf&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helmets&lt;loss_mask&gt; Triv&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; actor&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; appearance&lt;loss_mask&gt;&lt;loss_mask&gt; were&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; had obviously&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Bailey circus&lt;loss_mask&gt; She&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ben&lt;loss_mask&gt;&lt;loss_mask&gt; not&lt;loss_mask&gt; be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; H&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; must have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; hadn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facilitate&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hairst&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; sideburn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;.&lt;loss_mask&gt; prove&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;shaped&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tradition&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; volume&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt; unintended focus&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; bolts&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; recoil&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . Model . Our model can be instantiated with either pretrained or random weights. We also need to be careful to pass the model the attention_mask so that the model ignores padding tokens when training. . class LMModel(nn.Module): def __init__(self, lm_model_class=None, tokenizer=None, model_name=None, config_dict=None, pretrained=False): super().__init__() self.tok=tokenizer if pretrained: self.model = lm_model_class.from_pretrained(model_name) else: self.model = lm_model_class.from_config(config_dict) self.model = self.model.module if hasattr(self.model, &quot;module&quot;) else self.model self.model.resize_token_embeddings(len(tokenizer)) def forward(self, input_ids): attention_mask = (input_ids!=self.tok.pad_token_id).type(input_ids.type()) return self.model(input_ids, attention_mask=attention_mask)[0] # only return the prediction_scores (and not hidden states and attention) . Pretrained Language Model . Lets fine-tune our pretrained Language Model. We would typically do this before training the model on our specific text. Note that here we are not training the language model head before we train the full model, but we could do so if we created a splitter and passed it to our learner . To load the pretrained HuggingFace model just use pretrained=True when calling your model: . model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=True) . Training . From here we train our model as usual using fastai. Note that we use Perplexity as our metric as it is a good measure of how well a language model is training . #collapse opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, #splitter=model_splitter, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . We check our learning rate finder . #collapse-hide learn.lr_find(suggestions=True, stop_div=False) . . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.2089296132326126) . We do some training . #collapse-hide learn.fit_one_cycle(10, lr_max=1e-4) . . epoch train_loss valid_loss accuracy perplexity time . 0 | 13.052832 | 11.317341 | 0.052725 | 82235.375000 | 00:32 | . 1 | 8.539399 | 5.935386 | 0.049121 | 378.185822 | 00:32 | . 2 | 1.660586 | 0.785814 | 0.493350 | 2.194192 | 00:32 | . 3 | 0.731211 | 0.768679 | 0.493125 | 2.156916 | 00:32 | . 4 | 0.732979 | 0.772890 | 0.492373 | 2.166016 | 00:32 | . 5 | 0.681202 | 0.695503 | 0.493711 | 2.004716 | 00:33 | . 6 | 0.660206 | 0.681334 | 0.494063 | 1.976512 | 00:33 | . 7 | 0.469388 | 0.641964 | 0.495615 | 1.900209 | 00:33 | . 8 | 0.512519 | 0.612524 | 0.494834 | 1.845082 | 00:33 | . 9 | 0.545736 | 0.625833 | 0.495205 | 1.869804 | 00:33 | . And we see how our loss progressed . Lets Look at the model&#39;s predictions . Manually checking how well our model makes predictions for masked tokens is a simple way to see how it is training . Here function get_mask_pred takes masked string given by the user and returns the topk predictions given by the model for that masked token. With it we can sanity check that our model has learned something useful! . *Note that get_mask_pred is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . #collapse def get_mask_pred(model, masked_text:str, topk:int=5): &quot;Code lightly modified from `FillMaskPipeline` in the HuggingFace Transformers library&quot; aa=fastai_tokenizer.encodes(masked_text) bb=Numericalize(vocab=tokenizer_vocab_ls)(aa) cc=AddSpecialTokens(tokenizer)(bb) outs=model(cc.unsqueeze(0).cuda()) masked_index = (cc == tokenizer.mask_token_id).nonzero().item() logits = outs[0, masked_index, :] probs = logits.softmax(dim=0) values, predictions = probs.topk(topk) result=[] for i, vv in enumerate(zip(values.tolist(), predictions.tolist())): v, p =vv tokens = cc.numpy() if i == 0: result.append({&quot;word&quot;:&quot;Input text&quot;, &quot;score&quot;: 0., &quot;token&quot;: 0, &quot;sequence&quot;: tokenizer.decode(tokens)}) tokens[masked_index] = p tokens = tokens[np.where(tokens != tokenizer.pad_token_id)] w = tokenizer.decode(p) result.append({&quot;word&quot;:w, &quot;score&quot;: v, &quot;token&quot;: p, &quot;sequence&quot;: tokenizer.decode(tokens)}) return pd.DataFrame(result) . . Here we can input our own masked sentence and see how the model does. Note that even without fine-tuning the performance below will still be very strong as the pretrained RoBERTa model is very strong. . text2 = &#39;I was walking to &lt;mask&gt; when I came across a cat on the road&#39; pred2 = get_mask_pred(model1, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I came across a cat on the road&lt;/s&gt; | . 1 school | 0.791473 | 334 | &lt;s&gt; I was walking to school when I came across a cat on the road&lt;/s&gt; | . 2 church | 0.068957 | 2352 | &lt;s&gt; I was walking to church when I came across a cat on the road&lt;/s&gt; | . 3 work | 0.068007 | 173 | &lt;s&gt; I was walking to work when I came across a cat on the road&lt;/s&gt; | . 4 breakfast | 0.007202 | 7080 | &lt;s&gt; I was walking to breakfast when I came across a cat on the road&lt;/s&gt; | . Not bad at all! Now lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model1, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 would | 0.809723 | 74 | &lt;s&gt; This very funny British comedy shows what would happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 2 might | 0.131539 | 429 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 3 could | 0.042638 | 115 | &lt;s&gt; This very funny British comedy shows what could happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 will | 0.009556 | 40 | &lt;s&gt; This very funny British comedy shows what will happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br... | . Boom, pretty darn good! Lets try the same example, replacing ancient in discovery of ancient documents . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 historical | 0.585666 | 4566 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of historical documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /... | . 2 old | 0.086817 | 793 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of old documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 3 obscure | 0.040825 | 23732 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of obscure documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 ancient | 0.035504 | 8178 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . Again, pretty solid predictions! . Train a Language Model from Scratch! . We can follow the same procedure to train a language model from scratch by using pretrained=False when seeing up our model . #collapse model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=False) opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . Training . Untrained . Again, lets look at the predictions: . model2=learn.model . text2 = &#39;I was walking to &lt;mask&gt; when I cam across a cat on the road&#39; pred2 = get_mask_pred(model2, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I cam across a cat on the road&lt;/s&gt; | . 1 the | 0.037963 | 5 | &lt;s&gt; I was walking to the when I cam across a cat on the road&lt;/s&gt; | . 2 . | 0.036504 | 4 | &lt;s&gt; I was walking to. when I cam across a cat on the road&lt;/s&gt; | . 3 , | 0.033266 | 6 | &lt;s&gt; I was walking to, when I cam across a cat on the road&lt;/s&gt; | . 4 of | 0.024381 | 9 | &lt;s&gt; I was walking to of when I cam across a cat on the road&lt;/s&gt; | . Pretty bad üëé, and see how the unconfident it is in its predictions! This doesn&#39;t perform well because we have only used 800 movie reviews to train our model, we&#39;ll need a lot more text to get decent results! . Again, just for fun, lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 , | 0.044226 | 6 | &lt;s&gt; This very funny British comedy shows what, happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 2 the | 0.035027 | 5 | &lt;s&gt; This very funny British comedy shows what the happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 3 . | 0.028172 | 4 | &lt;s&gt; This very funny British comedy shows what. happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 4 and | 0.025764 | 8 | &lt;s&gt; This very funny British comedy shows what and happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . Ewww.. . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 the | 0.036510 | 5 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of the documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 2 , | 0.035627 | 6 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of, documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . 3 and | 0.029176 | 8 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of and documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 4 . | 0.029063 | 4 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of. documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . Yuck! . Notes &amp; Hacky Bits . Notes . The validation set will change slightly due to random masking. While the data in the validaion set remains constant, different tokens will be masked each time the validation dataloader is called due to MLMTokensLabels calling a random probability each time. . If a perfectly reproducable validation set is needed then you&#39;ll probably have to create a separate transform for it&#39;s masking and set it&#39;s split_idx = 1. | . | I didn&#39;t have time to get learn.predict working. One issue that needs to be fixed is that MLMTokensLabels transform shouldn&#39;t be called on your masked input text as it will add more masks, which you don&#39;t want. . | FastHugsTokenizer will have to be modified to: . enable sequence lengths larger than the tokenizer default | to use a non-pretrained tokenizer (e.g. one you trained yourself) | . | The HuggingFace encode_plus or batch_encode_plus functions are great and I would have used them, but don&#39;t play nice with fastai multiprocessiing . | . Hacks . I had to overwrite __getitem__ in the Datasets class so that it wouldn&#39;t return a tuple as what it thinks is our x is actually our (x,y). Wrapping this tuple in anoother tuple causes headaches down the line. Creating a custom Datasets class and inheriting from it didn&#39;t work as learn.predict calls on Datasets and not the custom dataset class. . | The function get_mask_pred (used to view predictions of masked text) is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . | . Give me a shout &#128227; . Thats it for this, I hope you found it useful and learned a thing or two. If you have any questions or would like to get in touch you can find me on Twitter @mcgenergy .",
            "url": "https://www.ntentional.com/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "relUrl": "/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "date": " ‚Ä¢ Apr 24, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "FastHugs: Sequence Classification with Transformers and Fastai",
            "content": "All FastHugs code can be found in my FastHugs GitHub . Things You Might Like (&#10084;&#65039; ?) . FastHugsTokenizer: A tokenizer wrapper than can be used with fastai-v2&#39;s tokenizer. . FastHugsModel: A model wrapper over the HF models, more or less the same to the wrapper&#39;s from HF fastai-v1 articles mentioned below . Padding: Padding settings for the padding token index and on whether the transformer prefers left or right padding . Model Splitters: Functions to split the classification head from the model backbone in line with fastai-v2&#39;s new definition of Learner (in splitters.py . Housekeeping . Pretrained Transformers only for now &#128528; . Initially, this notebook will only deal with finetuning HuggingFace&#39;s pretrained models. It covers BERT, DistilBERT, RoBERTa and ALBERT pretrained classification models only. These are the core transformer model architectures where HuggingFace have added a classification head. HuggingFace also has other versions of these model architectures such as the core model architecture and language model model architectures. . If you&#39;d like to try train a model from scratch HuggingFace just recently published an article on How to train a new language model from scratch using Transformers and Tokenizers. Its well worth reading to see how their tokenizers library can be used, independent of their pretrained transformer models. . Read these first &#128071; . This notebooks heavily borrows from this notebook , which in turn is based off of this tutorial and accompanying article. Huge thanks to Melissa Rajaram and Maximilien Roberti for these great resources, if you&#39;re not familiar with the HuggingFace library please given them a read first as they are quite comprehensive. . fastai-v2 &#9996;&#65039;2&#65039;&#8419; . This paper introduces the v2 version of the fastai library and you can follow and contribute to v2&#39;s progress on the forums. This notebook uses the small IMDB dataset and is based off the fastai-v2 ULMFiT tutorial. Huge thanks to Jeremy, Sylvain, Rachel and the fastai community for making this library what it is. I&#39;m super excited about the additinal flexibility v2 brings. üéâ . Dependencies &#128229; . If you haven&#39;t already, install HuggingFace&#39;s transformers library with: pip install transformers . #collapse path = untar_data(URLs.IMDB_SAMPLE) model_path = Path(&#39;models&#39;) df = pd.read_csv(path/&#39;texts.csv&#39;) . . FastHugs Tokenizer . This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used. . #collapse class FastHugsTokenizer(): &quot;&quot;&quot; transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class model_name : model type set by the user max_seq_len : override default sequence length, typically 512 for bert-like models sentence_pair : whether a single sentence (sequence) or pair of sentences are used &quot;&quot;&quot; def __init__(self, transformer_tokenizer=None, model_name = &#39;roberta&#39;, max_seq_len=None, sentence_pair=False, **kwargs): self.tok, self.max_seq_len=transformer_tokenizer, max_seq_len if self.max_seq_len: if self.max_seq_len&lt;=self.tok.max_len: print(&#39;WARNING: max_seq_len is larger than the model default transformer_tokenizer.max_len&#39;) if sentence_pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence) self.model_name = model_name def do_tokenize(self, o:str): &quot;&quot;&quot;Limits the maximum sequence length and add the special tokens&quot;&quot;&quot; CLS, SEP=self.tok.cls_token, self.tok.sep_token # Add prefix space, depending on model selected if &#39;roberta&#39; in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len] else: tokens = self.tok.tokenize(o)[:self.max_seq_len] # order of &#39;tokens&#39;, &#39;SEP&#39; and &#39;CLS&#39; if &#39;xlnet&#39; in model_name: return tokens + [SEP] + [CLS] else: return [CLS] + tokens + [SEP] def __call__(self, items): for o in items: yield self.do_tokenize(o) . . FastHugs Model . This nn.module wraps the pretrained transformer model and initialises it with its config file. . The forward of this module is taken straight from Melissa&#39;s notebook above and its purpose is to create the attention mask and grab only the logits from the output of the model (as the HappyFace transformer models also output the loss). . #collapse class FastHugsModel(nn.Module): &#39;Inspired by https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers/data&#39; def __init__(self, transformer_cls, config_dict, n_class, pretrained=True): super(FastHugsModel, self).__init__() self.config = config_dict self.config._num_labels = n_class # load model if pretrained: self.transformer = transformer_cls.from_pretrained(model_name, config=self.config) else: self.transformer = transformer_cls.from_config(config=self.config) def forward(self, input_ids, attention_mask=None): attention_mask = (input_ids!=1).type(input_ids.type()) logits = self.transformer(input_ids, attention_mask = attention_mask)[0] return logits . . The HuggingFace bit . Define HuggingFace Model + Config . AutoModelForSequenceClassification will define our model. When this is padded to the FastHugsModel class below then model will be instantiated and the weights downloaded (if you are using a pretrained model) | AutoConfig will define the model architecture and settings | model_name is the model architecture (and optionally model weights) you&#39;d like to use. Models tested: bert-base-uncased, roberta-base, distilbert-base-cased, albert-base-v2 | You can find all of HuggingFace&#39;s models at https://huggingface.co/models, although not all of them are supported by AutoModel,AutoConfig and AutoTokenizer | . | . model_name = &#39;roberta-base&#39; model_class = AutoModelForSequenceClassification config_dict = AutoConfig.from_pretrained(model_name) . HuggingFace Config changes . Some config settings can be changed even when using pretrained weights. For example in the FastHugsModel class below _num_labels is set when the model (pretrained or not) is instantiated, depending on how many classes you have in your dataloader. . When creating a non-pretrained model you can load a config with: . config_dict = AutoConfig.for_model(model_name) . Alternatively you could load a pretrained config and modify that. For example if your are not using a pretrained model you can change the size of your input embeddings by changing config_dict.max_position_embeddings = 1024. (This won&#39;t work when using pretrained models as the pre-trained weights need the default max_position_embeddings size). . HuggingFace Tokenizer &amp; Vocab . AutoTokenizer will load our tokenizer and enable us grab our vocab | . fastai expects vocab to be a list, however HuggingFace&#39;s get_vocab returns a token : index dict. We need to convert this dict to a list to be able to use it in fastai . tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer_vocab=tokenizer.get_vocab() tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])] len(tokenizer_vocab_ls) . 50265 . The Fastai bit . Get fastai model splitter function . In order to be able to fine-tune our classifier head we need to first split the HuggingFace model&#39;s classifier head from the body. These functions are dependent on the specific architecture and can be found in splitter.py of this repo . splitter_nm = model_name.split(&#39;-&#39;)[0] + &#39;_cls_splitter&#39; model_splitter = splitters[splitter_nm] . fasthugstok and our tok_fn . Lets incorporate the tokenizer from HuggingFace into fastai-v2&#39;s framework by specifying a function called fasthugstok that we can then pass on to Tokenizer.from_df. (Note .from_df is the only method I have tested) . Max Seqence Length . max_seq_len is the longest sequece our tokenizer will output. We can also the max sequence length for the tokenizer by changing max_seq_len. It uses the tokenizer&#39;s default, typically 512. 1024 or even 2048 can also be used depending on your GPU memory. Note when using pretrained models you won&#39;t be able to use a max_seq_len larger than the default. . max_seq_len = None sentence_pair=False fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, max_seq_len=max_seq_len, sentence_pair=sentence_pair) . Set up fastai&#39;s Tokenizer.from_df, we pass rules=[] to override fastai&#39;s default text processing rules . fastai_tokenizer = Tokenizer.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, tok_func=fasthugstok, rules=[]) . Setup Dataloaders . Create Dataset . Lets add our custom tokenizer function (tok_fn) and transformer_vocab here . splits = ColSplitter()(df) x_tfms = [attrgetter(&quot;text&quot;), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls)] dsets = Datasets(df, splits=splits, tfms=[x_tfms, [attrgetter(&quot;label&quot;), Categorize()]], dl_type=SortedDL) . Padding . We need to make sure our padding is done correctly as some transformer models prefer padding on the left while others prefer it on the right. tokenizer.padding_side will tell us which side is correct. e.g., BERT, Roberta prefers padding to the right, so we set pad_first=False . #collapse def transformer_padding(tokenizer=None, max_seq_len=None, sentence_pair=False): if tokenizer.padding_side == &#39;right&#39;: pad_first=False else: pad_first=True max_seq_len = ifnone(max_seq_len, tokenizer.max_len) return partial(pad_input_chunk, pad_first=pad_first, pad_idx=tokenizer.pad_token_id, seq_len=max_seq_len) . . Dataloaders . bs = 4 padding=transformer_padding(tokenizer) dls = dsets.dataloaders(bs=bs, before_batch=[padding]) . dls.show_batch(max_n=3, trunc_at=60) . text category . 0 &lt;s&gt; ƒ†I ƒ†was ƒ†fortunate ƒ†enough ƒ†to ƒ†meet ƒ†George ƒ†Pal ƒ†( and ƒ†still ƒ†have ƒ†my ƒ†DS : TM OB ƒ†poster ƒ†aut ographed ƒ†by ƒ†him ) ƒ†at ƒ†a ƒ†convention ƒ†shortly ƒ†after ƒ†the ƒ†release , ƒ†and ƒ†asked ƒ†him ƒ†why ƒ†he ƒ†chose ƒ†to ƒ†do ƒ†the ƒ†film ƒ†&quot; camp &quot;. ƒ†Before ƒ†he ƒ†could ƒ†answer , ƒ†two ƒ†studio ƒ†fl acks ƒ†intercepted ƒ†and ƒ†lect ured ƒ†me ƒ†on | negative | . 1 &lt;s&gt; ƒ†D ressed ƒ†to ƒ†Kill ƒ†starts ƒ†off ƒ†with ƒ†Kate ƒ†Miller ƒ†( Ang ie ƒ†Dickinson ) ƒ†having ƒ†a ƒ†sexually ƒ†explicit ƒ†nightmare , ƒ†later ƒ†on ƒ†that ƒ†day ƒ†she ƒ†visits ƒ†her ƒ†psychiatrist ƒ†Dr . ƒ†Robert ƒ†Elliott ƒ†( Michael ƒ†C aine ) ƒ†for ƒ†a ƒ†session ƒ†in ƒ†which ƒ†she ƒ†admits ƒ†to ƒ†be ƒ†sexually ƒ†frustrated ƒ†&amp; ƒ†un ful filled ƒ†in ƒ†her ƒ†current ƒ†marriage . ƒ†Kate ƒ†then | positive | . 2 &lt;s&gt; ƒ†SHALL OW ƒ†G RA VE ƒ†begins ƒ†with ƒ†either ƒ†a ƒ†tribute ƒ†or ƒ†a ƒ†rip ƒ†off ƒ†of ƒ†the ƒ†shower ƒ†scene ƒ†in ƒ†PS Y CHO . ƒ†( I &#39;m ƒ†leaning ƒ†toward ƒ†rip ƒ†off .) ƒ†After ƒ†that ƒ†it ƒ†gets ƒ†worse ƒ†and ƒ†then ƒ†surprisingly ƒ†gets ƒ†better , ƒ†almost ƒ†to ƒ†the ƒ†point ƒ†of ƒ†being ƒ†original . ƒ†Bad ƒ†acting ƒ†and ƒ†amateur ish ƒ†directing ƒ†bog ƒ†down ƒ†a | negative | . (Alternatively) Factory dataloader . Here we set: . tok_tfm=tok_fn to use our HF tokenizer | text_vocab=transformer_vocab to load our pretrained vocab | before_batch=transformer_padding(transformer_tokenizer) to use our custom padding function | . fct_dls = TextDataLoaders.from_df(df, text_col=&quot;text&quot;, tok_tfm=fastai_tokenizer, text_vocab=tokenizer_vocab_ls, before_batch=[padding], label_col=&#39;label&#39;, valid_col=&#39;is_valid&#39;, bs=bs) . fct_dls.show_batch(max_n=3, trunc_at=60) . text category . 0 &lt;s&gt; ƒ†I ƒ†was ƒ†fortunate ƒ†enough ƒ†to ƒ†meet ƒ†George ƒ†Pal ƒ†( and ƒ†still ƒ†have ƒ†my ƒ†DS : TM OB ƒ†poster ƒ†aut ographed ƒ†by ƒ†him ) ƒ†at ƒ†a ƒ†convention ƒ†shortly ƒ†after ƒ†the ƒ†release , ƒ†and ƒ†asked ƒ†him ƒ†why ƒ†he ƒ†chose ƒ†to ƒ†do ƒ†the ƒ†film ƒ†&quot; camp &quot;. ƒ†Before ƒ†he ƒ†could ƒ†answer , ƒ†two ƒ†studio ƒ†fl acks ƒ†intercepted ƒ†and ƒ†lect ured ƒ†me ƒ†on | negative | . 1 &lt;s&gt; ƒ†**** Don &#39;t ƒ†read ƒ†this ƒ†review ƒ†if ƒ†you ƒ†want ƒ†the ƒ†shocking ƒ†conclusion ƒ†of ƒ†&quot; The ƒ†Cr ater ƒ†Lake ƒ†Monster &quot; ƒ†to ƒ†be ƒ†a ƒ†total ƒ†surprise **** &lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; A ƒ†clay m ation ƒ†pl es ios aur ƒ†rises ƒ†from ƒ†the ƒ†depths ƒ†of ƒ†Cr ater ƒ†Lake ƒ†to ƒ†wre ak ƒ†havoc ƒ†on ƒ†a ƒ†group ƒ†of ƒ†local ƒ†red ne | negative | . 2 &lt;s&gt; ƒ†This ƒ†is ƒ†the ƒ†last ƒ†of ƒ†four ƒ†sw ash buck lers ƒ†from ƒ†France ƒ†I &#39;ve ƒ†scheduled ƒ†for ƒ†viewing ƒ†during ƒ†this ƒ†Christmas ƒ†season : ƒ†the ƒ†others ƒ†( in ƒ†order ƒ†of ƒ†viewing ) ƒ†were ƒ†the ƒ†un inspired ƒ†THE ƒ†BLACK ƒ†T UL IP ƒ†( 1964 ; ƒ†from ƒ†the ƒ†same ƒ†director ƒ†as ƒ†this ƒ†one ƒ†but ƒ†not ƒ†nearly ƒ†as ƒ†good ), ƒ†the ƒ†surprisingly ƒ†effective ƒ†L | positive | . Create our learner . opt_func = partial(Adam, decouple_wd=True) loss = LabelSmoothingCrossEntropy() fasthugs_model = FastHugsModel(transformer_cls=model_class, config_dict=config_dict, n_class=dls.c, pretrained=True) learn = Learner(dls, fasthugs_model, opt_func=opt_func, splitter=model_splitter, loss_func=loss, metrics=[accuracy]).to_fp16() . Stage 1 training . Lets freeze the model backbone and only train the classifier head. freeze_to(1) means that only the classifier head is trainable . learn.freeze_to(1) . Lets find a learning rate to train our classifier head . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=9.999999747378752e-07, lr_steep=0.10000000149011612) . learn.recorder.plot_lr_find() plt.vlines(9.999e-7, 0.65, 1.1) plt.vlines(0.10, 0.65, 1.1) . &lt;matplotlib.collections.LineCollection at 0x7f7582802450&gt; . learn.fit_one_cycle(3, lr_max=1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.692172 | 0.653464 | 0.550000 | 00:07 | . 1 | 0.573368 | 0.591558 | 0.635000 | 00:07 | . 2 | 0.522324 | 0.533852 | 0.810000 | 00:07 | . learn.save(&#39;roberta-fasthugs-stg1-1e-3&#39;) . learn.recorder.plot_loss() . Stage 2 training . And now lets train the full model with differential learning rates . learn.unfreeze() . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=0.03981071710586548) . learn.recorder.plot_lr_find() plt.vlines(6.30e-8, 0.6, 1.2) plt.vlines(0.039, 0.6, 1.2) . &lt;matplotlib.collections.LineCollection at 0x7f7582464490&gt; . learn.fit_one_cycle(3, lr_max=slice(1e-5, 1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.425518 | 0.354511 | 0.910000 | 00:31 | . 1 | 0.278425 | 0.372734 | 0.910000 | 00:32 | . 2 | 0.272590 | 0.366681 | 0.925000 | 00:31 | . learn.save(&#39;roberta-fasthugs-stg2-3e-5&#39;) . learn.recorder.plot_loss() . Lets Look at the model&#39;s predictions . learn.predict(&quot;This was a really good movie, i loved it&quot;) . (&#39;positive&#39;, tensor(1), tensor([0.1498, 0.8502])) . from fastai2.interpret import * #interp = Interpretation.from_learner(learn) interp = ClassificationInterpretation.from_learner(learn) . interp.plot_top_losses(3) . input target predicted probability loss . 0 &lt;s&gt; ƒ†This ƒ†movie ƒ†is ƒ†horrible - ƒ†in ƒ†a ƒ†&#39; so ƒ†bad ƒ†it &#39;s ƒ†good &#39; ƒ†kind ƒ†of ƒ†way .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; The ƒ†storyline ƒ†is ƒ†re h ashed ƒ†from ƒ†so ƒ†many ƒ†other ƒ†films ƒ†of ƒ†this ƒ†kind , ƒ†that ƒ†I &#39;m ƒ†not ƒ†going ƒ†to ƒ†even ƒ†bother ƒ†describing ƒ†it . ƒ†It &#39;s ƒ†a ƒ†sword / s or cery ƒ†picture , ƒ†has ƒ†a ƒ†kid ƒ†hoping ƒ†to ƒ†realize ƒ†how ƒ†important ƒ†he ƒ†is ƒ†in ƒ†this ƒ†world , ƒ†has ƒ†a ƒ†&quot; nom adic &quot; ƒ†adventurer , ƒ†an ƒ†evil ƒ†aide / s orce rer , ƒ†a ƒ†princess , ƒ†a ƒ†hairy ƒ†creature .... you ƒ†get ƒ†the ƒ†point .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; The ƒ†first ƒ†time ƒ†I ƒ†caught ƒ†this ƒ†movie ƒ†was ƒ†during ƒ†a ƒ†very ƒ†harsh ƒ†winter . ƒ†I ƒ†don &#39;t ƒ†know ƒ†why ƒ†I ƒ†decided ƒ†to ƒ†continue ƒ†watching ƒ†it ƒ†for ƒ†an ƒ†extra ƒ†five ƒ†minutes ƒ†before ƒ†turning ƒ†the ƒ†channel , ƒ†but ƒ†when ƒ†I ƒ†caught ƒ†site ƒ†of ƒ†Gulf ax | positive | negative | 0.970687747001648 | 3.354750156402588 | . 1 &lt;s&gt; ƒ†In ƒ†17 th ƒ†Century ƒ†Japan , ƒ†there ƒ†lived ƒ†a ƒ†samurai ƒ†who ƒ†would ƒ†set ƒ†the ƒ†standard ƒ†for ƒ†the ƒ†ages . ƒ†His ƒ†name ƒ†was ƒ†May eda . ƒ†He ƒ†is ƒ†sent ƒ†on ƒ†an ƒ†epic ƒ†journey ƒ†across ƒ†the ƒ†world ƒ†to ƒ†acquire ƒ†5 , 000 ƒ†mus cats ƒ†from ƒ†the ƒ†King ƒ†of ƒ†Spain . ƒ†Whilst ƒ†at ƒ†sea ƒ†a ƒ†violent ƒ†storm ƒ†swall ows ƒ†their ƒ†precious ƒ†gold ƒ†intended ƒ†to ƒ†buy ƒ†the ƒ†weapons ƒ†and ƒ†almost ƒ†takes ƒ†their ƒ†lives . ƒ†May eda ƒ†must ƒ†battle ƒ†all ƒ†odds ƒ†to ƒ†survive ƒ†and ƒ†the ƒ†secure ƒ†the ƒ†fate ƒ†of ƒ†his ƒ†beloved ƒ†Japan . ƒ†Shogun ƒ†May eda ƒ†is ƒ†a ƒ†multi ƒ†million ƒ†dollar ƒ†action ƒ†adventure ƒ†epic ƒ†set ƒ†across ƒ†three ƒ†continents .&lt; br ƒ†/ &gt;&lt; br ƒ†/&gt; Star ring ƒ†cinema ƒ†legends ƒ†Sho ƒ†Kos ugi ƒ†( T ench u : ƒ†Stealth ƒ†Assassins ), ƒ†Christopher ƒ†Lee ƒ†( Star ƒ†Wars , ƒ†Lord ƒ†of ƒ†the ƒ†Rings ƒ†Trilogy ), ƒ†John ƒ†Rh ys ƒ†Davies ƒ†( Lord ƒ†of ƒ†the ƒ†Rings ƒ†Trilogy , ƒ†Indiana ƒ†Jones | negative | positive | 0.9588471055030823 | 3.033039093017578 | . 2 &lt;s&gt; ƒ†&quot; How ƒ†To ƒ†Lose ƒ†Friends ƒ†&amp; ƒ†Alien ate ƒ†People &quot; ƒ†is ƒ†not ƒ†based ƒ†on ƒ†Tiger ƒ†Woods &#39; ƒ†inf idel ities . ƒ†It ƒ†is ƒ†a ƒ†mediocre ƒ†romantic ƒ†comedy ƒ†based ƒ†on ƒ†Toby ƒ†Young &#39;s ƒ†book ƒ†on ƒ†his ƒ†experiences ƒ†working ƒ†as ƒ†a ƒ†journalist ƒ†covering ƒ†celebrities . ƒ†The ƒ†film ƒ†stars ƒ†Simon ƒ†Pe gg ƒ†as ƒ†Sidney ƒ†Young , ƒ†a ƒ†z any ƒ†British ƒ†journalist ƒ†who ƒ†takes ƒ†a ƒ†job ƒ†in ƒ†an ƒ†illustrious ƒ†celebrity ƒ†magazine ƒ†in ƒ†New ƒ†York . ƒ†Young ƒ†is ƒ†restless ƒ†in ƒ†getting ƒ†caught ƒ†up ƒ†all ƒ†type ƒ†of ƒ†shenanigans ƒ†to ƒ†alien ate ƒ†all ƒ†around ƒ†him , ƒ†hence ƒ†movie ƒ†title . ƒ†He ƒ†is ƒ†upro arious , ƒ†daring , ƒ†and ƒ†mor onic . ƒ†But ƒ†nevertheless ƒ†for ƒ†some ƒ†very ƒ†bizarre ƒ†reason , ƒ†he ƒ†is ƒ†a ƒ†somewhat ƒ†lik able ƒ†character . ƒ†Sidney ƒ†be friends ƒ†a ƒ†fellow ƒ†journalist , ƒ†the ƒ†composed ƒ†Alison ƒ†Olsen , ƒ†played ƒ†quite ƒ†adm ir ably ƒ†by ƒ†Kirst en ƒ†Dun st . ƒ†However , ƒ†Sidney ƒ†is ƒ†primarily ƒ†longing | positive | negative | 0.942081868648529 | 2.7092723846435547 | .",
            "url": "https://www.ntentional.com/nlp/training%20technique/classification/2020/04/17/fasthugs_seq_classification.html",
            "relUrl": "/nlp/training%20technique/classification/2020/04/17/fasthugs_seq_classification.html",
            "date": " ‚Ä¢ Apr 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "FixRes: 'Fixing the train-test resolution discrepancy'",
            "content": "TL;DR . The paper outlines two easy-to-implement tips to improve your image classification test results: . Do your inference on the test set at a higher resolution than your train set | Fine-tune the last layers of your CNN classifier (i.e. the linear layer(s) after your pooling layer) at the higher test resolution | Overview . This article is a quick summary of &#39;Fixing the train-test resolution discrepancy&#39; from Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv√© J√©gou from Facebook AI Research, presented at NeurIPS 2019, with additional data from the note &#39;Fixing the train-test resolution discrepancy: FixEfficientNet&#39; from the same authors . Results . Facebook AI Research (FAIR) used this technique to achieve a new SOTA result on Imagenet (88.5% top-1 accuracy) using EfficientNet (using extra data) | . . The authors also claim that it can enable faster training by training at a lower resolution while still attaining similr/better results | . Our FixEfficientNet-L2 obtains a new state-of-the-art performance on ImageNet!You can find all our new results on the FixRes additional note (https://t.co/mvY3EkGucR) and also on @paperswithcode and @sotabench(In case you missed the FixRes paper : https://t.co/2NgQcrGDk5) pic.twitter.com/WiQtJQxdgT . &mdash; Hugo Touvron (@HugoTouvron) March 23, 2020 . But Why? . Using typical training transforms such as RandomResizedCrop result in objects in training images appearing larger than they do in the test set. Have a look at the example from the paper below. . Our original image is resized to 224 x 224 before it is shown to the model. RandomResizedCrop is used to resize our training image (and add a little regularisation) while for the test image a simple center crop is taken. As a result of these different resizing methods, the size of the white horse in the top left training image is much larger than what would be shown to the model in the test set. It is this difference in object (e.g. horse) size that the authors say that their FixRes technique addresses . . In other words: . ...resizing the input images in pre-processing changes the distribution of objects sizes. Since different pre-processing protocols are used at training and testing time, the size distribution differs in the two cases. . How? - Two Tricks . Test at a Higher Resolution | Simply testing at a higher resolution should yield a performance improvement. Here, the authors show ImageNet top-1 test set accuracy trained at 224 x 224, you can see that the optimal test resolution was 288 x 288: (This behaviour was previously been shown in 2016 in &quot;Identity Mappings in Deep Residual Networks&quot;). Alternatively if you don&#39;t want to/cannot test at higher resolution, then training at a lower resolution is said to deliver the same accuracy, while enabling you to train faster (as you will be able to use a larger batch size with your smaller image resolutions) . Fine-tuning of later (classifier) layers of your CNN model | For the convolutional part of the CNN, comprising linear convolution, subsampling, ReLU, and similar layers, changing the input crop size is approximately transparent because the receptive field is unaffected by the input size. However, for classification the network must be terminated by a pooling operator (usually average pooling) in order to produce a fixed-size vector. Changing the size of the input crop strongly affects the activation statistics of this layer. . When fine-tuning, the authors recommend using test-time augmentation, not the previous training augmentation as it is simplest and performs well. Using training augmentations gave only slightly better results. . Similarity to Fast.ai&#39;s Progressive Resizing . Interestingly this technique is a little similar to Progressive Resizing, first espoused in the fast.ai deep learning course. The idea behind Progressive Resizing is that you first train at a lower resolution before increasing resolution and training again, albeit you&#39;re always training the entire network as opposed fine-tuning the classifier layers as described above. Nevertheless, it makes me wonder if both the FixRes and Progressive Resizing training techniques work via correcting for the same Train/Test object size mis-match? . Any thoughts, comments, suggestions I&#39;d love to hear from you @mcgenergy on Twitter üòÉ .",
            "url": "https://www.ntentional.com/papers/training%20technique/classification/2020/04/15/fixres.html",
            "relUrl": "/papers/training%20technique/classification/2020/04/15/fixres.html",
            "date": " ‚Ä¢ Apr 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Morgan (Me)",
          "content": "Current on a self-directed learning journey, diving deep into ML and loving it . Previously at Facebook and before then working in electricity trading at Gaelectric and Danske Commodities . I really enjoy ML talk, give me a shout on @mcgenergy on Twitter or on LinkedIn or have a look at my latest work (such as HuggingFace, a bridge between between fastai and the HuggingFace library) on my Github . .",
          "url": "https://www.ntentional.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": ". Welcome! My latest articles are below, if you‚Äôd like to get in touch, find me at @mcgenergy on Twitter . Latest Articles üëá .",
          "url": "https://www.ntentional.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://www.ntentional.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}